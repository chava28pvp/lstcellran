{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "# =====================================================================\n",
    "# Processo de Automatización NOKIA 5G\n",
    "# =====================================================================\n",
    "\n",
    "import os, glob, io, re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython.lib.deepreload import load_next\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Alignment, Font, Border, PatternFill\n",
    "from openpyxl.utils import get_column_letter\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import unicodedata\n",
    "from charset_normalizer import from_path\n",
    "\n",
    "\n",
    "# Ruta base (ajústala si cambia)\n",
    "#BASE_DIR = Path(r\"C:\\Users\\EAlor\\OneDrive - ACS Solutions\\Documents\\AT&T\\LST Cell Ran\\Nokia New\\Nokia Noviembre\")\n",
    "BASE_DIR = Path(r\"C:\\Users\\EAlor\\OneDrive - ACS Solutions\\Documents\\AT&T\\LST Cell Ran\\Nokia New\\XML_Output\\Diciembre\")\n",
    "\n",
    "HEADERS = [\"AT&T_Site_Name\", \"Site ID\", \"VERSION\", \"DISTNAME\", \"MOID\", \"angle\", \"name\", \"actDl256Qam\", \"administrativeState\", \"availabilityStatus\", \"cellBarred\", \"cellName\", \"freqBandIndicatorNR\", \"lcrId\", \"nrCellIdentity\", \"nrCellType\", \"operationalState\", \"pMax\", \"physCellId\", \"arfcnSsbPbch\", \"chBwDl\", \"chBwUl\", \"nrarfcnDl\", \"nrarfcnUl\", \"enbPlmn_mcc_mnc_mncLength\", \"ltePhyCellId\", \"ssbPosition\", \"configuredEpsTac\", \"nrPlmnDNList\", \"LAT\", \"LON\"]\n",
    "\n",
    "\n",
    "# Lista de encabezados, en el orden requerido\n",
    "HEADER_NRCELL = [\"VERSION\", \"DISTNAME\", \"MOID\", \"angle\", \"name\", \"actDl256Qam\", \"administrativeState\", \"availabilityStatus\", \"cellBarred\", \"cellName\", \"freqBandIndicatorNR\", \"lcrId\", \"nrCellIdentity\", \"nrCellType\", \"operationalState\", \"pMax\", \"physCellId\", \"arfcnSsbPbch\"]\n",
    "\n",
    "HEADER_MRBTS = [\"FILENAME\", \"DATETIME\", \"VERSION\", \"DISTNAME\", \"MOID\", \"name\", \"altitude\", \"btsName\", \"latitude\", \"longitude\", \"blockingState\"]\n",
    "\n",
    "HEADER_NRCELL_FDD = [\"DISTNAME\", \"chBwDl\", \"chBwUl\", \"nrarfcnDl\", \"nrarfcnUl\"]\n",
    "\n",
    "HEADER_NRDSSLTE = [\"DISTNAME\", \"enbPlmn_mcc_mnc_mncLength\", \"ltePhyCellId\", \"ssbPosition\"]\n",
    "\n",
    "HEADER_NRPLMNSET_NSA = [\"DISTNAME\", \"configuredEpsTac\", \"nrPlmnDNList\"]\n"
   ],
   "id": "6c55b8bcd0129255",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def read_csv_files(filename: str, header, encoder):\n",
    "    filepath = str(BASE_DIR / f\"{filename}\")\n",
    "    # Leer sólo las columnas necesarias del csv\n",
    "    df = pd.read_csv(filepath, usecols = header, encoding = encoder, dtype=str)[header]\n",
    "\n",
    "#    print(nrcell_df.head())\n",
    "\n",
    "    return df\n"
   ],
   "id": "b886f8c3e48df98b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "nrcell_df = read_csv_files(\"NRCELL.csv\", HEADER_NRCELL, \"utf-8\")\n",
    "mrbts_df = read_csv_files(\"MRBTS.csv\", HEADER_MRBTS, \"latin-1\")\n",
    "nrcell_fdd_df = read_csv_files(\"NRCELL_FDD.csv\", HEADER_NRCELL_FDD, \"utf-8\")\n",
    "nrdsslte_df = read_csv_files(\"NRDSSLTE.csv\",HEADER_NRDSSLTE, \"utf-8\")\n",
    "nrplmnset_df = read_csv_files(\"NRPLMNSET_NSA.csv\",HEADER_NRPLMNSET_NSA, \"utf-8\")\n",
    "\n",
    "\n",
    "print(\"Shape NRCELL original:\", nrcell_df.shape, \"\\nShape MRBTS original:\", mrbts_df.shape, \"\\nShape NRCELL_FDD original:\", nrcell_fdd_df.shape, \"\\nShape NRDSSLTE original:\", nrdsslte_df.shape, \"\\nShape NRPLMNSET_NSA original:\", nrplmnset_df.shape)\n",
    "# print(NRCELL_fdd_df.head(5).to_string(index=False))\n",
    "\n",
    "try:\n",
    "    display(nrcell_df.head(5))\n",
    "    display(mrbts_df.head(5))\n",
    "    display(nrcell_fdd_df.head(5))\n",
    "    display(nrdsslte_df.head(5))\n",
    "    display(nrplmnset_df.head(5))\n",
    "except NameError:\n",
    "    # Por si no estás en notebook\n",
    "    print(nrcell_df.head(5).to_string(index=False))\n",
    "    print(mrbts_df.head(5).to_string(index=False))\n",
    "    print(nrcell_fdd_df.head(5).to_string(index=False))\n",
    "    print(nrdsslte_df.head(5).to_string(index=False))\n",
    "    print(nrplmnset_df.head(5).to_string(index=False))\n",
    "\n"
   ],
   "id": "f657dfa988130323",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =====================================================================\n",
    "# 1) Insertar columnas AT&TSite_Name y Site ID\n",
    "# =====================================================================\n",
    "\n",
    "# Copia para no tocar los originales\n",
    "nrcell_df_mod = nrcell_df.copy()\n",
    "\n",
    "\n",
    "print(\"Shape original:\", nrcell_df_mod.shape)\n",
    "\n",
    "# --- Crear columna 'AT&T_Site_Name' a partir de mrbts_df ---\n",
    "# 1. Crear un mapeo DISTNAME -> name\n",
    "mapa = dict(zip(mrbts_df['DISTNAME'], mrbts_df['name']))\n",
    "\n",
    "# 2. Generar la clave intermedia a partir de DISTNAME\n",
    "substr_distname = nrcell_df_mod['DISTNAME'].str.split('/', n=2).str[:2].str.join('/')\n",
    "\n",
    "# 3. Mapear con el diccionario\n",
    "tmp_name = substr_distname.map(mapa)\n",
    "\n",
    "# 4. Aplicar la lógica condicional:\n",
    "#    - Si contiene '-', usar la segunda parte (x.split('-', 2)[1])\n",
    "#    - Si no, dejar el valor original\n",
    "#    - Si está vacío, dejar NaN\n",
    "nrcell_df_mod.insert(\n",
    "    0,\n",
    "    'AT&T_Site_Name',\n",
    "    tmp_name.apply(\n",
    "        lambda x: x.split('-', 2)[1].strip() if isinstance(x, str) and '-' in x else x\n",
    "    )\n",
    ")\n",
    "\n",
    "# 5. Rellenar los NaN con el valor original de mrbts_df['name']\n",
    "nrcell_df_mod['AT&T_Site_Name'].fillna(substr_distname.map(mapa), inplace=True)\n",
    "\n",
    "# 6. Contar y reportar cuántos valores quedaron vacíos\n",
    "nan_count = nrcell_df_mod['AT&T_Site_Name'].isna().sum()\n",
    "if nan_count > 0:\n",
    "    print(f\"⚠️ {nan_count} registros sin 'AT&T_Site_Name' encontrados.\")\n",
    "else:\n",
    "    print(\"✅ Todos los registros tienen 'AT&T_Site_Name'.\")\n",
    "\n",
    "# Insertar la nueva columna Site ID extrayendo el texto deseado de DISTNAME\n",
    "nrcell_df_mod.insert(1, 'Site ID', nrcell_df_mod['DISTNAME'].str.split('-', n=3).str[3].str.split('/',n=2).str[0] )\n",
    "\n",
    "print(\"Shape nuevo:\", nrcell_df_mod.shape)\n",
    "\n",
    "# Vista de verificación (muestra solo unas filas)\n",
    "pd.set_option(\"display.max_columns\", None)  # opcional\n",
    "try:\n",
    "    display(nrcell_df_mod.head(5))\n",
    "except NameError:\n",
    "    # Por si no estás en notebook\n",
    "    print(nrcell_df_mod.head(5).to_string(index=False))\n"
   ],
   "id": "35c2d994e82c73c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =====================================================================\n",
    "# 2) Información de NRCELL_FDD\n",
    "# =====================================================================\n",
    "\n",
    "nrcell_df_merged = nrcell_df_mod.copy()\n",
    "\n",
    "print(\"Shape original:\", nrcell_df_merged.shape)\n",
    "\n",
    "# Remueve la ultima parte de DISTNAME en NRCELL_FDD para que haga match con DISTNAME de NRCELL\n",
    "nrcell_fdd_df['DISTNAME'] = nrcell_fdd_df['DISTNAME'].str.rsplit('/', n=1).str[0]\n",
    "\n",
    "columnas_a_insertar = [\"chBwDl\", \"chBwUl\", \"nrarfcnDl\", \"nrarfcnUl\"]\n",
    "\n",
    "# limpia string de DISTNAME\n",
    "nrcell_df_merged[\"DISTNAME\"] = nrcell_df_merged[\"DISTNAME\"].astype(str).str.strip().str.upper()\n",
    "nrcell_fdd_df[\"DISTNAME\"] = nrcell_fdd_df[\"DISTNAME\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "coincidencias = set(nrcell_df_merged[\"DISTNAME\"]) & set(nrcell_fdd_df[\"DISTNAME\"])\n",
    "print(len(coincidencias))\n",
    "\n",
    "nrcell_df_merged = nrcell_df_merged.merge(\n",
    "    nrcell_fdd_df[[\"DISTNAME\"] + columnas_a_insertar],\n",
    "    on = \"DISTNAME\",\n",
    "    how = \"left\")\n",
    "\n",
    "print(\"Shape nuevo:\", nrcell_df_merged.shape)\n",
    "\n",
    "# Vista de verificación (muestra solo unas filas)\n",
    "pd.set_option(\"display.max_columns\", None)  # opcional\n",
    "try:\n",
    "    display(nrcell_df_merged.head(5))\n",
    "except NameError:\n",
    "    # Por si no estás en notebook\n",
    "    print(nrcell_df_merged.head(5).to_string(index=False))\n"
   ],
   "id": "a5896f135827df8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =====================================================================\n",
    "# 3) Información de NRDSSLTE\n",
    "# =====================================================================\n",
    "\n",
    "nrcell_df_mergelte = nrcell_df_merged.copy()\n",
    "\n",
    "print(\"Shape original:\", nrcell_df_mergelte.shape)\n",
    "\n",
    "# Remueve la ultima parate de DISTNAME en NRDSSLTE para que haga match con DISTNAME de NRCELL\n",
    "nrdsslte_df['DISTNAME'] = nrdsslte_df['DISTNAME'].str.rsplit('/', n=1).str[0]\n",
    "\n",
    "columnas_a_insertar = [\"enbPlmn_mcc_mnc_mncLength\", \"ltePhyCellId\", \"ssbPosition\"]\n",
    "\n",
    "# limpia string de DISTNAME\n",
    "nrcell_df_mergelte[\"DISTNAME\"] = nrcell_df_mergelte[\"DISTNAME\"].astype(str).str.strip().str.upper()\n",
    "nrdsslte_df[\"DISTNAME\"] = nrdsslte_df[\"DISTNAME\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "coincidencias = set(nrcell_df_mergelte[\"DISTNAME\"]) & set(nrdsslte_df[\"DISTNAME\"])\n",
    "print(len(coincidencias))\n",
    "\n",
    "nrcell_df_mergelte = nrcell_df_mergelte.merge(\n",
    "    nrdsslte_df[[\"DISTNAME\"] + columnas_a_insertar],\n",
    "    on = \"DISTNAME\",\n",
    "    how = \"left\")\n",
    "\n",
    "print(\"Shape nuevo:\", nrcell_df_mergelte.shape)\n",
    "\n",
    "# Vista de verificación (muestra solo unas filas)\n",
    "pd.set_option(\"display.max_columns\", None)  # opcional\n",
    "try:\n",
    "    display(nrcell_df_mergelte.head(5))\n",
    "except NameError:\n",
    "    # Por si no estás en notebook\n",
    "    print(nrcell_df_mergelte.head(5).to_string(index=False))\n"
   ],
   "id": "a87d624a489db6f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =====================================================================\n",
    "# 4) Información de NRPLMNSET_NSA\n",
    "# =====================================================================\n",
    "\n",
    "nrcell_df_mergelmnset = nrcell_df_mergelte.copy()\n",
    "\n",
    "print(\"Shape original:\", nrcell_df_mergelmnset.shape)\n",
    "\n",
    "# Remueve la ultima parte de DISTNAME en NRDSSLTE para que haga match con DISTNAME de NRCELL\n",
    "nrplmnset_df['DISTNAME'] = nrplmnset_df['DISTNAME'].str.rsplit('/', n=1).str[0]\n",
    "\n",
    "columnas_a_insertar = [\"configuredEpsTac\", \"nrPlmnDNList\"]\n",
    "\n",
    "# limpia string de DISTNAME\n",
    "nrcell_df_mergelmnset[\"DISTNAME\"] = nrcell_df_mergelmnset[\"DISTNAME\"].astype(str).str.strip().str.upper()\n",
    "nrplmnset_df[\"DISTNAME\"] = nrplmnset_df[\"DISTNAME\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "coincidencias = set(nrcell_df_mergelmnset[\"DISTNAME\"]) & set(nrplmnset_df[\"DISTNAME\"])\n",
    "print(len(coincidencias))\n",
    "\n",
    "nrcell_df_mergelmnset = nrcell_df_mergelmnset.merge(\n",
    "    nrplmnset_df[[\"DISTNAME\"] + columnas_a_insertar],\n",
    "    on = \"DISTNAME\",\n",
    "    how = \"left\")\n",
    "\n",
    "print(\"Shape nuevo:\", nrcell_df_mergelmnset.shape)\n",
    "\n",
    "# Vista de verificación (muestra solo unas filas)\n",
    "pd.set_option(\"display.max_columns\", None)  # opcional\n",
    "try:\n",
    "    display(nrcell_df_mergelmnset.head(5))\n",
    "except NameError:\n",
    "    # Por si no estás en notebook\n",
    "    print(nrcell_df_mergelmnset.head(5).to_string(index=False))\n",
    "\n"
   ],
   "id": "7456e8efe62dcbe7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =====================================================================\n",
    "# 5) LAT/LON desde All_Nokia_5G_{YYYYMM} (mes anterior)\n",
    "# =====================================================================\n",
    "nrcell_df_mergelatlon = nrcell_df_mergelmnset.copy()\n",
    "\n",
    "today = date.today()\n",
    "prev_year  = today.year if today.month > 1 else today.year - 1\n",
    "prev_month = today.month - 1 or 12\n",
    "yyyymm = f\"{prev_year}{prev_month:02d}\"\n",
    "print(yyyymm)\n",
    "\n",
    "# an_path = BASE_DIR / f\"All_Nokia_5G_{yyyymm}.xlsx\"\n",
    "an_path = BASE_DIR / f\"All_Nokia_5G_20260121.xlsx\"\n",
    "an_df = pd.read_excel(an_path, usecols=[\"AT&T_Site_Name\", \"LAT\", \"LON\"])\n",
    "an_df[\"AT&T_Site_Name\"] = an_df[\"AT&T_Site_Name\"].astype(str).str.strip()\n",
    "an_df = an_df.drop_duplicates(subset=[\"AT&T_Site_Name\"], keep=\"first\")\n",
    "display(an_df.head(5))\n",
    "\n",
    "nrcell_df_mergelatlon[\"AT&T_Site_Name\"] = nrcell_df_mergelatlon[\"AT&T_Site_Name\"].astype(str).str.strip()\n",
    "\n",
    "merged_df = nrcell_df_mergelatlon.merge(\n",
    "    an_df,\n",
    "    on=\"AT&T_Site_Name\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_an\")\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "merged = nrcell_df_mod.merge(\n",
    "    an_df[[\"AT&T_Site_Name\", \"LAT\", \"LON\"]],\n",
    "    on = \"AT&T_Site_Name\",\n",
    "    how=\"left\")\n",
    "for col in [\"LAT\", \"LON\"]:\n",
    "    m = _is_blank(merged[col]) if col in merged.columns else pd.Series(True, index=merged.index)\n",
    "    merged.loc[m, col] = merged.loc[m, col + \"_an\"]\n",
    "    if col + \"_an\" in merged:\n",
    "        merged.drop(columns=[col + \"_an\"], inplace=True)\n",
    "\n",
    "faltan = (\n",
    "    _is_blank(merged[\"LAT\"]) |\n",
    "    _is_blank(merged[\"LON\"])\n",
    ")\n",
    "\n",
    "if not faltan.any():\n",
    "    nrcell_df_mod = merged\n",
    "    print(\"All_Nokia cubrió 100% (LAT/LON).\")\n",
    "    display(df_out.loc[:, [\"AT&T_Site_Name\",\"LAT\",\"LON\"]].head(5))\n",
    "else:\n",
    "    print(f\"Quedan {int(faltan.sum())} filas con faltantes. Se aplica fallback EPT…\") \"\"\"\n",
    "\n",
    "print(\"Shape nuevo:\", merged_df.shape)\n",
    "\n",
    "# Vista de verificación (muestra solo unas filas)\n",
    "pd.set_option(\"display.max_columns\", None)  # opcional\n",
    "try:\n",
    "    display(merged_df.head(5))\n",
    "except NameError:\n",
    "    # Por si no estás en notebook\n",
    "    print(merged_df.head(5).to_string(index=False))\n"
   ],
   "id": "759128248e673c2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =====================================================================\n",
    "# 6) LAT/LON desde EPT (mas reciente)\n",
    "# =====================================================================\n",
    "\n",
    "if merged_df[[\"LAT\", \"LON\"]].isna().any(axis=1).any():\n",
    "    # Ejecuta el proceso si hay al menos un NaN en LAT o LON\n",
    "\n",
    "    nan_count = merged_df[[\"LAT\", \"LON\"]].isna().sum().sum()\n",
    "    print(\"Se necesita buscar LAT/LON restantes en EPT\")\n",
    "\n",
    "    ruta_ept = BASE_DIR\n",
    "\n",
    "    # Prefijo del archivo\n",
    "    prefijo_ept = \"EPT_ATT_UMTS_LTE_\"\n",
    "\n",
    "    # Busca archivo que empiece con el prefijo\n",
    "    archivo = glob.glob(os.path.join(ruta_ept, f\"{prefijo_ept}*.xlsx\"))\n",
    "\n",
    "    # Verifica si se encontró archivo\n",
    "    if archivo:\n",
    "        archivo_encontrado = archivo[0]\n",
    "        nombre_archivo = os.path.basename(archivo_encontrado)\n",
    "\n",
    "        # Lista de hojas a leer\n",
    "        hojas_fijas = [\n",
    "            \"EPT_3G_LTE_OUTDOOR\",\n",
    "            \"PLAN_OUTDOOR\",\n",
    "            \"EPT_3G_LTE_INDOOR\",\n",
    "            \"PLAN_INDOOR\",\n",
    "            \"Eventos_Especiales\"\n",
    "        ]\n",
    "\n",
    "        # Detecta automáticamente las hojas que contienen \"Nokia\" (para este vendor en particular)\n",
    "        todas_las_hojas = pd.ExcelFile(archivo_encontrado, engine=\"openpyxl\").sheet_names\n",
    "        hojas_vendor = [h for h in todas_las_hojas if \"nokia\" in h.lower()]\n",
    "        # Combina ambas listas (sin duplicar)\n",
    "        hojas = list(dict.fromkeys(hojas_fijas + hojas_vendor))\n",
    "        print(hojas)\n",
    "\n",
    "        # Lee todas las hojas y agrega el nombre de la hoja en columna\n",
    "        dfs = [\n",
    "            pd.read_excel(archivo_encontrado, sheet_name=hoja, usecols=[\"AT&T_Site_Name\", \"Latitud\", \"Longitud\"], engine=\"openpyxl\")\n",
    "            .assign(Hoja=hoja, Origen=nombre_archivo)\n",
    "            for hoja in hojas\n",
    "        ]\n",
    "\n",
    "        # Concatena todo en un solo DataFrame\n",
    "        df_EPT_inicial = pd.concat(dfs, ignore_index=True).drop_duplicates(subset=[\"AT&T_Site_Name\"])\n",
    "\n",
    "        # Unir con df principal (solo para los NaN)\n",
    "        merged_df = merged_df.merge(df_EPT_inicial, on=\"AT&T_Site_Name\", how=\"left\", suffixes=(\"\", \"_extra\"))\n",
    "        merged_df[\"LAT\"] = merged_df[\"LAT\"].fillna(merged_df[\"Latitud\"])\n",
    "        merged_df[\"LON\"] = merged_df[\"LON\"].fillna(merged_df[\"Longitud\"])\n",
    "        merged_df = merged_df.drop(columns=[\"Latitud\", \"Longitud\"])\n",
    "\n",
    "        if merged_df[[\"LAT\", \"LON\"]].isna().any(axis=1).any():\n",
    "            nan_count = merged_df[[\"LAT\", \"LON\"]].isna().sum().sum()\n",
    "            print(nan_count, \" LAT/LON no encontrados. Se producirá archivo excel con estos faltantes.\")\n",
    "\n",
    "        # Vista de verificación (muestra solo unas filas)\n",
    "        pd.set_option(\"display.max_columns\", None)  # opcional\n",
    "        try:\n",
    "            display(merged_df.head(5))\n",
    "        except NameError:\n",
    "            # Por si no estás en notebook\n",
    "            print(merged_df.head(5).to_string(index=False))\n",
    "    else:\n",
    "        print(\"⚠️ No se encontró archivo EPT\")\n",
    "else:\n",
    "    print(\"LAT y LON encontrados en su totalidad en archivo anterior. No se necesita EPT.\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)  # opcional\n",
    "try:\n",
    "    display(merged_df.head(5))\n",
    "except NameError:\n",
    "    # Por si no estás en notebook\n",
    "    print(merged_df.head(5).to_string(index=False))\n",
    "\n"
   ],
   "id": "2b70df9100de9658",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =====================================================================\n",
    "# 7) Archivo final en excel\n",
    "# =====================================================================\n",
    "# Eliminar columnas auxiliares si existen\n",
    "merged_df = merged_df.drop(columns=[c for c in [\"Hoja\", \"Origen\"] if c in merged_df.columns])\n",
    "\n",
    "# === 0) Config y fecha actual ===\n",
    "\n",
    "today   = date.today()\n",
    "yyyymm  = f\"{today.year}{today.month:02d}{today.day:02d}\"\n",
    "\n",
    "final_excel = BASE_DIR / f\"All_Nokia_5G_{yyyymm}.xlsx\"\n",
    "tmp_excel   = BASE_DIR / f\"~tmp_All_Nokia_5G_{yyyymm}.xlsx\"\n",
    "\n",
    "# Usa tu DataFrame final en memoria\n",
    "df_out = merged_df.copy()  # o df_sorted si ya lo traes ordenado\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)  # opcional\n",
    "try:\n",
    "    display(df_out.head(5))\n",
    "except NameError:\n",
    "    # Por si no estás en notebook\n",
    "    print(df_out.head(5).to_string(index=False))\n",
    "\n",
    "# === 2) Guardar sin formato, sin 'nan' ===\n",
    "df_out.to_excel(final_excel, index=False, na_rep=\"\")\n",
    "\n",
    "# 2) Reabrir el MISMO archivo y aplicar formato\n",
    "wb = load_workbook(final_excel)\n",
    "ws = wb.active\n",
    "\n",
    "ws.freeze_panes = \"A2\"\n",
    "for col_idx, header in enumerate(HEADERS, start=1):\n",
    "    cell = ws.cell(row=1, column=col_idx)\n",
    "    cell.value = header\n",
    "    cell.font = Font(name=\"Aptos Narrow\", size=11)\n",
    "    cell.alignment = Alignment(textRotation=90, horizontal=\"center\", vertical=\"center\", wrap_text=True)\n",
    "    cell.border = Border()\n",
    "\n",
    "# Agregar filtro automático en el header\n",
    "ws.auto_filter.ref = ws.dimensions  # aplica el filtro a todo el rango con datos\n",
    "\n",
    "wb.save(final_excel)\n",
    "wb.close()\n",
    "\n",
    "print(f\"✅ Archivo final sin formato guardado → {final_excel}\")"
   ],
   "id": "63a1fecf0809f261",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
