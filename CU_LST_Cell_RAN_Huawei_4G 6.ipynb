{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T22:30:56.929242Z",
     "start_time": "2026-02-11T22:30:56.874248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "########################## Proceso ETL - Huawei 4G ##########################\n",
    "# MA. 20250930.\n",
    "\n",
    "# Librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "#import datetime\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import Alignment\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "\n",
    "## Variables compartidas\n",
    "\n",
    "# Ubicación archivos\n",
    "ruta_carpeta = 'C:/Users/SCaracoza/Documents/AT&T/LST Cell Ran/Huawei/Huawei-Feb'\n",
    "\n",
    "ruta_ept = 'C:/Users/SCaracoza/Documents/AT&T/LST Cell Ran/Huawei/Huawei-Feb'\n",
    "\n",
    "ruta_destino = 'C:/Users/SCaracoza/Documents/AT&T/LST Cell Ran/Huawei/Huawei-Feb'\n",
    "\n",
    "# Fecha para el nombre de los archivos a crear\n",
    "fecha_ejecucion: str = datetime.now().strftime('%Y%m')"
   ],
   "id": "5fb1e259bd2ee1c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T22:31:11.799418Z",
     "start_time": "2026-02-11T22:30:58.557080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MA. 20250930\n",
    "### LST CELL. Lectura y unificación robusta (vertical + horizontal) ###\n",
    "\n",
    "# Archivos\n",
    "archivo_prefijo = \"MML_Task_Result_LST CELL_c\"\n",
    "archivos_txt = glob.glob(os.path.join(ruta_carpeta, f\"{archivo_prefijo}*.txt\"))\n",
    "\n",
    "# Columnas objetivo (FAILED + SUCCEEDED)\n",
    "columnas = [\n",
    "    \"Gestor\", \"Seccion\", \"NE\", \"Report\",\n",
    "    \"Local Cell ID\",\"Cell Name\",\"Csg indicator\",\n",
    "    \"Uplink cyclic prefix length\",\"Downlink cyclic prefix length\",\"NB-IoT Cell Flag\",\n",
    "    \"Coverage Level Type\",\"Frequency band\",\n",
    "    \"Uplink EARFCN indication\",\"Uplink EARFCN\",\"Downlink EARFCN\",\"Uplink bandwidth\",\"Downlink bandwidth\",\n",
    "    \"Cell ID\",\"Physical cell ID\",\"Additional spectrum emission\",\"Cell active state\",\n",
    "    \"Cell admin state\",\"Cell middle block timer(min)\",\"Cell Blocking Duration(min)\",\"Cell FDD TDD indication\",\n",
    "    \"Subframe assignment\",\"Special subframe patterns\",\n",
    "    \"SSP6 DwPTS Mode\",\"Cell Standby Mode\",\"Cell specific offset(dB)\",\n",
    "    \"Frequency offset(dB)\",\"Root sequence index\",\"High speed flag\",\n",
    "    \"Preamble format\",\"Cell radius(m)\",\n",
    "    \"Customized bandwidth configure indicator\",\"Customized uplink bandwidth(0.1MHz)\",\n",
    "    \"Customized downlink bandwidth(0.1MHz)\",\n",
    "    \"Emergency Area Id indicator\",\"Emergency Area ID\",\"Ue max power allowed configure indicator\",\n",
    "    \"Max transmit power allowed(dBm)\",\"Flag of Multi-RRU Cell\",\"Mode of Multi-RRU Cell\",\n",
    "    \"CPRI Ethernet Compression Ratio\",\"CPRI Compression\",\"Physical Cell Number of SFN Cell\",\n",
    "    \"Air Cell Flag\",\"CRS Port Number\",\"Cell transmission and reception mode\",\n",
    "    \"CRS Antenna Port Mapping\",\"User label\",\"Work mode\",\n",
    "    \"CN Operator Sharing Group ID\",\"Intra Frequency RAN Sharing Indication\",\"IntraFreq ANR Indication\",\n",
    "    \"ANR Frequency Priority\",\"Cell Radius Start Location(m)\",\"Specified Cell Flag\",\n",
    "    \"Downlink Punctured RB Number\",\"SFN Master Cell Label\",\"Multi Cell Share Mode\",\n",
    "    \"Standby Cell SFN Recovery Time(h)\",\"Compact Bandwidth Control Interference Mode\",\n",
    "    \"Uplink Punctured RB Number Offset\",\"Ultra High-Speed Cell Root Sequence Index\"\n",
    "]\n",
    "\n",
    "# --- Parseo robusto para \"Display static parameters of cells\" ---\n",
    "def _parse_tabla(cmd_text: str):\n",
    "    \"\"\"\n",
    "    Devuelve lista de dicts. Soporta múltiples tablas \"Display static parameters of cells\"\n",
    "    (por 'To be continued...') y formato vertical clave=valor.\n",
    "    \"\"\"\n",
    "    patron = re.compile(\n",
    "        r\"Display static parameters of cells\\s*-+\\s*\\n(.*?)(?=\\n\\s*\\(Number of results|\\n---\\s*END|\\nMML Command-----|$)\",\n",
    "        flags=re.S\n",
    "    )\n",
    "\n",
    "    resultados = []\n",
    "    for m in patron.finditer(cmd_text):\n",
    "        bloque = m.group(1)\n",
    "        lineas = [ln.rstrip(\"\\n\") for ln in bloque.splitlines() if ln.strip()]\n",
    "\n",
    "        # Variante vertical clave=valor\n",
    "        kv_pairs = []\n",
    "        for ln in lineas:\n",
    "            mm = re.match(r\"^\\s*(.*?)\\s*=\\s*(.*)$\", ln)\n",
    "            if mm:\n",
    "                kv_pairs.append((mm.group(1).strip(), mm.group(2).strip()))\n",
    "        if kv_pairs and not any(re.match(r\"^\\d+\\s\", ln.strip()) for ln in lineas):\n",
    "            resultados.append({k: v for k, v in kv_pairs})\n",
    "            continue\n",
    "\n",
    "        # Variante tabla ancha\n",
    "        if not lineas:\n",
    "            continue\n",
    "        header_line = lineas[0]\n",
    "        tokens = re.split(r\"\\s{2,}\", header_line.strip())\n",
    "\n",
    "        spans, pos = [], 0\n",
    "        for tok in tokens:\n",
    "            idx = header_line.find(tok, pos)\n",
    "            spans.append((tok, idx))\n",
    "            pos = idx + len(tok)\n",
    "        spans = [(name, start, (spans[i+1][1] if i+1 < len(spans) else None))\n",
    "                 for i, (name, start) in enumerate(spans)]\n",
    "\n",
    "        data_lines = [ln for ln in lineas[1:] if re.match(r\"^\\d+\", ln.strip())]\n",
    "        for ln in data_lines:\n",
    "            rec = {name: (ln[start:end].rstrip() if end is not None else ln[start:].rstrip())\n",
    "                   for name, start, end in spans}\n",
    "            resultados.append(rec)\n",
    "\n",
    "    return resultados\n",
    "\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for archivo in archivos_txt:\n",
    "    with open(archivo, \"r\", encoding=\"utf-8\") as f:\n",
    "        contenido = f.read()\n",
    "\n",
    "    # Gestor: p.ej. 'LST CELL_c1'\n",
    "    gestor = os.path.basename(archivo)\n",
    "    gestor = \"_\".join(re.sub(r\"^MML_Task_Result_\", \"\", gestor).split(\"_\")[0:2])\n",
    "\n",
    "    # -------- FAILED --------\n",
    "    m_failed = re.search(r\"=+Failed MML Command=+\\s*(.*?)(?=\\n=+)\", contenido, flags=re.S)\n",
    "    if m_failed:\n",
    "        bloque_failed = m_failed.group(1)\n",
    "        for seg in re.split(r\"MML Command-----LST CELL:;\", bloque_failed):\n",
    "            if not seg.strip():\n",
    "                continue\n",
    "            m_ne = re.search(r\"NE\\s*:\\s*(.*)\", seg)\n",
    "            m_rp = re.search(r\"Report\\s*:\\s*(.*?)(?:\\n---\\s*END|\\n\\n|$)\", seg, flags=re.S)\n",
    "            if not m_ne:\n",
    "                continue\n",
    "            ne = m_ne.group(1).strip()\n",
    "            report = (m_rp.group(1).strip() if m_rp else \"\")\n",
    "            if report.startswith(\"+++\"):\n",
    "                report = \"\"\n",
    "            dfs.append({\"Gestor\": gestor, \"Seccion\": \"FAILED\", \"NE\": ne, \"Report\": report})\n",
    "\n",
    "    # -------- SUCCEEDED --------\n",
    "    m_succ = re.search(r\"=+Succeeded MML Command=+\\s*(.*)\", contenido, flags=re.S)\n",
    "    if m_succ:\n",
    "        bloque_succ = m_succ.group(1)\n",
    "        for cmd in re.split(r\"MML Command-----LST CELL:;\", bloque_succ):\n",
    "            if not cmd.strip():\n",
    "                continue\n",
    "            m_ne = re.search(r\"NE\\s*:\\s*(\\S+)\", cmd)\n",
    "            ne = m_ne.group(1).strip() if m_ne else \"\"\n",
    "            m_rp = re.search(r\"Report\\s*:(.*?)(?:\\n---\\s*END|\\n\\n|$)\", cmd, flags=re.S)\n",
    "            report = (m_rp.group(1).strip() if m_rp else \"\")\n",
    "            if report.startswith(\"+++\"):\n",
    "                report = \"\"\n",
    "\n",
    "            # Tabla / KV\n",
    "            for rec in _parse_tabla(cmd):\n",
    "                fila = {\"Gestor\": gestor, \"Seccion\": \"SUCCEEDED\", \"NE\": ne, \"Report\": report}\n",
    "                fila.update(rec)\n",
    "                dfs.append(fila)\n",
    "\n",
    "# DataFrame final, asegura columnas\n",
    "df_LST_CELL_inicial = pd.DataFrame(dfs)\n",
    "for col in columnas:\n",
    "    if col not in df_LST_CELL_inicial.columns:\n",
    "        df_LST_CELL_inicial[col] = \"\"\n",
    "df_LST_CELL_inicial = df_LST_CELL_inicial[columnas].fillna(\"\")\n",
    "\n",
    "# Se deja solo la información necesaria\n",
    "df_LST_CELL_inicial = df_LST_CELL_inicial[\n",
    "    (df_LST_CELL_inicial[\"Seccion\"] == \"SUCCEEDED\") |\n",
    "    ((df_LST_CELL_inicial[\"Seccion\"] == \"FAILED\") &\n",
    "     (df_LST_CELL_inicial[\"Report\"].str.lower() == \"ne is not connected.\"))\n",
    "]\n",
    "\n",
    "# NE -> Sitio\n",
    "df_LST_CELL_inicial.rename(columns={\"NE\": \"Sitio\"}, inplace=True)\n",
    "\n",
    "# Comentarios\n",
    "df_LST_CELL_inicial[\"Comentarios\"] = np.where(\n",
    "    #(df_LST_CELL_inicial[\"Seccion\"] == \"FAILED\") &\n",
    "    (df_LST_CELL_inicial[\"Report\"].str.lower() == \"ne is not connected.\"),\n",
    "    \"Offline\",\n",
    "    \"\"\n",
    ")\n",
    "\n",
    "# Exporta a Excel\n",
    "#salida = os.path.join(ruta_destino, f\"LST_CELL_{fecha_ejecucion}.xlsx\")\n",
    "#df_LST_CELL_inicial.to_excel(salida, index=False, engine=\"openpyxl\")"
   ],
   "id": "5c393e4250305635",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T22:31:21.867665Z",
     "start_time": "2026-02-11T22:31:11.815871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MA. 20250930\n",
    "### LST CNOPERATORTA. Lectura y unificación ###\n",
    "\n",
    "# Archivos\n",
    "# MA. 20250930\n",
    "archivo_prefijo = \"MML_Task_Result_LST CNOPERATORTA_c\"\n",
    "archivos_txt = glob.glob(os.path.join(ruta_carpeta, f\"{archivo_prefijo}*.txt\"))\n",
    "\n",
    "# Inicializa lista de DataFrames\n",
    "dfs = []\n",
    "\n",
    "for archivo in archivos_txt:\n",
    "    with open(archivo, \"r\", encoding=\"utf-8\") as f:\n",
    "        contenido = f.read()\n",
    "\n",
    "    # Obtiene nombre de archivo origen reducido\n",
    "    origen_match = re.search(r\"(LST CNOPERATORTA_c\\d+)\", os.path.basename(archivo))\n",
    "    origen = origen_match.group(1) if origen_match else os.path.basename(archivo)\n",
    "\n",
    "    # Procesa sección FAILED\n",
    "    bloques_failed = re.split(r\"=+Failed MML Command=+\", contenido)\n",
    "    if len(bloques_failed) > 1:\n",
    "        fallidos = re.findall(r\"NE\\s*:\\s*(.*?)\\nReport\\s*:\\s*(.*?)(?:\\n|$)\", bloques_failed[1], re.DOTALL)\n",
    "        for ne, report in fallidos:\n",
    "            report = report.strip()\n",
    "            if report.startswith(\"+++\"):\n",
    "                report = \"\"\n",
    "            dfs.append(pd.DataFrame([{\n",
    "                \"Gestor\": origen, \"Seccion\": \"FAILED\", \"NE\": ne.strip(), \"Report\": report\n",
    "            }]))\n",
    "\n",
    "    # Procesa sección SUCCEEDED\n",
    "    bloques_succ = re.split(r\"=+Succeeded MML Command=+\", contenido)\n",
    "    if len(bloques_succ) > 1:\n",
    "        comandos = re.split(r\"MML Command-----LST CNOPERATORTA:;\", bloques_succ[1])\n",
    "        for cmd in comandos:\n",
    "            match_ne = re.search(r\"NE\\s*:\\s*(\\S+)\", cmd)\n",
    "            if not match_ne:\n",
    "                continue\n",
    "            ne = match_ne.group(1).strip()\n",
    "            report = re.search(r\"Report\\s*:(.*?)\\n\", cmd)\n",
    "            report = report.group(1).strip() if report else \"\"\n",
    "            if report.startswith(\"+++\"):\n",
    "                report = \"\"\n",
    "\n",
    "            if \"Local tracking area ID\" in cmd:\n",
    "                partes = cmd.splitlines()\n",
    "                headers = None\n",
    "                for i, linea in enumerate(partes):\n",
    "                    if re.match(r\"^\\s*Local tracking area ID\", linea):\n",
    "                        headers = re.split(r\"\\s{2,}\", linea.strip())\n",
    "                        data_lines = partes[i+1:]\n",
    "                        break\n",
    "                if headers:\n",
    "                    # Filtra columnas basura \"=\" y \"0\"\n",
    "                    headers = [h for h in headers if h not in [\"=\", \"0\"]]\n",
    "                    for dl in data_lines:\n",
    "                        if not dl.strip() or dl.startswith(\"---\") or dl.startswith(\"RETCODE\"):\n",
    "                            continue\n",
    "                        valores = re.split(r\"\\s{2,}\", dl.strip())\n",
    "                        if len(valores) <= 1:\n",
    "                            continue\n",
    "                        fila = dict(zip(headers, valores[:len(headers)]))\n",
    "                        fila.update({\"Gestor\": origen, \"Seccion\": \"SUCCEEDED\", \"NE\": ne, \"Report\": report})\n",
    "                        dfs.append(pd.DataFrame([fila]))\n",
    "            else:\n",
    "                dfs.append(pd.DataFrame([{\n",
    "                    \"Gestor\": origen, \"Seccion\": \"SUCCEEDED\", \"NE\": ne, \"Report\": report\n",
    "                }]))\n",
    "\n",
    "# Une todos los resultados en un DataFrame con todas las columnas detectadas\n",
    "df_LST_CNOPERATORTA_inicial = pd.concat(dfs, ignore_index=True).fillna(\"\")\n",
    "\n",
    "# --- Limpieza cruzada ---\n",
    "# Sitios presentes en SUCCEEDED\n",
    "sitios_succ = set(df_LST_CNOPERATORTA_inicial.loc[\n",
    "    df_LST_CNOPERATORTA_inicial[\"Seccion\"] == \"SUCCEEDED\", \"NE\"\n",
    "])\n",
    "\n",
    "# Elimina elementos FAILED\n",
    "df_LST_CNOPERATORTA_inicial = df_LST_CNOPERATORTA_inicial[\n",
    "    ~(\n",
    "        (df_LST_CNOPERATORTA_inicial[\"Seccion\"] == \"FAILED\") & (\n",
    "            # Regla 1: Sitio en SUCCEEDED + Local tracking area ID vacía/nula\n",
    "            (\n",
    "                df_LST_CNOPERATORTA_inicial[\"NE\"].isin(sitios_succ) &\n",
    "                (\n",
    "                    df_LST_CNOPERATORTA_inicial[\"Local tracking area ID\"].astype(\"string\").isna() |\n",
    "                    df_LST_CNOPERATORTA_inicial[\"Local tracking area ID\"].astype(\"string\").str.strip().eq(\"\")\n",
    "                )\n",
    "            )\n",
    "            |\n",
    "            # Regla 2: Report vacío/nulo\n",
    "            (\n",
    "                df_LST_CNOPERATORTA_inicial[\"Report\"].astype(\"string\").isna() |\n",
    "                df_LST_CNOPERATORTA_inicial[\"Report\"].astype(\"string\").str.strip().eq(\"\")\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Renombra columnas\n",
    "df_LST_CNOPERATORTA_inicial.rename(columns={\"NE\": \"Sitio\", \"Tracking area code\":\"TAC\"}, inplace=True)\n",
    "\n",
    "# Crea campo llaveTAC (Sitio + Local tracking area ID)\n",
    "df_LST_CNOPERATORTA_inicial[\"llaveTAC\"] = (\n",
    "    df_LST_CNOPERATORTA_inicial[\"Sitio\"].astype(\"string\").str.strip()\n",
    "      .str.cat(df_LST_CNOPERATORTA_inicial[\"Local tracking area ID\"].astype(\"string\").str.strip(),\n",
    "               sep=\"\", na_rep=None)\n",
    ")\n",
    "\n",
    "# Convierte TAC a string\n",
    "df_LST_CNOPERATORTA_inicial[\"TAC\"] = df_LST_CNOPERATORTA_inicial[\"TAC\"].apply(str)\n",
    "\n",
    "# Exporta a Excel\n",
    "#salida = os.path.join(ruta_destino, f\"LST_CNOPERATORTA_{fecha_ejecucion}.xlsx\")\n",
    "#df_LST_CNOPERATORTA_inicial.to_excel(salida, index=False, engine=\"openpyxl\")"
   ],
   "id": "8f867431c9ce3046",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T22:31:36.260472Z",
     "start_time": "2026-02-11T22:31:21.878709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MA. 20250930\n",
    "### LST CNOPERATOR. Lectura y unificación (con limpieza cruzada) ###\n",
    "\n",
    "# Archivos\n",
    "archivo_prefijo = \"MML_Task_Result_LST CNOPERATOR_c\"\n",
    "archivos_txt = glob.glob(os.path.join(ruta_carpeta, f\"{archivo_prefijo}*.txt\"))\n",
    "\n",
    "# Inicializa lista de DataFrames\n",
    "dfs = []\n",
    "\n",
    "for archivo in archivos_txt:\n",
    "    with open(archivo, \"r\", encoding=\"utf-8\") as f:\n",
    "        contenido = f.read()\n",
    "\n",
    "    # Origen reducido, p.ej. 'LST CNOPERATOR_c1'\n",
    "    origen_match = re.search(r\"(LST CNOPERATOR_c\\d+)\", os.path.basename(archivo))\n",
    "    origen = origen_match.group(1) if origen_match else os.path.basename(archivo)\n",
    "\n",
    "    # --------- FAILED ---------\n",
    "    bloques_failed = re.split(r\"=+Failed MML Command=+\", contenido)\n",
    "    if len(bloques_failed) > 1:\n",
    "        fallidos = re.findall(r\"NE\\s*:\\s*(.*?)\\nReport\\s*:\\s*(.*?)(?:\\n|$)\", bloques_failed[1], re.DOTALL)\n",
    "        for ne, report in fallidos:\n",
    "            report = report.strip()\n",
    "            if report.startswith(\"+++\"):\n",
    "                report = \"\"\n",
    "            # Inicializamos CN Operator ID vacío para poder filtrar después\n",
    "            dfs.append(pd.DataFrame([{\n",
    "                \"Gestor\": origen, \"Seccion\": \"FAILED\", \"NE\": ne.strip(), \"Report\": report,\n",
    "                \"CN Operator ID\": \"\"\n",
    "            }]))\n",
    "\n",
    "    # --------- SUCCEEDED ---------\n",
    "    bloques_succ = re.split(r\"=+Succeeded MML Command=+\", contenido)\n",
    "    if len(bloques_succ) > 1:\n",
    "        comandos = re.split(r\"MML Command-----LST CNOPERATOR:;\", bloques_succ[1])\n",
    "        for cmd in comandos:\n",
    "            match_ne = re.search(r\"NE\\s*:\\s*(\\S+)\", cmd)\n",
    "            if not match_ne:\n",
    "                continue\n",
    "            ne = match_ne.group(1).strip()\n",
    "            report_m = re.search(r\"Report\\s*:(.*?)\\n\", cmd)\n",
    "            report = report_m.group(1).strip() if report_m else \"\"\n",
    "            if report.startswith(\"+++\"):\n",
    "                report = \"\"\n",
    "\n",
    "            # Detecta tabla\n",
    "            if \"CN Operator\" in cmd:\n",
    "                partes = cmd.splitlines()\n",
    "                headers = None\n",
    "                for i, linea in enumerate(partes):\n",
    "                    if re.match(r\"^\\s*CN Operator ID\", linea):\n",
    "                        headers = re.split(r\"\\s{2,}\", linea.strip())\n",
    "                        data_lines = partes[i+1:]\n",
    "                        break\n",
    "                if headers:\n",
    "                    for dl in data_lines:\n",
    "                        if not dl.strip() or dl.startswith(\"---\") or dl.startswith(\"RETCODE\"):\n",
    "                            continue\n",
    "                        valores = re.split(r\"\\s{2,}\", dl.strip())\n",
    "                        if len(valores) <= 1:\n",
    "                            continue\n",
    "                        fila = dict(zip(headers, valores))\n",
    "                        fila.update({\"Gestor\": origen, \"Seccion\": \"SUCCEEDED\", \"NE\": ne, \"Report\": report})\n",
    "                        dfs.append(pd.DataFrame([fila]))\n",
    "            else:\n",
    "                # Sin tabla -> registro de SUCCEEDED sin detalle\n",
    "                dfs.append(pd.DataFrame([{\n",
    "                    \"Gestor\": origen, \"Seccion\": \"SUCCEEDED\", \"NE\": ne, \"Report\": report,\n",
    "                    \"CN Operator ID\": \"\"\n",
    "                }]))\n",
    "\n",
    "# Une todos los resultados\n",
    "df_LST_CNOPERATOR_inicial = pd.concat(dfs, ignore_index=True).fillna(\"\")\n",
    "\n",
    "# Asegura columna clave para la limpieza (por si algún archivo no la trajo)\n",
    "if \"CN Operator ID\" not in df_LST_CNOPERATOR_inicial.columns:\n",
    "    df_LST_CNOPERATOR_inicial[\"CN Operator ID\"] = \"\"\n",
    "\n",
    "# --- Limpieza cruzada ---\n",
    "# Sitios presentes en SUCCEEDED\n",
    "sitios_succ = set(df_LST_CNOPERATOR_inicial.loc[\n",
    "    df_LST_CNOPERATOR_inicial[\"Seccion\"] == \"SUCCEEDED\", \"NE\"\n",
    "])\n",
    "\n",
    "# Elimina FAILED cuando mismo Sitio está en SUCCEEDED y CN Operator ID está vacío/nulo\n",
    "df_LST_CNOPERATOR_inicial = df_LST_CNOPERATOR_inicial[\n",
    "    ~(\n",
    "        (df_LST_CNOPERATOR_inicial[\"Seccion\"] == \"FAILED\") &\n",
    "        (df_LST_CNOPERATOR_inicial[\"NE\"].isin(sitios_succ)) &\n",
    "        (df_LST_CNOPERATOR_inicial[\"CN Operator ID\"].isna() | (df_LST_CNOPERATOR_inicial[\"CN Operator ID\"] == \"\"))\n",
    "    )\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Renombra NE -> Sitio\n",
    "df_LST_CNOPERATOR_inicial.rename(columns={\"NE\": \"Sitio\"}, inplace=True)\n",
    "\n",
    "# Exporta a Excel\n",
    "#salida = os.path.join(ruta_destino, f\"LST_CNOPERATOR_{fecha_ejecucion}.xlsx\")\n",
    "#df_LST_CNOPERATOR_inicial.to_excel(salida, index=False, engine=\"openpyxl\")"
   ],
   "id": "4e3383ff39477ff",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T22:32:25.224222Z",
     "start_time": "2026-02-11T22:31:36.270783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MA. 20251001\n",
    "### DSP S1INTARFACE. Lectura y unificación ###\n",
    "\n",
    "# Archivos\n",
    "archivo_prefijo = \"MML_Task_Result_DSP S1INTARFACE_c\"\n",
    "archivos_txt = glob.glob(os.path.join(ruta_carpeta, f\"{archivo_prefijo}*.txt\"))\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for archivo in archivos_txt:\n",
    "    with open(archivo, \"r\", encoding=\"utf-8\") as f:\n",
    "        contenido = f.read()\n",
    "\n",
    "    # Nombre de archivo reducido (ej: DSP S1INTARFACE_c1)\n",
    "    origen_match = re.search(r\"(DSP S1INTARFACE_c\\d+)\", os.path.basename(archivo))\n",
    "    origen = origen_match.group(1) if origen_match else os.path.basename(archivo)\n",
    "\n",
    "    # --------- FAILED ---------\n",
    "    bloques_failed = re.split(r\"=+Failed MML Command=+\", contenido)\n",
    "    if len(bloques_failed) > 1:\n",
    "        fallidos = re.findall(r\"NE\\s*:\\s*(.*?)\\nReport\\s*:\\s*(.*?)(?:\\n|$)\", bloques_failed[1], re.DOTALL)\n",
    "        for ne, report in fallidos:\n",
    "            report = report.strip()\n",
    "            if report.startswith(\"+++\"):\n",
    "                report = \"\"\n",
    "            dfs.append(pd.DataFrame([{\n",
    "                \"Gestor\": origen, \"Seccion\": \"FAILED\", \"NE\": ne.strip(), \"Report\": report,\n",
    "                \"S1 Interface ID\": \"\"  # inicializado vacío porque no aparece en FAILED\n",
    "            }]))\n",
    "\n",
    "    # --------- SUCCEEDED ---------\n",
    "    bloques_succ = re.split(r\"=+Succeeded MML Command=+\", contenido)\n",
    "    if len(bloques_succ) > 1:\n",
    "        comandos = re.split(r\"MML Command-----DSP S1IN(?:TER|TAR)FACE:;\", bloques_succ[1])\n",
    "        #comandos = re.split(r\"MML Command-----DSP S1INTARFACE:;\", bloques_succ[1])\n",
    "        for cmd in comandos:\n",
    "            match_ne = re.search(r\"NE\\s*:\\s*(\\S+)\", cmd)\n",
    "            if not match_ne:\n",
    "                continue\n",
    "            ne = match_ne.group(1).strip()\n",
    "            report = re.search(r\"Report\\s*:(.*?)\\n\", cmd)\n",
    "            report = report.group(1).strip() if report else \"\"\n",
    "            if report.startswith(\"+++\"):\n",
    "                report = \"\"\n",
    "\n",
    "            # Busca tabla\n",
    "            if \"Display S1 Interface Information\" in cmd:\n",
    "                partes = cmd.split(\"Display S1 Interface Information\", 1)[1].splitlines()\n",
    "                headers = None\n",
    "                for i, linea in enumerate(partes):\n",
    "                    if re.match(r\"^\\s*S1 Interface ID\", linea):\n",
    "                        headers = re.split(r\"\\s{2,}\", linea.strip())\n",
    "                        data_lines = partes[i+1:]\n",
    "                        break\n",
    "                if headers:\n",
    "                    for dl in data_lines:\n",
    "                        if not dl.strip() or dl.startswith(\"---\") or dl.startswith(\"RETCODE\"):\n",
    "                            continue\n",
    "                        valores = re.split(r\"\\s{2,}\", dl.strip())\n",
    "                        if len(valores) <= 1:\n",
    "                            continue\n",
    "                        fila = dict(zip(headers, valores))\n",
    "                        fila.update({\n",
    "                            \"Gestor\": origen, \"Seccion\": \"SUCCEEDED\",\n",
    "                            \"NE\": ne, \"Report\": report\n",
    "                        })\n",
    "                        dfs.append(pd.DataFrame([fila]))\n",
    "            else:\n",
    "                dfs.append(pd.DataFrame([{\n",
    "                    \"Gestor\": origen, \"Seccion\": \"SUCCEEDED\",\n",
    "                    \"NE\": ne, \"Report\": report,\n",
    "                    \"S1 Interface ID\": \"\"  # inicializado vacío si no hay tabla\n",
    "                }]))\n",
    "\n",
    "# Unifica en DataFrame\n",
    "df_DSP_S1INTARFACE_inicial = pd.concat(dfs, ignore_index=True).fillna(\"\")\n",
    "\n",
    "# --- Limpieza cruzada ---\n",
    "# Sitios presentes en SUCCEEDED\n",
    "sitios_succ = set(df_DSP_S1INTARFACE_inicial.loc[\n",
    "    df_DSP_S1INTARFACE_inicial[\"Seccion\"] == \"SUCCEEDED\", \"NE\"\n",
    "])\n",
    "\n",
    "# Elimina registros FAILED cuando el mismo Sitio está en SUCCEEDED y S1 Interface ID vacío/nulo\n",
    "df_DSP_S1INTARFACE_inicial = df_DSP_S1INTARFACE_inicial[\n",
    "    ~(\n",
    "        (df_DSP_S1INTARFACE_inicial[\"Seccion\"] == \"FAILED\") &\n",
    "        (df_DSP_S1INTARFACE_inicial[\"NE\"].isin(sitios_succ)) &\n",
    "        (df_DSP_S1INTARFACE_inicial[\"S1 Interface ID\"].isna() |\n",
    "         (df_DSP_S1INTARFACE_inicial[\"S1 Interface ID\"] == \"\"))\n",
    "    )\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Descarta registros cuando \"S1 Interface ID\" contiene \"+++\" o \"O&M\"\n",
    "df_DSP_S1INTARFACE_inicial = df_DSP_S1INTARFACE_inicial[\n",
    "    ~df_DSP_S1INTARFACE_inicial[\"S1 Interface ID\"].astype(str).str.contains(r\"\\+\\+\\+|O&M\", na=False)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Renombramiento columnas\n",
    "df_DSP_S1INTARFACE_inicial.rename(columns={\"NE\": \"Sitio\"}, inplace=True)\n",
    "\n",
    "# Cuenta la cantidad de veces que se repite CN Operator ID por cada Sitio\n",
    "df_MMEs = (\n",
    "    df_DSP_S1INTARFACE_inicial\n",
    "    .groupby([\"Sitio\", \"CN Operator ID\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"MMEs\")\n",
    ")\n",
    "\n",
    "\n",
    "# Exporta a Excel\n",
    "#salida = os.path.join(ruta_destino, f\"DSP_S1INTARFACE_{fecha_ejecucion}.xlsx\")\n",
    "#df_MMEs.to_excel(salida, index=False, engine=\"openpyxl\")"
   ],
   "id": "d0b076f56e5e08d9",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T22:36:09.713222Z",
     "start_time": "2026-02-11T22:32:25.240059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MA. 20250930\n",
    "### LST CELLOP. Lectura y unificación ###\n",
    "\n",
    "# Archivos\n",
    "archivo_prefijo = \"MML_Task_Result_LST CELLOP_c\"\n",
    "archivos_txt = glob.glob(os.path.join(ruta_carpeta, f\"{archivo_prefijo}*.txt\"))\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for archivo in archivos_txt:\n",
    "    with open(archivo, \"r\", encoding=\"utf-8\") as f:\n",
    "        contenido = f.read()\n",
    "\n",
    "    # Nombre de archivo reducido\n",
    "    origen_match = re.search(r\"(LST CELLOP_c\\d+)\", os.path.basename(archivo))\n",
    "    origen = origen_match.group(1) if origen_match else os.path.basename(archivo)\n",
    "\n",
    "    # --------- FAILED ---------\n",
    "    bloques_failed = re.split(r\"=+Failed MML Command=+\", contenido)\n",
    "    if len(bloques_failed) > 1:\n",
    "        fallidos = re.findall(r\"NE\\s*:\\s*(.*?)\\nReport\\s*:\\s*(.*?)(?:\\n|$)\", bloques_failed[1], re.DOTALL)\n",
    "        for ne, report in fallidos:\n",
    "            report = report.strip()\n",
    "            if report.startswith(\"+++\"):\n",
    "                report = \"\"\n",
    "            dfs.append(pd.DataFrame([{\n",
    "                \"Gestor\": origen, \"Seccion\": \"FAILED\", \"NE\": ne.strip(), \"Report\": report,\n",
    "                \"Local cell ID\": \"\"  # se inicializa vacío porque no se listan en FAILED\n",
    "            }]))\n",
    "\n",
    "    # --------- SUCCEEDED ---------\n",
    "    bloques_succ = re.split(r\"=+Succeeded MML Command=+\", contenido)\n",
    "    if len(bloques_succ) > 1:\n",
    "\n",
    "        # --- Agrupa fragmentos por NE (maneja \"To be continued...\") ---\n",
    "        comandos_raw = re.split(r\"MML Command-----LST CELLOP:;\", bloques_succ[1])\n",
    "        bloques_por_ne = {}\n",
    "        for cmd in comandos_raw:\n",
    "            match_ne = re.search(r\"NE\\s*:\\s*(\\S+)\", cmd)\n",
    "            if match_ne:\n",
    "                ne = match_ne.group(1).strip()\n",
    "                bloques_por_ne.setdefault(ne, \"\")\n",
    "                bloques_por_ne[ne] += cmd  # concatena fragmentos continuados\n",
    "\n",
    "        # --- Procesa cada NE completo ---\n",
    "        for ne, cmd in bloques_por_ne.items():\n",
    "            report = re.search(r\"Report\\s*:(.*?)\\n\", cmd)\n",
    "            report = report.group(1).strip() if report else \"\"\n",
    "            if report.startswith(\"+++\"):\n",
    "                report = \"\"\n",
    "\n",
    "            # --- Busca secciones de \"List Cell Operator\" ---\n",
    "            if \"List Cell Operator\" in cmd:\n",
    "                partes = cmd.split(\"List Cell Operator\")\n",
    "                for parte in partes[1:]:  # puede haber más de una tabla por NE\n",
    "                    lineas = parte.splitlines()\n",
    "                    headers = None\n",
    "                    for i, linea in enumerate(lineas):\n",
    "                        if re.match(r\"^\\s*Local cell ID\", linea):\n",
    "                            headers = re.split(r\"\\s{2,}\", linea.strip())\n",
    "                            data_lines = lineas[i+1:]\n",
    "                            break\n",
    "                    if headers:\n",
    "                        for dl in data_lines:\n",
    "                            if not dl.strip() or dl.startswith(\"---\") or dl.startswith(\"RETCODE\"):\n",
    "                                continue\n",
    "                            valores = re.split(r\"\\s{2,}\", dl.strip())\n",
    "                            if len(valores) <= 1:\n",
    "                                continue\n",
    "                            fila = dict(zip(headers, valores))\n",
    "                            fila.update({\"Gestor\": origen, \"Seccion\": \"SUCCEEDED\", \"NE\": ne, \"Report\": report})\n",
    "                            dfs.append(pd.DataFrame([fila]))\n",
    "            else:\n",
    "                dfs.append(pd.DataFrame([{\n",
    "                    \"Gestor\": origen, \"Seccion\": \"SUCCEEDED\", \"NE\": ne, \"Report\": report,\n",
    "                    \"Local cell ID\": \"\"\n",
    "                }]))\n",
    "\n",
    "# --- Unificación en DataFrame ---\n",
    "df_LST_CELLOP_inicial = pd.concat(dfs, ignore_index=True).fillna(\"\")\n",
    "\n",
    "# --- Limpieza cruzada ---\n",
    "# Sitios presentes en SUCCEEDED\n",
    "sitios_succ = set(df_LST_CELLOP_inicial.loc[\n",
    "    df_LST_CELLOP_inicial[\"Seccion\"] == \"SUCCEEDED\", \"NE\"\n",
    "])\n",
    "\n",
    "# Elimina registros FAILED cuando el mismo Sitio está en SUCCEEDED y Local Cell ID está vacío/nulo\n",
    "df_LST_CELLOP_inicial = df_LST_CELLOP_inicial[\n",
    "    ~(\n",
    "        (df_LST_CELLOP_inicial[\"Seccion\"] == \"FAILED\") &\n",
    "        (df_LST_CELLOP_inicial[\"NE\"].isin(sitios_succ)) &\n",
    "        (df_LST_CELLOP_inicial[\"Local cell ID\"].isna() | (df_LST_CELLOP_inicial[\"Local cell ID\"] == \"\"))\n",
    "    )\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# --- Descarta registros cuando \"Local cell ID\" contiene \"+++\" o \"O&M\" ---\n",
    "df_LST_CELLOP_inicial = (\n",
    "    df_LST_CELLOP_inicial[\n",
    "        ~df_LST_CELLOP_inicial[\"Local cell ID\"].astype(str).str.contains(r\"\\+\\+\\+|O&M\", na=False)\n",
    "    ].reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# --- Renombramiento columnas ---\n",
    "df_LST_CELLOP_inicial.rename(columns={\"NE\": \"Sitio\", \"Local cell ID\": \"Local Cell ID\"}, inplace=True)\n",
    "\n",
    "# --- Exporta a Excel ---\n",
    "#salida = os.path.join(ruta_destino, f\"LST_CELLOP_{fecha_ejecucion}.xlsx\")\n",
    "#df_LST_CELLOP_inicial.to_excel(salida, index=False, engine=\"openpyxl\")\n"
   ],
   "id": "43b9bdb2d5280d92",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T22:36:14.975681Z",
     "start_time": "2026-02-11T22:36:10.396674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MA. 20251001\n",
    "### DSP CELL. Lectura y unificación robusta (vertical + horizontal) ###\n",
    "\n",
    "# Archivos\n",
    "archivo_prefijo = \"MML_Task_Result_DSP CELL_c\"\n",
    "archivos_txt = glob.glob(os.path.join(ruta_carpeta, f\"{archivo_prefijo}*.txt\"))\n",
    "\n",
    "# Columnas objetivo (FAILED + SUCCEEDED)\n",
    "columnas = [\n",
    "    \"Gestor\", \"Seccion\", \"NE\", \"Report\",\n",
    "    \"Local Cell ID\", \"Cell Name\", \"Cell instance state\",\n",
    "    \"Reason for latest state change\", \"Cell latest setup time\",\n",
    "    \"Cell latest setup operate type\", \"Cell latest remove time\",\n",
    "    \"Cell latest remove operate type\", \"Cell power save state\",\n",
    "    \"Symbol shutdown state\", \"Anti Interfere Status in High Speed Scenario\",\n",
    "    \"Primary BBP information\", \"Cell topology type\",\n",
    "    \"Maximum transmit power(0.1dBm)\", \"Cell PLMN Info\",\n",
    "    \"Time of Cell State Change to Unavailable\",\n",
    "    \"Time of Last Config Before Cell Unavailable\"\n",
    "]\n",
    "\n",
    "# --- Parseo robusto para \"Display dynamic parameters of cells\" ---\n",
    "def _parse_tabla(cmd_text: str):\n",
    "    \"\"\"\n",
    "    Devuelve una lista de dicts. Soporta:\n",
    "    1) Tabla horizontal (encabezado + filas con números)\n",
    "    2) Tabla vertical (clave = valor)\n",
    "    \"\"\"\n",
    "    m = re.search(\n",
    "        r\"Display dynamic parameters of cells\\s*-+\\s*\\n(.*?)(?=\\n\\s*\\(Number of results|\\n---\\s*END|\\nMML Command-----|$)\",\n",
    "        cmd_text, flags=re.S\n",
    "    )\n",
    "    if not m:\n",
    "        return []\n",
    "\n",
    "    bloque = m.group(1)\n",
    "    lineas = [ln.rstrip(\"\\n\") for ln in bloque.splitlines() if ln.strip()]\n",
    "    if not lineas:\n",
    "        return []\n",
    "\n",
    "    # --- Variante 2: clave = valor ---\n",
    "    kv_pairs = []\n",
    "    for ln in lineas:\n",
    "        mm = re.match(r\"^\\s*(.*?)\\s*=\\s*(.*)$\", ln)\n",
    "        if mm:\n",
    "            kv_pairs.append((mm.group(1).strip(), mm.group(2).strip()))\n",
    "    if kv_pairs and not any(re.match(r\"^\\d+\\s\", ln.strip()) for ln in lineas):\n",
    "        return [{k: v for k, v in kv_pairs}]\n",
    "\n",
    "    # --- Variante 1: tabla horizontal ---\n",
    "    header_line = lineas[0]\n",
    "    tokens = re.split(r\"\\s{2,}\", header_line.strip())\n",
    "\n",
    "    spans, pos = [], 0\n",
    "    for tok in tokens:\n",
    "        idx = header_line.find(tok, pos)\n",
    "        spans.append((tok, idx))\n",
    "        pos = idx + len(tok)\n",
    "    spans = [(name, start, (spans[i+1][1] if i+1 < len(spans) else None))\n",
    "             for i, (name, start) in enumerate(spans)]\n",
    "\n",
    "    data_lines = [ln for ln in lineas[1:] if re.match(r\"^\\d+\", ln.strip())]\n",
    "    filas = []\n",
    "    for ln in data_lines:\n",
    "        rec = {name: (ln[start:end].rstrip() if end is not None else ln[start:].rstrip())\n",
    "               for name, start, end in spans}\n",
    "        filas.append(rec)\n",
    "    return filas\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for archivo in archivos_txt:\n",
    "    with open(archivo, \"r\", encoding=\"utf-8\") as f:\n",
    "        contenido = f.read()\n",
    "\n",
    "    # Gestor: p.ej. 'DSP CELL_c1'\n",
    "    gestor = os.path.basename(archivo)\n",
    "    gestor = \"_\".join(re.sub(r\"^MML_Task_Result_\", \"\", gestor).split(\"_\")[0:2])\n",
    "\n",
    "    # -------- FAILED --------\n",
    "    m_failed = re.search(r\"=+Failed MML Command=+\\s*(.*?)(?=\\n=+)\", contenido, flags=re.S)\n",
    "    if m_failed:\n",
    "        bloque_failed = m_failed.group(1)\n",
    "        for seg in re.split(r\"MML Command-----DSP CELL:;\", bloque_failed):\n",
    "            if not seg.strip():\n",
    "                continue\n",
    "            m_ne = re.search(r\"NE\\s*:\\s*(.*)\", seg)\n",
    "            m_rp = re.search(r\"Report\\s*:\\s*(.*?)(?:\\n---\\s*END|\\n\\n|$)\", seg, flags=re.S)\n",
    "            if not m_ne:\n",
    "                continue\n",
    "            ne = m_ne.group(1).strip()\n",
    "            report = (m_rp.group(1).strip() if m_rp else \"\")\n",
    "            if report.startswith(\"+++\"):\n",
    "                report = \"\"\n",
    "            dfs.append({\"Gestor\": gestor, \"Seccion\": \"FAILED\", \"NE\": ne, \"Report\": report})\n",
    "\n",
    "    # -------- SUCCEEDED --------\n",
    "    m_succ = re.search(r\"=+Succeeded MML Command=+\\s*(.*)\", contenido, flags=re.S)\n",
    "    if m_succ:\n",
    "        bloque_succ = m_succ.group(1)\n",
    "        for cmd in re.split(r\"MML Command-----DSP CELL:;\", bloque_succ):\n",
    "            if not cmd.strip():\n",
    "                continue\n",
    "            m_ne = re.search(r\"NE\\s*:\\s*(\\S+)\", cmd)\n",
    "            ne = m_ne.group(1).strip() if m_ne else \"\"\n",
    "            m_rp = re.search(r\"Report\\s*:(.*?)(?:\\n---\\s*END|\\n\\n|$)\", cmd, flags=re.S)\n",
    "            report = (m_rp.group(1).strip() if m_rp else \"\")\n",
    "            if report.startswith(\"+++\"):\n",
    "                report = \"\"\n",
    "\n",
    "            # Parse tabla si existe\n",
    "            recs = _parse_tabla(cmd)\n",
    "            if recs:\n",
    "                for rec in recs:\n",
    "                    fila = {\"Gestor\": gestor, \"Seccion\": \"SUCCEEDED\", \"NE\": ne, \"Report\": report}\n",
    "                    fila.update(rec)\n",
    "                    dfs.append(fila)\n",
    "            else:\n",
    "                dfs.append({\"Gestor\": gestor, \"Seccion\": \"SUCCEEDED\", \"NE\": ne, \"Report\": report})\n",
    "\n",
    "# DataFrame final, asegura columnas\n",
    "df_DSP_CELL_inicial = pd.DataFrame(dfs)\n",
    "for col in columnas:\n",
    "    if col not in df_DSP_CELL_inicial.columns:\n",
    "        df_DSP_CELL_inicial[col] = \"\"\n",
    "df_DSP_CELL_inicial = df_DSP_CELL_inicial[columnas].fillna(\"\")\n",
    "\n",
    "# NE -> Sitio\n",
    "df_DSP_CELL_inicial.rename(columns={\"NE\": \"Sitio\"}, inplace=True)\n",
    "\n",
    "# Comentarios\n",
    "df_DSP_CELL_inicial[\"Comentarios\"] = np.where(\n",
    "    #(df_DSP_CELL_inicial[\"Seccion\"] == \"FAILED\") &\n",
    "    (df_DSP_CELL_inicial[\"Report\"].str.lower() == \"ne is not connected.\"),\n",
    "    \"Offline\",\n",
    "    \"\"\n",
    ")\n",
    "\n",
    "# Exporta a Excel\n",
    "#salida = os.path.join(ruta_destino, f\"DSP_CELL_{fecha_ejecucion}.xlsx\")\n",
    "#df_DSP_CELL_inicial.to_excel(salida, index=False, engine=\"openpyxl\")"
   ],
   "id": "c7c8ca02a483f846",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T22:37:59.573747Z",
     "start_time": "2026-02-11T22:36:14.986733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MA. 20251002.\n",
    "### Información archivo mes anterior ###\n",
    "\n",
    "# Sufijo mes anterior\n",
    "today = date.today()\n",
    "prev_year  = today.year if today.month > 1 else today.year - 1\n",
    "prev_month = today.month - 1 or 12\n",
    "yyyymm = f\"{prev_year}{prev_month:02d}\"\n",
    "fecha_hoy = today.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "# Busca archivo\n",
    "ruta: str = ruta_destino  # --> MA. Por Definir.\n",
    "archivo = f\"All_Huawei_4G_{yyyymm}.xlsx\"\n",
    "path = os.path.join(ruta, archivo)\n",
    "\n",
    "if os.path.exists(path):\n",
    "    # Fecha del día y de creación de archivo anterior\n",
    "    wb = load_workbook(path, read_only=True)\n",
    "    props = wb.properties\n",
    "    fecha_creacion = props.created.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "    # Info archivo anterior\n",
    "    df_All_Huawei_4G_Anterior = pd.read_excel(path)\n",
    "    df_All_Huawei_4G_Anterior = df_All_Huawei_4G_Anterior.drop(columns=\"Comentarios\")\n",
    "\n",
    "    # Agrega sufijo _ant a todas las columnas\n",
    "    df_All_Huawei_4G_Anterior = df_All_Huawei_4G_Anterior.add_suffix(\"_ant\")\n",
    "    # Convierte valor a string\n",
    "    df_All_Huawei_4G_Anterior[\"Local Cell ID_ant\"] = df_All_Huawei_4G_Anterior[\"Local Cell ID_ant\"].apply(str)\n",
    "\n",
    "    # Info TAC y TAL\n",
    "    df_All_Huawei_4G_Ant_TAC = pd.read_excel(\n",
    "        path,\n",
    "        usecols=[\"TAC\", \"TAL\"]\n",
    "    )\n",
    "\n",
    "    # TAC -> TAC_ant\n",
    "    df_All_Huawei_4G_Ant_TAC.rename(columns={\"TAC\": \"TAC_ant\"}, inplace=True)\n",
    "    # Convierte valor a string\n",
    "    df_All_Huawei_4G_Ant_TAC[\"TAC_ant\"] = df_All_Huawei_4G_Ant_TAC[\"TAC_ant\"].apply(str)\n",
    "\n",
    "    # Trae solo registros únicos\n",
    "    df_All_Huawei_4G_Ant_TAC = (\n",
    "    df_All_Huawei_4G_Ant_TAC\n",
    "    .drop_duplicates(subset=[\"TAC_ant\"])   # elimina duplicados\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # No existe archivo, crea DataFrame vacío con columnas necesarias\n",
    "    df_All_Huawei_4G_Anterior = pd.DataFrame(columns=[\"Sitio_ant\", \"Local Cell ID_ant\"])\n",
    "    df_All_Huawei_4G_Ant_TAC = pd.DataFrame(columns=[\"TAC_ant\", \"TAL\"])\n",
    "\n",
    "# Exporta a Excel\n",
    "#salida = os.path.join(ruta_destino, f\"All_Huawei_4G_Anterior_{fecha_ejecucion}.xlsx\")\n",
    "#df_All_Huawei_4G_Anterior.to_excel(salida, index=False, engine=\"openpyxl\")"
   ],
   "id": "ccb9829af92dd8a5",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T22:41:16.084595Z",
     "start_time": "2026-02-11T22:37:59.616233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MA. 20251002.\n",
    "### Información del EPT ###\n",
    "\n",
    "# Prefijo del archivo\n",
    "prefijo_ept = \"EPT_ATT_UMTS_LTE_\"\n",
    "\n",
    "# Busca archivo que empiece con el prefijo\n",
    "archivo = glob.glob(os.path.join(ruta_ept, f\"{prefijo_ept}*.xlsx\"))\n",
    "\n",
    "# Verifica si se encontró archivo\n",
    "if archivo:\n",
    "    archivo_encontrado = archivo[0]\n",
    "    nombre_archivo = os.path.basename(archivo_encontrado)\n",
    "\n",
    "    # Lista de hojas a leer\n",
    "    hojas = [\n",
    "        \"EPT_3G_LTE_OUTDOOR\",\n",
    "        \"PLAN_OUTDOOR\",\n",
    "        \"EPT_3G_LTE_INDOOR\",\n",
    "        \"PLAN_INDOOR\",\n",
    "        \"Eventos_Especiales\"\n",
    "    ]\n",
    "\n",
    "    # Lee todas las hojas y agrega el nombre de la hoja en columna\n",
    "    dfs = [\n",
    "        pd.read_excel(archivo_encontrado, sheet_name=hoja, engine=\"openpyxl\")\n",
    "        .assign(Hoja=hoja, Origen=nombre_archivo)\n",
    "        for hoja in hojas\n",
    "    ]\n",
    "\n",
    "    # Concatena todo en un solo DataFrame\n",
    "    df_EPT_inicial = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "    # Renombramiento columna(s)\n",
    "    nuevos_nombres = {\"CellName\" : \"Cell Name\", \"Latitud\" : \"LAT\", \"Longitud\" : \"LON\"}\n",
    "    df_EPT_inicial.rename(columns=nuevos_nombres, inplace=True)\n",
    "\n",
    "    # Convierte columnas totalmente numéricas (cuando aplica)\n",
    "    df_EPT_ini = df_EPT_inicial.apply(\n",
    "        lambda col: pd.to_numeric(col, errors=\"coerce\")\n",
    "        if not pd.to_numeric(col, errors=\"coerce\").isna().any()\n",
    "        else col\n",
    "    )\n",
    "\n",
    "    # EPT\n",
    "    df_EPT_unificado = df_EPT_ini[[\"Cell Name\", \"LAT\", \"LON\", \"AT&T_Site_Name\"]]\n",
    "\n",
    "# Archivo unificado en Excel, para fines de validación.\n",
    "#ruta_salida = os.path.join(ruta_destino, f\"EPT_unificado_{fecha_ejecucion}.xlsx\")\n",
    "#df_EPT_unificado.to_excel(ruta_salida, index=False)"
   ],
   "id": "ac94fae6a18da543",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T22:49:13.269016Z",
     "start_time": "2026-02-11T22:41:16.263785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MA. 20251001.\n",
    "### Creación archivo final ###\n",
    "\n",
    "## Extraemos solo las columnas requeridas de cada DataFrame.\n",
    "\n",
    "## LST_CELL\n",
    "df_LST_CELL = df_LST_CELL_inicial[[\"Gestor\", \"Seccion\", \"Sitio\", \"Local Cell ID\", \"Cell Name\",\n",
    "\"Csg indicator\", \"Uplink cyclic prefix length\", \"Downlink cyclic prefix length\",\n",
    "\"NB-IoT Cell Flag\", \"Coverage Level Type\", \"Frequency band\", \"Uplink EARFCN indication\",\n",
    "\"Uplink EARFCN\", \"Downlink EARFCN\", \"Uplink bandwidth\", \"Downlink bandwidth\",\n",
    "\"Cell ID\", \"Physical cell ID\", \"Additional spectrum emission\", \"Cell active state\",\n",
    "\"Cell admin state\", \"Cell middle block timer(min)\", \"Cell FDD TDD indication\",\n",
    "\"Subframe assignment\", \"Special subframe patterns\", \"SSP6 DwPTS Mode\", \"Cell Standby Mode\",\n",
    "\"Cell specific offset(dB)\", \"Frequency offset(dB)\", \"Root sequence index\", \"High speed flag\",\n",
    "\"Preamble format\", \"Cell radius(m)\", \"Customized bandwidth configure indicator\",\n",
    "\"Customized uplink bandwidth(0.1MHz)\", \"Customized downlink bandwidth(0.1MHz)\",\n",
    "\"Emergency Area Id indicator\", \"Emergency Area ID\", \"Ue max power allowed configure indicator\",\n",
    "\"Max transmit power allowed(dBm)\", \"Flag of Multi-RRU Cell\", \"Mode of Multi-RRU Cell\",\n",
    "\"CPRI Ethernet Compression Ratio\", \"CPRI Compression\", \"Physical Cell Number of SFN Cell\",\n",
    "\"Air Cell Flag\", \"CRS Port Number\", \"Cell transmission and reception mode\",\n",
    "\"CRS Antenna Port Mapping\", \"User label\", \"Work mode\", \"CN Operator Sharing Group ID\",\n",
    "\"Intra Frequency RAN Sharing Indication\", \"IntraFreq ANR Indication\", \"ANR Frequency Priority\",\n",
    "\"Cell Radius Start Location(m)\", \"Specified Cell Flag\", \"Downlink Punctured RB Number\",\n",
    "\"SFN Master Cell Label\", \"Multi Cell Share Mode\", \"Standby Cell SFN Recovery Time(h)\",\n",
    "\"Compact Bandwidth Control Interference Mode\", \"Uplink Punctured RB Number Offset\",\n",
    "\"Ultra High-Speed Cell Root Sequence Index\", \"Comentarios\"]]\n",
    "\n",
    "## LST_CELLOP\n",
    "df_LST_CELLOP = df_LST_CELLOP_inicial[[\"Sitio\", \"Local Cell ID\", \"Local tracking area ID\"]]\n",
    "\n",
    "# Obtiene valor máximo de \"Local tracking area ID\"\n",
    "df_LST_CELLOP = (\n",
    "    df_LST_CELLOP\n",
    "    .loc[df_LST_CELLOP[\"Local Cell ID\"].notna() & (df_LST_CELLOP[\"Local Cell ID\"] != \"\")]\n",
    "    .groupby([\"Sitio\", \"Local Cell ID\"], as_index=False)[\"Local tracking area ID\"]\n",
    "    .max()\n",
    ")\n",
    "\n",
    "# \"Local tracking area ID\" -> \"Cellda con MOCN\"\n",
    "df_LST_CELLOP.rename(columns={\"Local tracking area ID\": \"Cellda con MOCN\"}, inplace=True)\n",
    "\n",
    "## LST_CNOPERATOR\n",
    "df_LST_CNOPERATOR = df_LST_CNOPERATOR_inicial[[\"Sitio\", \"CN Operator ID\"]]\n",
    "\n",
    "# Obtiene valor máximo de \"CN Operator ID\"\n",
    "df_LST_CNOPERATOR = (\n",
    "    df_LST_CNOPERATOR\n",
    "    .loc[df_LST_CNOPERATOR[\"CN Operator ID\"].notna() & (df_LST_CNOPERATOR[\"CN Operator ID\"] != \"\")]\n",
    "    .groupby([\"Sitio\"], as_index=False)[\"CN Operator ID\"]\n",
    "    .max()\n",
    ")\n",
    "\n",
    "# \"CN Operator ID\" -> \"MOCN Configurado Sitio\"\n",
    "df_LST_CNOPERATOR.rename(columns={\"CN Operator ID\": \"MOCN Configurado Sitio\"}, inplace=True)\n",
    "\n",
    "## DSP_S1INTARFACE\n",
    "df_DSP_S1INTARFACE = df_DSP_S1INTARFACE_inicial[[\"Sitio\", \"S1 Interface ID\", \"CN Operator ID\", \"S1 Interface Fault Reason\"]]\n",
    "\n",
    "# Obtiene valor máximo de \"CN Operator ID\"\n",
    "df_DSP_S1INTARFACE = (\n",
    "    df_DSP_S1INTARFACE\n",
    "    .loc[df_DSP_S1INTARFACE[\"CN Operator ID\"].notna() & (df_DSP_S1INTARFACE[\"CN Operator ID\"] != \"\")]\n",
    "    .groupby([\"Sitio\"], as_index=False)[[\"CN Operator ID\", \"S1 Interface ID\", \"S1 Interface Fault Reason\"]]\n",
    "    .max()\n",
    ")\n",
    "# \"CN Operator ID\" -> \"S1 MOCN\"\n",
    "df_DSP_S1INTARFACE.rename(columns={\"CN Operator ID\": \"S1 MOCN\"}, inplace=True)\n",
    "## DSP_OPERATORTA\n",
    "df_LST_CNOPERATORTA = df_LST_CNOPERATORTA_inicial[[\"llaveTAC\", \"TAC\"]]\n",
    "## DSP_CELL\n",
    "df_DSP_CELL = df_DSP_CELL_inicial[[\"Sitio\", \"Local Cell ID\", \"Cell instance state\"]]\n",
    "## Se unen los DataFrames en uno solo por medio de left joins.\n",
    "df_Huawei_4G_inicial = df_LST_CELL.merge(df_LST_CELLOP, on=[\"Sitio\", \"Local Cell ID\"], how=\"left\")\n",
    "df_Huawei_4G_inicial = df_Huawei_4G_inicial.merge(df_LST_CNOPERATOR, on=[\"Sitio\"], how=\"left\")\n",
    "df_Huawei_4G_inicial = df_Huawei_4G_inicial.merge(df_DSP_S1INTARFACE, on=[\"Sitio\"], how=\"left\")\n",
    "df_Huawei_4G_inicial = df_Huawei_4G_inicial.merge(df_DSP_CELL, on=[\"Sitio\", \"Local Cell ID\"], how=\"left\")\n",
    "\n",
    "# Crea campo llaveTAC (Sitio + Local tracking area ID) para unirse con LST_OPERATORTA\n",
    "df_Huawei_4G_inicial[\"llaveTAC\"] = (\n",
    "    df_Huawei_4G_inicial[\"Sitio\"].astype(\"string\").str.strip()\n",
    "      .str.cat(df_Huawei_4G_inicial[\"Cellda con MOCN\"].astype(\"string\").str.strip(),\n",
    "               sep=\"\", na_rep=None)\n",
    ")\n",
    "\n",
    "# Obtiene informacion de LST_CNOPERATORTA (TAC)\n",
    "df_Huawei_4G_inicial = df_Huawei_4G_inicial.merge(df_LST_CNOPERATORTA, on=[\"llaveTAC\"], how=\"left\")\n",
    "\n",
    "## Obtiene elementos del Archivo anterior\n",
    "# Divide los registros en SUCCEEDED y FAILED\n",
    "df_succeeded = df_Huawei_4G_inicial[df_Huawei_4G_inicial[\"Seccion\"] == \"SUCCEEDED\"].copy()\n",
    "df_failed    = df_Huawei_4G_inicial[df_Huawei_4G_inicial[\"Seccion\"] == \"FAILED\"].copy()\n",
    "\n",
    "# --- Caso SUCCEEDED ---\n",
    "df_succeeded = df_succeeded.merge(\n",
    "    df_All_Huawei_4G_Anterior,\n",
    "    left_on=[\"Sitio\", \"Local Cell ID\"],\n",
    "    right_on=[\"Sitio_ant\", \"Local Cell ID_ant\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# --- Caso FAILED ---\n",
    "# Para asegurar que solo tome el primer \"Local Cell ID\" por \"Sitio_ant\"\n",
    "df_all_failed = (\n",
    "    df_All_Huawei_4G_Anterior\n",
    "    .sort_values(by=[\"Sitio_ant\", \"Local Cell ID_ant\"])\n",
    "    .drop_duplicates(subset=[\"Sitio_ant\"], keep=\"first\")\n",
    ")\n",
    "df_failed = df_failed.merge(\n",
    "    df_all_failed,\n",
    "    left_on=\"Sitio\",\n",
    "    right_on=\"Sitio_ant\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Une resultados\n",
    "df_Huawei_4G_inicial = pd.concat([df_succeeded, df_failed], ignore_index=True)\n",
    "\n",
    "# Reemplazo automático de columnas con sufijo \"_ant\" cuando Seccion == \"FAILED\"\n",
    "mask_failed = df_Huawei_4G_inicial[\"Seccion\"] == \"FAILED\"\n",
    "for col_ant in df_Huawei_4G_inicial.columns:\n",
    "    if col_ant.endswith(\"_ant\"):\n",
    "        col = col_ant[:-4]\n",
    "        if col in df_Huawei_4G_inicial.columns:\n",
    "            df_Huawei_4G_inicial.loc[mask_failed, col] = df_Huawei_4G_inicial.loc[mask_failed, col_ant]\n",
    "\n",
    "\n",
    "# Se une con EPT\n",
    "# ===================== FIX EPT MATCH (pegar ANTES del merge con EPT) =====================\n",
    "\n",
    "# 1) Normaliza Cell Name en ambos DF (quita invisibles/espacios)\n",
    "df_Huawei_4G_inicial[\"Cell Name\"] = (\n",
    "    df_Huawei_4G_inicial[\"Cell Name\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\"\\u00a0\", \" \", regex=False)  # NBSP\n",
    "    .str.replace(\"\\r\", \"\", regex=False)\n",
    "    .str.replace(\"\\n\", \"\", regex=False)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "df_EPT_unificado[\"Cell Name\"] = (\n",
    "    df_EPT_unificado[\"Cell Name\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\"\\u00a0\", \" \", regex=False)\n",
    "    .str.replace(\"\\r\", \"\", regex=False)\n",
    "    .str.replace(\"\\n\", \"\", regex=False)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# 2) Renombra columnas EPT para que SIEMPRE queden *_EPT (evita colisiones)\n",
    "df_EPT_unificado = df_EPT_unificado.rename(columns={\n",
    "    \"LAT\": \"LAT_EPT\",\n",
    "    \"LON\": \"LON_EPT\",\n",
    "    \"AT&T_Site_Name\": \"AT&T_Site_Name_EPT\"\n",
    "})\n",
    "\n",
    "# =================== FIN FIX EPT MATCH (antes del merge con EPT) ===================\n",
    "\n",
    "# Se une con EPT (ahora EPT ya trae nombres *_EPT)\n",
    "df_Huawei_4G_inicial = df_Huawei_4G_inicial.merge(\n",
    "    df_EPT_unificado,\n",
    "    on=\"Cell Name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Asegura columna base AT&T_Site_Name\n",
    "if \"AT&T_Site_Name\" not in df_Huawei_4G_inicial.columns:\n",
    "    df_Huawei_4G_inicial[\"AT&T_Site_Name\"] = np.nan\n",
    "\n",
    "# Rellena AT&T_Site_Name: primero EPT, luego mes anterior (_ant)\n",
    "df_Huawei_4G_inicial[\"AT&T_Site_Name\"] = (\n",
    "    df_Huawei_4G_inicial[\"AT&T_Site_Name\"].replace(\"\", np.nan)\n",
    "    .combine_first(df_Huawei_4G_inicial.get(\"AT&T_Site_Name_EPT\", pd.Series(index=df_Huawei_4G_inicial.index)).replace(\"\", np.nan))\n",
    "    .combine_first(df_Huawei_4G_inicial.get(\"AT&T_Site_Name_ant\", pd.Series(index=df_Huawei_4G_inicial.index)).replace(\"\", np.nan))\n",
    ")\n",
    "\n",
    "# Rellena LAT/LON: primero EPT, luego _ant\n",
    "for base, ept, ant in [(\"LAT\", \"LAT_EPT\", \"LAT_ant\"), (\"LON\", \"LON_EPT\", \"LON_ant\")]:\n",
    "    if base not in df_Huawei_4G_inicial.columns:\n",
    "        df_Huawei_4G_inicial[base] = np.nan\n",
    "\n",
    "    if ept in df_Huawei_4G_inicial.columns:\n",
    "        df_Huawei_4G_inicial[base] = (\n",
    "            df_Huawei_4G_inicial[base].replace(\"\", np.nan)\n",
    "            .combine_first(df_Huawei_4G_inicial[ept].replace(\"\", np.nan))\n",
    "        )\n",
    "\n",
    "    if ant in df_Huawei_4G_inicial.columns:\n",
    "        df_Huawei_4G_inicial[base] = (\n",
    "            df_Huawei_4G_inicial[base].replace(\"\", np.nan)\n",
    "            .combine_first(df_Huawei_4G_inicial[ant].replace(\"\", np.nan))\n",
    "        )\n",
    "\n",
    "# ======================= Obtiene valor TAL del Archivo anterior =======================\n",
    "def _norm_tac_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normaliza TAC a string sin .0, sin espacios, solo dígitos.\"\"\"\n",
    "    s = s.astype(str).str.strip()\n",
    "    s = s.str.replace(r\"\\.0$\", \"\", regex=True)   # 30136.0 -> 30136\n",
    "    s = s.str.replace(r\"[^\\d]\", \"\", regex=True)  # quita signos no numéricos\n",
    "    s = s.mask(s.eq(\"\"), np.nan)\n",
    "    return s\n",
    "\n",
    "# Normaliza TAC en ambos lados\n",
    "if \"TAC\" in df_Huawei_4G_inicial.columns:\n",
    "    df_Huawei_4G_inicial[\"TAC\"] = _norm_tac_series(df_Huawei_4G_inicial[\"TAC\"])\n",
    "if not df_All_Huawei_4G_Ant_TAC.empty:\n",
    "    df_All_Huawei_4G_Ant_TAC[\"TAC_ant\"] = _norm_tac_series(df_All_Huawei_4G_Ant_TAC[\"TAC_ant\"])\n",
    "\n",
    "# Deduplicación robusta de TAC_ant priorizando TAL no nulo\n",
    "if not df_All_Huawei_4G_Ant_TAC.empty:\n",
    "    _aux = df_All_Huawei_4G_Ant_TAC.copy()\n",
    "    _aux[\"_tal_isna\"] = _aux[\"TAL\"].isna() | (_aux[\"TAL\"].astype(str).str.strip().eq(\"\"))\n",
    "    # Ordena por TAC y pone primero los TAL válidos\n",
    "    _aux = _aux.sort_values(by=[\"TAC_ant\", \"_tal_isna\"])\n",
    "    df_All_Huawei_4G_Ant_TAC_clean = _aux.drop_duplicates(subset=[\"TAC_ant\"], keep=\"first\").drop(columns=\"_tal_isna\")\n",
    "else:\n",
    "    df_All_Huawei_4G_Ant_TAC_clean = df_All_Huawei_4G_Ant_TAC\n",
    "\n",
    "# Merge principal TAC -> TAL\n",
    "df_Huawei_4G_inicial = df_Huawei_4G_inicial.merge(\n",
    "    df_All_Huawei_4G_Ant_TAC_clean[[\"TAC_ant\", \"TAL\"]],\n",
    "    left_on=\"TAC\",\n",
    "    right_on=\"TAC_ant\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Fallback por Sitio si sigue faltando TAL y el archivo anterior trae TAL_ant\n",
    "if \"TAL\" not in df_Huawei_4G_inicial.columns:\n",
    "    df_Huawei_4G_inicial[\"TAL\"] = np.nan\n",
    "\n",
    "if \"TAL_ant\" in df_All_Huawei_4G_Anterior.columns:\n",
    "    tal_por_sitio = (\n",
    "        df_All_Huawei_4G_Anterior[[\"Sitio_ant\", \"TAL_ant\"]]\n",
    "        .dropna(subset=[\"Sitio_ant\"])\n",
    "        .drop_duplicates(subset=[\"Sitio_ant\"], keep=\"first\")\n",
    "        .rename(columns={\n",
    "            \"Sitio_ant\": \"__sitio_prev__\",\n",
    "            \"TAL_ant\": \"TAL_fallback_sitio\"\n",
    "        })\n",
    "    )\n",
    "\n",
    "    # Por si ya existiera accidentalmente la columna temporal\n",
    "    if \"__sitio_prev__\" in df_Huawei_4G_inicial.columns:\n",
    "        df_Huawei_4G_inicial.drop(columns=[\"__sitio_prev__\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    df_Huawei_4G_inicial = df_Huawei_4G_inicial.merge(\n",
    "        tal_por_sitio,\n",
    "        left_on=\"Sitio\",\n",
    "        right_on=\"__sitio_prev__\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"\")\n",
    "    )\n",
    "\n",
    "    # Rellena TAL sólo donde está vacío\n",
    "    _tal_vacio = df_Huawei_4G_inicial[\"TAL\"].isna() | df_Huawei_4G_inicial[\"TAL\"].astype(str).str.strip().eq(\"\")\n",
    "    if \"TAL_fallback_sitio\" in df_Huawei_4G_inicial.columns:\n",
    "        df_Huawei_4G_inicial.loc[_tal_vacio, \"TAL\"] = df_Huawei_4G_inicial.loc[_tal_vacio, \"TAL_fallback_sitio\"]\n",
    "\n",
    "    # Limpieza de columnas auxiliares\n",
    "    df_Huawei_4G_inicial.drop(columns=[\"__sitio_prev__\", \"TAL_fallback_sitio\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "\n",
    "# =============================================================================================\n",
    "\n",
    "## Calculo valor \"Almenos una celda encendida con MOCN\"\n",
    "df_Huawei_4G_inicial[\"Almenos una celda encendida con MOCN\"] = np.where(\n",
    "    pd.to_numeric(df_Huawei_4G_inicial[\"Cellda con MOCN\"], errors=\"coerce\").eq(2),\n",
    "    \"Si\",\n",
    "    \"NO\"\n",
    ")\n",
    "\n",
    "## Obtiene valor de MMEs\n",
    "df_Huawei_4G_inicial = df_Huawei_4G_inicial.merge(\n",
    "    df_MMEs, left_on=[\"Sitio\", \"S1 MOCN\"], right_on=[\"Sitio\", \"CN Operator ID\"], how=\"left\"\n",
    ")\n",
    "\n",
    "## Reemplaza 'LST CELL_c' por 'MAE-' y conserva el número final\n",
    "df_Huawei_4G_inicial[\"Gestor\"] = df_Huawei_4G_inicial[\"Gestor\"].str.replace(\n",
    "    r\"LST CELL_c(\\d+)\", r\"MAE-\\1\", regex=True\n",
    ")\n",
    "\n",
    "## Reordenamiento Estructura Final\n",
    "nuevo_orden = [\"AT&T_Site_Name\", \"Gestor\", \"Sitio\", \"Local Cell ID\", \"Cell Name\",\n",
    "\"Csg indicator\", \"Uplink cyclic prefix length\", \"Downlink cyclic prefix length\",\n",
    "\"NB-IoT Cell Flag\", \"Coverage Level Type\", \"Frequency band\", \"Uplink EARFCN indication\",\n",
    "\"Uplink EARFCN\", \"Downlink EARFCN\", \"Uplink bandwidth\", \"Downlink bandwidth\",\n",
    "\"Cell ID\", \"Physical cell ID\", \"Additional spectrum emission\", \"Cell active state\",\n",
    "\"Cell admin state\", \"Cell middle block timer(min)\", \"Cell FDD TDD indication\",\n",
    "\"Subframe assignment\", \"Special subframe patterns\", \"SSP6 DwPTS Mode\", \"Cell Standby Mode\",\n",
    "\"Cell specific offset(dB)\", \"Frequency offset(dB)\", \"Root sequence index\", \"High speed flag\",\n",
    "\"Preamble format\", \"Cell radius(m)\", \"Customized bandwidth configure indicator\",\n",
    "\"Customized uplink bandwidth(0.1MHz)\", \"Customized downlink bandwidth(0.1MHz)\",\n",
    "\"Emergency Area Id indicator\", \"Emergency Area ID\", \"Ue max power allowed configure indicator\",\n",
    "\"Max transmit power allowed(dBm)\", \"Flag of Multi-RRU Cell\", \"Mode of Multi-RRU Cell\",\n",
    "\"CPRI Ethernet Compression Ratio\", \"CPRI Compression\", \"Physical Cell Number of SFN Cell\",\n",
    "\"Air Cell Flag\", \"CRS Port Number\", \"Cell transmission and reception mode\",\n",
    "\"CRS Antenna Port Mapping\", \"User label\", \"Work mode\", \"CN Operator Sharing Group ID\",\n",
    "\"Intra Frequency RAN Sharing Indication\", \"IntraFreq ANR Indication\", \"ANR Frequency Priority\",\n",
    "\"Cell Radius Start Location(m)\", \"Specified Cell Flag\", \"Downlink Punctured RB Number\",\n",
    "\"SFN Master Cell Label\", \"Multi Cell Share Mode\", \"Standby Cell SFN Recovery Time(h)\",\n",
    "\"Compact Bandwidth Control Interference Mode\", \"Uplink Punctured RB Number Offset\",\n",
    "\"Ultra High-Speed Cell Root Sequence Index\", \"Cellda con MOCN\", \"MOCN Configurado Sitio\",\n",
    "\"S1 MOCN\", \"S1 Interface Fault Reason\", \"Almenos una celda encendida con MOCN\", \"TAC\", \"TAL\",\n",
    "\"Cell instance state\", \"LAT\", \"LON\", \"MMEs\", \"Comentarios\"]\n",
    "\n",
    "df_Huawei_4G_inicial = df_Huawei_4G_inicial[nuevo_orden]\n",
    "\n",
    "# Remueve sufijo _ant\n",
    "df_Huawei_4G_inicial.columns = [col.replace(\"_ant\", \"\") for col in df_Huawei_4G_inicial.columns]\n",
    "#print(\"\\n[DEBUG] ANTES del filtro de AT&T_Site_Name:\", df_Huawei_4G_inicial.shape)\n",
    "\n",
    "#mask_site = df_Huawei_4G_inicial[\"Sitio\"].eq(\"YUCMER0399\")\n",
    "#cols_att = [c for c in df_Huawei_4G_inicial.columns if \"AT&T_Site\" in c]\n",
    "\n",
    "#print(\"[DEBUG] YUCMER0399 antes del filtro:\", df_Huawei_4G_inicial.loc[mask_site].shape)\n",
    "#print(df_Huawei_4G_inicial.loc[mask_site, [\"Sitio\",\"Local Cell ID\",\"Cell Name\"] + cols_att].head(20))\n",
    "\n",
    "# Descarta valores sin AT&T_Site_Name\n",
    "df_Huawei_4G_inicial = df_Huawei_4G_inicial.loc[\n",
    "    df_Huawei_4G_inicial[\"AT&T_Site_Name\"].notna() & (df_Huawei_4G_inicial[\"AT&T_Site_Name\"].str.strip() != \"\")\n",
    "]\n",
    "#print(\"\\n[DEBUG] DESPUÉS del filtro de AT&T_Site_Name:\", df_Huawei_4G_inicial.shape)\n",
    "#print(\"[DEBUG] YUCMER0399 después del filtro:\", df_Huawei_4G_inicial.loc[df_Huawei_4G_inicial[\"Sitio\"].eq(\"YUCMER0399\")].shape)\n",
    "\n",
    "\n",
    "# Convierte columnas totalmente numéricas (sin forzar TAL a numérico)\n",
    "df_Huawei_4G = df_Huawei_4G_inicial.apply(\n",
    "    lambda col: pd.to_numeric(col, errors=\"coerce\")\n",
    "    if not pd.to_numeric(col, errors=\"coerce\").isna().any()\n",
    "    else col\n",
    ")\n",
    "\n",
    "# Asegura TAL como string (evita perder ceros a la izquierda si aplica)\n",
    "if \"TAL\" in df_Huawei_4G.columns:\n",
    "    # después de construir df_Huawei_4G\n",
    "    df_Huawei_4G[\"TAL\"] = (\n",
    "    df_Huawei_4G[\"TAL\"]\n",
    "        .astype(str).str.strip()\n",
    "        .str.replace(r\"\\.0+$\", \"\", regex=True)   # 30136.0 -> 30136\n",
    "        .replace({\"nan\": \"\", \"NaN\": \"\", \"<NA>\": \"\"})\n",
    ")\n",
    "\n",
    "## Archivo Final formateado\n",
    "ruta_salida = os.path.join(ruta_destino, f\"All_Huawei_4G_{fecha_ejecucion}.xlsx\")\n",
    "df_Huawei_4G.to_excel(ruta_salida, index=False, engine=\"openpyxl\")\n",
    "\n",
    "# Abre archivo creado para rotar los encabezados\n",
    "wb = load_workbook(ruta_salida)\n",
    "ws = wb.active\n",
    "\n",
    "# Rota los encabezados 90° (vertical)\n",
    "for col_num, column_title in enumerate(df_Huawei_4G.columns, 1):\n",
    "    cell = ws[f\"{get_column_letter(col_num)}1\"]\n",
    "    cell.alignment = Alignment(textRotation=90, horizontal=\"center\", vertical=\"bottom\")\n",
    "\n",
    "# Guarda cambios\n",
    "wb.save(ruta_salida)\n"
   ],
   "id": "9c0d669ce66a341",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SCaracoza\\AppData\\Local\\Temp\\ipykernel_24480\\561819019.py:144: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_EPT_unificado[\"Cell Name\"] = (\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
