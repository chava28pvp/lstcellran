{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T00:07:56.873074Z",
     "start_time": "2026-01-28T00:07:54.293405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "########################## Proceso ETL - Huawei 3G ##########################\n",
    "# MA. 20250923.\n",
    "\n",
    "# Librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import Alignment\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "\n",
    "## Variables compartidas\n",
    "\n",
    "# Ubicación archivos\n",
    "ruta_carpeta = \"C:/Users/SCaracoza/Documents/AT&T/LST Cell Ran/Huawei/Huawei_3G\"\n",
    "\n",
    "ruta_ept = \"C:/Users/SCaracoza/Documents/AT&T/LST Cell Ran/Huawei/Huawei_3G\"\n",
    "\n",
    "ruta_destino = \"C:/Users/SCaracoza/Documents/AT&T/LST Cell Ran/Huawei/Huawei_3G\"\n",
    "\n",
    "# Fecha para el nombre de los archivos a crear\n",
    "fecha_ejecucion: str = datetime.now().strftime(\"%Y%m\")"
   ],
   "id": "9764f221f9d3d314",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T00:08:14.049177Z",
     "start_time": "2026-01-28T00:07:59.564598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MA. 20250926\n",
    "### LST UCELL. Lectura y unificación de archivos. ###\n",
    "\n",
    "# Prefijo de archivos\n",
    "archivo_prefijo = \"MML_Task_Result_LST UCELL_c\"\n",
    "archivos_txt = glob.glob(os.path.join(ruta_carpeta, f\"{archivo_prefijo}*.txt\"))\n",
    "\n",
    "# Regex\n",
    "pat_section = re.compile(r\"(MML Command-----LST UCELL.*?)(?=MML Command-----LST UCELL|$)\", re.S)\n",
    "pat_ne = re.compile(r\"NE\\s*:\\s*(\\S+)\")\n",
    "pat_plus = re.compile(r\"^\\+\\+\\+\\s+(\\S+)\", re.M)\n",
    "\n",
    "# Columnas estándar\n",
    "columnas_lst = [\n",
    "    \"Origen\", \"RNC\", \"Logical RNC ID\", \"Cell ID\", \"Cell Name\", \"Max Transmit Power of Cell\",\n",
    "    \"Band Indicator\", \"Cn Operator Group Index\", \"UL Frequency Ind\", \"Uplink UARFCN\",\n",
    "    \"Downlink UARFCN\", \"Time Offset\", \"Num of Continuous in Sync Ind\",\n",
    "    \"Num of Continuous Out of Sync Ind\", \"Radio Link Failure Timer Length\",\n",
    "    \"DL Power Control Mode 1\", \"DL Primary Scrambling Code\", \"TX Diversity Indication\",\n",
    "    \"Service Priority Group Identity\", \"NodeB Name\", \"Local Cell ID\",\n",
    "    \"Location Area Code\", \"Service Area Code\", \"RAC Configuration Indication\",\n",
    "    \"Routing Area Code\", \"STTD Support Indicator\", \"CP1 Support Indicator\",\n",
    "    \"Closed Loop Time Adjust Mode\", \"DPCH Tx Diversity Mode for Other User\",\n",
    "    \"FDPCH Tx Diversity Mode for Other User\", \"DPCH Tx Diversity Mode for MIMO User\",\n",
    "    \"FDPCH Tx Diversity Mode for MIMO User\", \"Tx Diversity Mode for DC-HSDPA User\",\n",
    "    \"Cell Oriented Cell Individual Offset\", \"Cell VP Limit Indicator\", \"DSS Cell Flag\",\n",
    "    \"Maximum TX Power in Small DSS Coverage\", \"Common Channel Bandwidth Operator Index\",\n",
    "    \"Hierarchy ID of Terminal Type\", \"Heterogeneous Cell Flag\", \"HostType\",\n",
    "    \"Validation indication\", \"Cell administrative state\", \"Cell MIMO state\",\n",
    "    \"IPDL flag\", \"Cell CBS state\", \"Cell ERACH state\",\n",
    "    \"Subrack No.\", \"Subrack name\", \"Slot No.\", \"Subsystem No.\", \"Cell MBMS state\",\n",
    "    \"SSN\"\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for archivo in archivos_txt:\n",
    "    with open(archivo, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        contenido = f.read()\n",
    "\n",
    "    # Divide por secciones de MML Command\n",
    "    secciones = pat_section.findall(contenido)\n",
    "\n",
    "    for seccion in secciones:\n",
    "        # Detecta NE\n",
    "        m_ne = pat_ne.search(seccion)\n",
    "        ne_actual = m_ne.group(1) if m_ne else \"\"\n",
    "\n",
    "        # Si NE no existe, intenta con +++\n",
    "        if not ne_actual:\n",
    "            m_plus = pat_plus.search(seccion)\n",
    "            if m_plus:\n",
    "                ne_actual = m_plus.group(1)\n",
    "\n",
    "        # Divide en bloques de List Cell Basic Information\n",
    "        bloques = re.split(r\"List Cell Basic Information\\s*-+\\s*\", seccion)[1:]\n",
    "\n",
    "        for bloque in bloques:\n",
    "            lineas = bloque.strip().splitlines()\n",
    "            if len(lineas) < 2:\n",
    "                continue\n",
    "\n",
    "            encabezado = re.split(r\"\\s{2,}\", lineas[0].strip())\n",
    "            datos = []\n",
    "\n",
    "            for linea in lineas[1:]:\n",
    "                if linea.strip().startswith((\"---\", \"+++\", \"Number of results\", \"To be continued\")):\n",
    "                    break\n",
    "                if not linea.strip():\n",
    "                    continue\n",
    "                fila = re.split(r\"\\s{2,}\", linea.strip())\n",
    "                if len(fila) < len(encabezado):\n",
    "                    fila += [\"\"] * (len(encabezado) - len(fila))\n",
    "                elif len(fila) > len(encabezado):\n",
    "                    fila = fila[:len(encabezado)]\n",
    "                datos.append(fila)\n",
    "\n",
    "            if datos:\n",
    "                df_tmp = pd.DataFrame(datos, columns=encabezado)\n",
    "                df_tmp[\"Origen\"] = os.path.basename(archivo)\n",
    "                df_tmp[\"RNC\"] = ne_actual\n",
    "\n",
    "                # Normaliza columnas\n",
    "                for col in columnas_lst:\n",
    "                    if col not in df_tmp.columns:\n",
    "                        df_tmp[col] = \"\"\n",
    "\n",
    "                df_tmp = df_tmp[columnas_lst]\n",
    "                dfs.append(df_tmp)\n",
    "\n",
    "# Unión\n",
    "df_final = pd.concat(dfs, ignore_index=True, sort=False) if dfs else pd.DataFrame(columns=columnas_lst)\n",
    "\n",
    "# --- Limpieza y transformaciones ---\n",
    "df_final.columns = df_final.columns.str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "df_final = df_final.loc[:, ~df_final.columns.duplicated()]\n",
    "\n",
    "# Reemplaza <NULL> en Uplink UARFCN\n",
    "if \"Uplink UARFCN\" in df_final.columns:\n",
    "    df_final[\"Uplink UARFCN\"] = df_final[\"Uplink UARFCN\"].replace(\"<NULL>\", \"0\")\n",
    "\n",
    "# Asegura columnas de SSN\n",
    "for col in [\"Subrack No.\", \"Slot No.\", \"Subsystem No.\"]:\n",
    "    if col not in df_final.columns:\n",
    "        df_final[col] = \"\"\n",
    "\n",
    "# Genera SSN\n",
    "df_final[\"SSN\"] = (\n",
    "    df_final[[\"Subrack No.\", \"Slot No.\", \"Subsystem No.\"]]\n",
    "    .astype(str).fillna(\"\")\n",
    "    .agg(\"-\".join, axis=1)\n",
    ")\n",
    "df_final[\"SSN\"] = df_final[\"SSN\"].apply(lambda x: \"--\" if x.replace(\"-\", \"\").strip() == \"\" else x)\n",
    "\n",
    "# Filtrado\n",
    "if \"Logical RNC ID\" in df_final.columns:\n",
    "    df_final = df_final[~df_final[\"Logical RNC ID\"].str.contains(r\"\\+\\+\\+\", na=False)]\n",
    "if \"Cell Name\" in df_final.columns:\n",
    "    df_final = df_final[df_final[\"Cell Name\"].astype(str).str.strip() != \"\"]\n",
    "\n",
    "# Convierte columnas numéricas cuando aplica\n",
    "df_LST_UCELL_inicial = df_final.apply(\n",
    "    lambda col: pd.to_numeric(col, errors=\"coerce\")\n",
    "    if not pd.to_numeric(col, errors=\"coerce\").isna().any()\n",
    "    else col\n",
    ")\n",
    "\n",
    "# Archivo unificado en Excel (validación)\n",
    "#salida_excel = os.path.join(ruta_destino, f\"LST_UCELL_{fecha_ejecucion}.xlsx\")\n",
    "#df_LST_UCELL_inicial[columnas_lst].to_excel(salida_excel, index=False, engine=\"openpyxl\")"
   ],
   "id": "8d33b6d897bc1d9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T00:08:19.810629Z",
     "start_time": "2026-01-28T00:08:17.469221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MA. 20250929\n",
    "### DSP UCELL. Lectura y unificación de archivos. ###\n",
    "\n",
    "# Prefijo de archivos\n",
    "archivo_prefijo_dsp = \"MML_Task_Result_DSP UCELL_\"\n",
    "archivos_dsp = glob.glob(os.path.join(ruta_carpeta, f\"{archivo_prefijo_dsp}*.txt\"))\n",
    "\n",
    "# Columnas esperadas\n",
    "columnas_dsp = [\n",
    "    \"Cell ID\", \"Cell Name\", \"Operation state\", \"Administrative state\",\n",
    "    \"DSS state\", \"State explanation\", \"Subrack No.\", \"Slot No.\", \"Subsystem No.\"\n",
    "]\n",
    "\n",
    "dfs_dsp = []\n",
    "\n",
    "if archivos_dsp:\n",
    "    for archivo in archivos_dsp:\n",
    "        with open(archivo, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            contenido = f.read()\n",
    "\n",
    "        # Divide en bloques a partir de \"Cell state information\"\n",
    "        bloques = re.split(r\"Cell state information\\s*-+\\s*\", contenido)[1:]\n",
    "\n",
    "        for bloque in bloques:\n",
    "            lineas = bloque.strip().splitlines()\n",
    "            if len(lineas) < 2:\n",
    "                continue\n",
    "\n",
    "            encabezado = re.split(r\"\\s{2,}\", lineas[0].strip())\n",
    "            datos = []\n",
    "\n",
    "            for linea in lineas[1:]:\n",
    "                if linea.strip().startswith((\"---\", \"+++\", \"To be continued\", \"Number of results\")):\n",
    "                    break\n",
    "                if not linea.strip():\n",
    "                    continue\n",
    "\n",
    "                fila = re.split(r\"\\s{2,}\", linea.strip())\n",
    "                # Normaliza longitud\n",
    "                if len(fila) < len(encabezado):\n",
    "                    fila += [\"\"] * (len(encabezado) - len(fila))\n",
    "                elif len(fila) > len(encabezado):\n",
    "                    fila = fila[:len(encabezado)]\n",
    "                datos.append(fila)\n",
    "\n",
    "            if datos:\n",
    "                df_tmp = pd.DataFrame(datos, columns=encabezado)\n",
    "                df_tmp[\"Origen\"] = os.path.basename(archivo)\n",
    "                dfs_dsp.append(df_tmp)\n",
    "\n",
    "if dfs_dsp:\n",
    "    df_DSP_UCELL_ini = pd.concat(dfs_dsp, ignore_index=True, sort=True)\n",
    "\n",
    "    # Renombra \"Cell name\" -> \"Cell Name\"\n",
    "    if \"Cell name\" in df_DSP_UCELL_ini.columns:\n",
    "        df_DSP_UCELL_ini.rename(columns={\"Cell name\": \"Cell Name\"}, inplace=True)\n",
    "\n",
    "    # Asegura columnas esperadas\n",
    "    for col in columnas_dsp:\n",
    "        if col not in df_DSP_UCELL_ini.columns:\n",
    "            df_DSP_UCELL_ini[col] = \"\"\n",
    "\n",
    "    # Reordena\n",
    "    df_DSP_UCELL_inicial = df_DSP_UCELL_ini[columnas_dsp + [\"Origen\"]]\n",
    "\n",
    "    # Elimina registros donde \"Cell Name\" esté vacío o NaN\n",
    "    if \"Cell Name\" in df_DSP_UCELL_inicial.columns:\n",
    "        df_DSP_UCELL_inicial = df_DSP_UCELL_inicial[df_DSP_UCELL_inicial[\"Cell Name\"].astype(str).str.strip() != \"\"]\n",
    "\n",
    "    # Convierte columnas numéricas cuando aplica\n",
    "    df_DSP_UCELL_inicial = df_DSP_UCELL_inicial.apply(\n",
    "        lambda col: pd.to_numeric(col, errors=\"coerce\")\n",
    "        if not pd.to_numeric(col, errors=\"coerce\").isna().any()\n",
    "        else col\n",
    "    )\n",
    "else:\n",
    "    df_DSP_UCELL_inicial = pd.DataFrame(columns=columnas_dsp + [\"Origen\"])\n",
    "\n",
    "\n",
    "# Archivo unificado en Excel (validación)\n",
    "#ruta_salida = os.path.join(ruta_destino, f\"DSP_UCELL_{fecha_ejecucion}.xlsx\")\n",
    "#df_DSP_UCELL_inicial.to_excel(ruta_salida, index=False, engine=\"openpyxl\")"
   ],
   "id": "f4f16530b34125b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T00:08:24.661486Z",
     "start_time": "2026-01-28T00:08:22.707121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MA. 20250923.\n",
    "### LST UCELLURA. Lectura y unificación de archivos. ###\n",
    "\n",
    "# Prefijo de archivos\n",
    "archivo_prefijo_ura = \"MML_Task_Result_LST UCELLURA_c\"\n",
    "archivos_ura = glob.glob(os.path.join(ruta_carpeta, f\"{archivo_prefijo_ura}*.txt\"))\n",
    "\n",
    "# Columnas esperadas\n",
    "columnas_ura = [\"Origen\", \"NE\", \"Logical RNC ID\", \"Cell ID\", \"Cell Name\", \"URA ID\"]\n",
    "\n",
    "dfs_ura = []\n",
    "\n",
    "if archivos_ura:\n",
    "    for archivo in archivos_ura:\n",
    "        with open(archivo, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            contenido = f.read()\n",
    "\n",
    "        # Detecta NE (si viene vacío, lo toma de +++)\n",
    "        ne_actual = None\n",
    "        m_ne = re.search(r\"^NE\\s*:\\s*(\\S+)?\", contenido, re.MULTILINE)\n",
    "        if m_ne and m_ne.group(1):\n",
    "            ne_actual = m_ne.group(1)\n",
    "        else:\n",
    "            m_plus = re.search(r\"^\\+\\+\\+\\s+(\\S+)\", contenido, re.MULTILINE)\n",
    "            if m_plus:\n",
    "                ne_actual = m_plus.group(1)\n",
    "\n",
    "        # Divide por bloques de tabla \"List Cell URA\"\n",
    "        bloques = re.split(r\"List Cell URA\\s*-+\\s*\", contenido)[1:]\n",
    "\n",
    "        for bloque in bloques:\n",
    "            lineas = bloque.strip().splitlines()\n",
    "            if len(lineas) < 2:\n",
    "                continue\n",
    "\n",
    "            # Encabezado y datos\n",
    "            encabezado = re.split(r\"\\s{2,}\", lineas[0].strip())\n",
    "            datos = []\n",
    "            for linea in lineas[1:]:\n",
    "                if linea.strip().startswith((\"---\", \"+++\", \"Number of results\", \"To be continued\")):\n",
    "                    break\n",
    "                if not linea.strip():\n",
    "                    continue\n",
    "                fila = re.split(r\"\\s{2,}\", linea.strip())\n",
    "                if len(fila) < len(encabezado):\n",
    "                    fila += [\"\"] * (len(encabezado) - len(fila))\n",
    "                elif len(fila) > len(encabezado):\n",
    "                    fila = fila[:len(encabezado)]\n",
    "                datos.append(fila)\n",
    "\n",
    "            if datos:\n",
    "                df_tmp = pd.DataFrame(datos, columns=encabezado)\n",
    "                df_tmp[\"Origen\"] = os.path.basename(archivo)\n",
    "                df_tmp[\"NE\"] = ne_actual\n",
    "                dfs_ura.append(df_tmp)\n",
    "\n",
    "# Une resultados\n",
    "if dfs_ura:\n",
    "    df_LST_UCELLURA_ini = pd.concat(dfs_ura, ignore_index=True, sort=True)\n",
    "\n",
    "    # Reordena y garantiza columnas esperadas\n",
    "    for col in columnas_ura:\n",
    "        if col not in df_LST_UCELLURA_ini.columns:\n",
    "            df_LST_UCELLURA_ini[col] = \"\"\n",
    "\n",
    "    df_LST_UCELLURA_inicial = df_LST_UCELLURA_ini[columnas_ura]\n",
    "\n",
    "\n",
    "# Elimina registros donde \"Cell Name\" esté vacío o NaN\n",
    "    if \"Cell Name\" in df_LST_UCELLURA_inicial.columns:\n",
    "        df_LST_UCELLURA_inicial = df_LST_UCELLURA_inicial[\n",
    "        df_LST_UCELLURA_inicial[\"Cell Name\"].astype(str).str.strip() != \"\"\n",
    "    ]\n",
    "else:\n",
    "    df_LST_UCELLURA_inicial = pd.DataFrame(columns=columnas_ura)\n",
    "\n",
    "# Convierte columnas numéricas si aplica\n",
    "df_LST_UCELLURA_inicial = df_LST_UCELLURA_inicial.apply(\n",
    "    lambda col: pd.to_numeric(col, errors=\"coerce\")\n",
    "    if not pd.to_numeric(col, errors=\"coerce\").isna().any()\n",
    "    else col\n",
    ")\n",
    "\n",
    "# Archivo unificado en CSV, para fines de validación.\n",
    "#ruta_salida = os.path.join(ruta_destino, f\"LST_UCELLURA_{fecha_ejecucion}.xlsx\")\n",
    "#df_LST_UCELLURA_inicial.to_excel(ruta_salida, index=False, engine=\"openpyxl\")"
   ],
   "id": "4bb8a680e5736a4e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T00:09:36.944721Z",
     "start_time": "2026-01-28T00:08:30.090028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MA. 20250924.\n",
    "### Información archivo mes anterior ###\n",
    "\n",
    "# Sufijo mes anterior\n",
    "today = date.today()\n",
    "prev_year  = today.year if today.month > 1 else today.year - 1\n",
    "prev_month = today.month - 1 or 12\n",
    "yyyymm = f\"{prev_year}{prev_month:02d}\"\n",
    "fecha_hoy = today.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "# Busca archivo\n",
    "ruta: str = ruta_destino  # --> MA. Por Definir.\n",
    "archivo = f\"All_Huawei_3G_{yyyymm}.xlsx\"\n",
    "path = os.path.join(ruta, archivo)\n",
    "\n",
    "# Columnas necesarias\n",
    "columnas_anteriores = [\"ANT Cell Id\", \"ANT Cell Name\", \"RNC\", \"LAT\", \"LON\", \"AT&T_Site_Name\", \"En el gestor\"]\n",
    "\n",
    "if os.path.exists(path):\n",
    "    # Fecha del día y de creación de archivo anterior\n",
    "    wb = load_workbook(path, read_only=True)\n",
    "    props = wb.properties\n",
    "    fecha_creacion = props.created.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "    # Extrae columnas necesarias\n",
    "    df_All_Huawei_3G_Anterior = pd.read_excel(\n",
    "        path,\n",
    "        usecols=[\"Cell Id\", \"Cell Name\", \"LAT\", \"LON\", \"AT&T_Site_Name\", \"En el gestor\"]\n",
    "    )\n",
    "\n",
    "    # Renombra columnas llave\n",
    "    df_All_Huawei_3G_Anterior = df_All_Huawei_3G_Anterior.rename(\n",
    "        columns={\"Cell Id\": \"ANT Cell Id\", \"Cell Name\": \"ANT Cell Name\"}\n",
    "    )\n",
    "\n",
    "    ## Formato columna \"En el gestor\"\n",
    "    df_All_Huawei_3G_Anterior[\"En el gestor\"] = df_All_Huawei_3G_Anterior[\"En el gestor\"].apply(\n",
    "    lambda x: pd.to_datetime(x, dayfirst=True, errors=\"coerce\")\n",
    "              .strftime(\"%d/%m/%Y\") if not pd.isna(pd.to_datetime(x, dayfirst=True, errors=\"coerce\")) else x\n",
    "        )\n",
    "\n",
    "else:\n",
    "    # No existe archivo, crea DataFrame vacío con columnas necesarias\n",
    "    df_All_Huawei_3G_Anterior = pd.DataFrame(columns=columnas_anteriores)"
   ],
   "id": "2ffaa94401f0c280",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T00:12:46.774054Z",
     "start_time": "2026-01-28T00:09:36.973027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MA. 20250926.\n",
    "### Información del EPT ###\n",
    "\n",
    "# Prefijo del archivo\n",
    "prefijo_ept = \"EPT_ATT_UMTS_LTE_\"\n",
    "\n",
    "# Busca archivo que empiece con el prefijo\n",
    "archivo = glob.glob(os.path.join(ruta_ept, f\"{prefijo_ept}*.xlsx\"))\n",
    "\n",
    "# Inicializa por si no hay archivo\n",
    "df_EPT_ini = pd.DataFrame()\n",
    "df_EPT_unificado = pd.DataFrame(columns=[\"Cell Name\", \"LAT\", \"LON\", \"AT&T_Site_Name\"])\n",
    "df_EPT_nodes = pd.DataFrame(columns=[\"node_key\", \"LAT\", \"LON\", \"AT&T_Site_Name\"])\n",
    "\n",
    "if archivo:\n",
    "    archivo_encontrado = archivo[0]\n",
    "    nombre_archivo = os.path.basename(archivo_encontrado)\n",
    "\n",
    "    # Lista de hojas a leer\n",
    "    hojas = [\n",
    "        \"EPT_3G_LTE_OUTDOOR\",\n",
    "        \"PLAN_OUTDOOR\",\n",
    "        \"EPT_3G_LTE_INDOOR\",\n",
    "        \"PLAN_INDOOR\",\n",
    "        \"Eventos_Especiales\"\n",
    "    ]\n",
    "\n",
    "    # Lee todas las hojas y agrega el nombre de la hoja en columna\n",
    "    dfs = [\n",
    "        pd.read_excel(archivo_encontrado, sheet_name=hoja, engine=\"openpyxl\")\n",
    "        .assign(Hoja=hoja, Origen=nombre_archivo)\n",
    "        for hoja in hojas\n",
    "    ]\n",
    "\n",
    "    # Concatena todo en un solo DataFrame\n",
    "    df_EPT_inicial = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Elimina duplicados\n",
    "    df_EPT_inicial = df_EPT_inicial.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Renombramiento columna(s)\n",
    "    nuevos_nombres = {\"CellName\": \"Cell Name\", \"Latitud\": \"LAT\", \"Longitud\": \"LON\"}\n",
    "    df_EPT_inicial.rename(columns=nuevos_nombres, inplace=True)\n",
    "\n",
    "    # Convierte columnas totalmente numéricas (cuando aplica)\n",
    "    df_EPT_ini = df_EPT_inicial.apply(\n",
    "        lambda col: pd.to_numeric(col, errors=\"coerce\")\n",
    "        if not pd.to_numeric(col, errors=\"coerce\").isna().any()\n",
    "        else col\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # 1) EPT por Cell Name (como ya tenías)\n",
    "    # -------------------------------\n",
    "    df_EPT_unificado = df_EPT_ini[[\"Cell Name\", \"LAT\", \"LON\", \"AT&T_Site_Name\"]]\n",
    "\n",
    "    # -------------------------------\n",
    "    # 2) EPT por NodeB Name (preparado aquí)\n",
    "    #    Genera una tabla \"larga\" con AT&T_Node_Name y Node_B_U2000\n",
    "    # -------------------------------\n",
    "    cols_nodo_ept = [c for c in [\"AT&T_Node_Name\", \"Node_B_U2000\"] if c in df_EPT_ini.columns]\n",
    "    cols_site = [c for c in [\"LAT\", \"LON\", \"AT&T_Site_Name\"] if c in df_EPT_ini.columns]\n",
    "\n",
    "    if cols_nodo_ept and set([\"LAT\", \"LON\", \"AT&T_Site_Name\"]).issubset(df_EPT_ini.columns):\n",
    "\n",
    "        df_ept_nodes_raw = (\n",
    "            df_EPT_ini[cols_site + cols_nodo_ept]\n",
    "            .melt(id_vars=cols_site, value_vars=cols_nodo_ept, value_name=\"node_name\")\n",
    "            .drop(columns=[\"variable\"])\n",
    "            .dropna(subset=[\"node_name\"])\n",
    "            .drop_duplicates()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Normalizador consistente\n",
    "        def _norm(s: str) -> str:\n",
    "            s = str(s)\n",
    "            return (s.upper()\n",
    "                      .replace(\" \", \"\")\n",
    "                      .replace(\"_\", \"\")\n",
    "                      .replace(\"-\", \"\")\n",
    "                      .replace(\".\", \"\")\n",
    "                      .strip())\n",
    "\n",
    "        df_ept_nodes_raw[\"node_key\"] = df_ept_nodes_raw[\"node_name\"].map(_norm)\n",
    "\n",
    "        # Mantén una fila por node_key (si hay duplicados, toma la primera con datos)\n",
    "        df_EPT_nodes = (\n",
    "            df_ept_nodes_raw\n",
    "            .drop_duplicates(subset=[\"node_key\"], keep=\"first\")\n",
    "            .loc[:, [\"node_key\", \"LAT\", \"LON\", \"AT&T_Site_Name\"]]\n",
    "            .reset_index(drop=True)\n",
    "        )\n"
   ],
   "id": "e3732eb14de95941",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T00:17:51.513538Z",
     "start_time": "2026-01-28T00:12:46.804796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MA. 20250923.\n",
    "### Creación archivo final ###\n",
    "\n",
    "## Extraemos solo las columnas requeridas de cada DataFrame.\n",
    "\n",
    "## LST_UCELL\n",
    "df_LST_UCELL = df_LST_UCELL_inicial[[\n",
    "    \"RNC\",\"Logical RNC ID\",\"Cell ID\",\"Cell Name\",\"Max Transmit Power of Cell\",\"Band Indicator\",\n",
    "    \"Cn Operator Group Index\",\"UL Frequency Ind\",\"Uplink UARFCN\",\"Downlink UARFCN\",\"Time Offset\",\n",
    "    \"Num of Continuous in Sync Ind\",\"Num of Continuous Out of Sync Ind\",\"Radio Link Failure Timer Length\",\n",
    "    \"DL Power Control Mode 1\",\"DL Primary Scrambling Code\",\"TX Diversity Indication\",\n",
    "    \"Service Priority Group Identity\",\"NodeB Name\",\"Local Cell ID\",\"Location Area Code\",\"Service Area Code\",\n",
    "    \"RAC Configuration Indication\",\"Routing Area Code\",\"STTD Support Indicator\",\"CP1 Support Indicator\",\n",
    "    \"Closed Loop Time Adjust Mode\",\"DPCH Tx Diversity Mode for Other User\",\"FDPCH Tx Diversity Mode for Other User\",\n",
    "    \"DPCH Tx Diversity Mode for MIMO User\",\"FDPCH Tx Diversity Mode for MIMO User\",\"Tx Diversity Mode for DC-HSDPA User\",\n",
    "    \"Cell Oriented Cell Individual Offset\",\"Cell VP Limit Indicator\",\"DSS Cell Flag\",\n",
    "    \"Maximum TX Power in Small DSS Coverage\",\"Common Channel Bandwidth Operator Index\",\n",
    "    \"Hierarchy ID of Terminal Type\",\"Heterogeneous Cell Flag\",\"HostType\",\"Validation indication\",\n",
    "    \"Cell administrative state\",\"Cell MIMO state\",\"IPDL flag\",\"Cell CBS state\",\"Cell ERACH state\",\n",
    "    \"Subrack No.\",\"Subrack name\",\"Slot No.\",\"Subsystem No.\",\"Cell MBMS state\",\"SSN\"\n",
    "]]\n",
    "df_LST_UCELL = df_LST_UCELL.rename(columns={\"Logical RNC ID\": \"RNC ID\"})\n",
    "# >>> NUEVO: columna Gestor a partir de 'Origen' del LST <<<\n",
    "# 'Origen' existe en df_LST_UCELL_inicial porque lo agregaste al leer cada archivo.\n",
    "_gestor = df_LST_UCELL_inicial[\"Origen\"].str.extract(r\"LST UCELL_c(\\d+)\", expand=True).iloc[:, 0]\n",
    "df_LST_UCELL[\"Gestor\"] = np.where(_gestor.notna() & (_gestor != \"\"), \"MAE-\" + _gestor.astype(str), \"\")\n",
    "\n",
    "\n",
    "## DSP_UCELL\n",
    "df_DSP_UCELL = df_DSP_UCELL_inicial[[\n",
    "    \"Cell ID\",\"Cell Name\",\"Operation state\",\"Administrative state\",\"State explanation\"\n",
    "]]\n",
    "\n",
    "## LST_UCELLURA\n",
    "df_LST_UCELLURA = df_LST_UCELLURA_inicial[[\"Cell ID\",\"Cell Name\",\"URA ID\"]]\n",
    "df_LST_UCELLURA = df_LST_UCELLURA.rename(columns={\"URA ID\": \"URA\"})\n",
    "\n",
    "## Joins base\n",
    "df_Huawei_3G_inicial = df_LST_UCELL.merge(df_DSP_UCELL, on=[\"Cell ID\",\"Cell Name\"], how=\"left\")\n",
    "df_Huawei_3G_inicial = df_Huawei_3G_inicial.merge(df_LST_UCELLURA, on=[\"Cell ID\",\"Cell Name\"], how=\"left\")\n",
    "\n",
    "## Archivo anterior\n",
    "df_Huawei_3G_inicial = pd.merge(\n",
    "    df_Huawei_3G_inicial, df_All_Huawei_3G_Anterior,\n",
    "    left_on=[\"Cell ID\",\"Cell Name\"], right_on=[\"ANT Cell Id\",\"ANT Cell Name\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "## EPT por Cell Name\n",
    "df_Huawei_3G_inicial = df_Huawei_3G_inicial.merge(\n",
    "    df_EPT_unificado, on=\"Cell Name\", suffixes=(\"\", \"_EPT\"), how=\"left\"\n",
    ")\n",
    "\n",
    "# Fallback por Cell Name\n",
    "fallback_map = {\n",
    "    \"LAT\": \"LAT_EPT\",\n",
    "    \"LON\": \"LON_EPT\",\n",
    "    \"AT&T_Site_Name\": \"AT&T_Site_Name_EPT\"\n",
    "}\n",
    "\n",
    "for col, col_ept in fallback_map.items():\n",
    "    if col in df_Huawei_3G_inicial.columns and col_ept in df_Huawei_3G_inicial.columns:\n",
    "        # EPT = fuente primaria, AE (col) = respaldo\n",
    "        ept_vals = df_Huawei_3G_inicial[col_ept].replace(\"\", np.nan)\n",
    "        ae_vals  = df_Huawei_3G_inicial[col].replace(\"\", np.nan)\n",
    "        df_Huawei_3G_inicial[col] = ept_vals.combine_first(ae_vals)\n",
    "\n",
    "# (Opcional) Limpia columnas *_EPT\n",
    "df_Huawei_3G_inicial.drop(columns=[\"LAT_EPT\",\"LON_EPT\",\"AT&T_Site_Name_EPT\"],\n",
    "                          inplace=True, errors=\"ignore\")\n",
    "\n",
    "## Fallback adicional por NodeB Name usando df_EPT_nodes (preparado en el bloque EPT)\n",
    "if 'df_EPT_nodes' in globals() and not df_EPT_nodes.empty:\n",
    "    def _norm(s: str) -> str:\n",
    "        s = str(s)\n",
    "        return (s.upper().replace(\" \",\"\").replace(\"_\",\"\").replace(\"-\",\"\").replace(\".\",\"\").strip())\n",
    "\n",
    "    # Solo filas que aún falten LAT/LON/Site\n",
    "    mask_missing = (\n",
    "        df_Huawei_3G_inicial[[\"LAT\",\"LON\",\"AT&T_Site_Name\"]]\n",
    "        .replace(\"\", np.nan).isna().any(axis=1)\n",
    "    )\n",
    "    if mask_missing.any():\n",
    "        df_Huawei_3G_inicial.loc[mask_missing, \"node_key\"] = (\n",
    "            df_Huawei_3G_inicial.loc[mask_missing, \"NodeB Name\"].astype(str).map(_norm)\n",
    "        )\n",
    "\n",
    "        # Trae LAT/LON/Site por node_key\n",
    "        df_tmp = df_Huawei_3G_inicial.loc[mask_missing, [\"node_key\"]].merge(\n",
    "            df_EPT_nodes.rename(columns={\n",
    "                \"LAT\":\"LAT_ept_nb\",\n",
    "                \"LON\":\"LON_ept_nb\",\n",
    "                \"AT&T_Site_Name\":\"AT&T_Site_Name_ept_nb\"\n",
    "            }),\n",
    "            on=\"node_key\", how=\"left\"\n",
    "        )\n",
    "\n",
    "        # Escribe columnas temporales en el mismo slice\n",
    "        df_Huawei_3G_inicial.loc[mask_missing, [\"LAT_ept_nb\",\"LON_ept_nb\",\"AT&T_Site_Name_ept_nb\"]] = \\\n",
    "            df_tmp[[\"LAT_ept_nb\",\"LON_ept_nb\",\"AT&T_Site_Name_ept_nb\"]].values\n",
    "\n",
    "        # Rellena solo lo que sigue vacío\n",
    "        for col, nb_col in [(\"LAT\",\"LAT_ept_nb\"), (\"LON\",\"LON_ept_nb\"), (\"AT&T_Site_Name\",\"AT&T_Site_Name_ept_nb\")]:\n",
    "            if col in df_Huawei_3G_inicial.columns and nb_col in df_Huawei_3G_inicial.columns:\n",
    "                df_Huawei_3G_inicial[col] = (\n",
    "                    df_Huawei_3G_inicial[col].replace(\"\", np.nan).fillna(df_Huawei_3G_inicial[nb_col])\n",
    "                )\n",
    "\n",
    "        # Limpieza\n",
    "        df_Huawei_3G_inicial.drop(columns=[\"node_key\",\"LAT_ept_nb\",\"LON_ept_nb\",\"AT&T_Site_Name_ept_nb\"],\n",
    "                                  inplace=True, errors=\"ignore\")\n",
    "\n",
    "## \"En el gestor\"\n",
    "df_Huawei_3G_inicial[\"En el gestor\"] = np.where(\n",
    "    df_Huawei_3G_inicial[\"En el gestor\"].isna() | (df_Huawei_3G_inicial[\"En el gestor\"] == \"\"),\n",
    "    fecha_hoy, df_Huawei_3G_inicial[\"En el gestor\"]\n",
    ")\n",
    "\n",
    "## Orden\n",
    "df_Huawei_3G_ordenado = df_Huawei_3G_inicial.sort_values(by=[\"RNC\",\"NodeB Name\",\"Local Cell ID\"])\n",
    "\n",
    "## Consolidado\n",
    "df_Huawei_3G_ordenado[\"Consolidado\"] = np.where(\n",
    "    df_Huawei_3G_ordenado[\"NodeB Name\"].str.len() == 10, \"Si\", \"No\"\n",
    ")\n",
    "\n",
    "## NodeB Unique\n",
    "_name = df_Huawei_3G_ordenado[\"NodeB Name\"].astype(str).fillna(\"\").str.strip()\n",
    "is_new = _name.ne(_name.shift())\n",
    "df_Huawei_3G_ordenado[\"NodeB Unique\"] = np.where(is_new & _name.ne(\"\"), df_Huawei_3G_ordenado[\"NodeB Name\"], \"\")\n",
    "\n",
    "## Renombres finales\n",
    "df_Huawei_3G_ordenado = df_Huawei_3G_ordenado.rename(columns={\"Cell ID\": \"Cell Id\"})\n",
    "\n",
    "## Numeric-only casts\n",
    "df_Huawei_3G_ordenado = df_Huawei_3G_ordenado.apply(\n",
    "    lambda col: pd.to_numeric(col, errors=\"coerce\")\n",
    "    if not pd.to_numeric(col, errors=\"coerce\").isna().any()\n",
    "    else col\n",
    ")\n",
    "\n",
    "## Estructura final\n",
    "df_Huawei_3G = df_Huawei_3G_ordenado[[\n",
    "    \"RNC\",\"Cell Id\",\"Cell Name\",\"RNC ID\",\"Max Transmit Power of Cell\",\"Band Indicator\",\n",
    "    \"Cn Operator Group Index\",\"UL Frequency Ind\",\"Uplink UARFCN\",\"Downlink UARFCN\",\"Time Offset\",\n",
    "    \"Num of Continuous in Sync Ind\",\"Num of Continuous Out of Sync Ind\",\"Radio Link Failure Timer Length\",\n",
    "    \"DL Power Control Mode 1\",\"DL Primary Scrambling Code\",\"TX Diversity Indication\",\n",
    "    \"Service Priority Group Identity\",\"NodeB Name\",\"Local Cell ID\",\"Location Area Code\",\"Service Area Code\",\n",
    "    \"RAC Configuration Indication\",\"Routing Area Code\",\"STTD Support Indicator\",\"CP1 Support Indicator\",\n",
    "    \"Closed Loop Time Adjust Mode\",\"DPCH Tx Diversity Mode for Other User\",\"FDPCH Tx Diversity Mode for Other User\",\n",
    "    \"DPCH Tx Diversity Mode for MIMO User\",\"FDPCH Tx Diversity Mode for MIMO User\",\"Tx Diversity Mode for DC-HSDPA User\",\n",
    "    \"Cell Oriented Cell Individual Offset\",\"Cell VP Limit Indicator\",\"DSS Cell Flag\",\n",
    "    \"Maximum TX Power in Small DSS Coverage\",\"Common Channel Bandwidth Operator Index\",\n",
    "    \"Hierarchy ID of Terminal Type\",\"Subrack No.\",\"Subrack name\",\"Slot No.\",\"Subsystem No.\",\n",
    "    \"Heterogeneous Cell Flag\",\"HostType\",\"Validation indication\",\"Cell administrative state\",\"Cell MBMS state\",\n",
    "    \"Cell MIMO state\",\"IPDL flag\",\"Cell CBS state\",\"Cell ERACH state\",\"NodeB Unique\",\"LAT\",\"LON\",\n",
    "    \"Operation state\",\"Administrative state\",\"State explanation\",\"SSN\",\"En el gestor\",\"URA\",\"Consolidado\",\n",
    "    \"AT&T_Site_Name\", \"Gestor\"\n",
    "]]\n",
    "\n",
    "## Archivo Final\n",
    "ruta_salida = os.path.join(ruta_destino, f\"All_Huawei_3G_{fecha_ejecucion}.xlsx\")\n",
    "df_Huawei_3G.to_excel(ruta_salida, index=False, engine=\"openpyxl\")\n",
    "\n",
    "# Rotar encabezados\n",
    "wb = load_workbook(ruta_salida)\n",
    "ws = wb.active\n",
    "for col_num, column_title in enumerate(df_Huawei_3G.columns, 1):\n",
    "    cell = ws[f\"{get_column_letter(col_num)}1\"]\n",
    "    cell.alignment = Alignment(textRotation=90, horizontal=\"center\", vertical=\"bottom\")\n",
    "wb.save(ruta_salida)\n"
   ],
   "id": "3fdc40bc99d18f67",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T19:29:41.828840Z",
     "start_time": "2025-10-02T19:29:41.826022Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7ab6149a203c8ba9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
