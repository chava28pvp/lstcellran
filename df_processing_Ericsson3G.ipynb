{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-29T18:53:24.355139Z",
     "start_time": "2025-09-29T18:53:24.350479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Alignment, Font\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "# Ruta base (ajústala si cambia)\n",
    "BASE_DIR = Path(r\"C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\")\n",
    "\n",
    "# Lista de 31 encabezados, en el orden en que los quieres (eNBId queda en W si mantienes este orden)\n",
    "HEADERS = [\n",
    "     \"eNodeB Name\",\"CellName\",\"activePlmnList_mcc\",\"additionalPlmnList_mcc\",\n",
    "    \"administrativeState\",\"cellBarred\",\"cellId\",\"cellSubscriptionCapacity\",\n",
    "    \"channelSelectionSetSize\",\"dlChannelBandwidth\",\"earfcndl\",\"earfcnul\",\n",
    "    \"freqBand\",\"noOfPucchCqiUsers\",\"noOfPucchSrUsers\",\"operationalState\",\n",
    "    \"physicalLayerCellIdGroup\",\"physicalLayerSubCellId\",\"sectorCarrierRef\",\n",
    "    \"tac\",\"timeOfLastModification\",\"ulChannelBandwidth\",\n",
    "    \"eNBId\",\"eNodeB Name Unique\",\"LAT\",\"LON\",\"PCI\",\"AT&T_Site_Name\",\n",
    "    \"MOCN Activo por Celda\",\"Al menos una celda de MOCN encendida\",\"MME TEF\"\n",
    "]\n",
    "\n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T18:53:26.445411Z",
     "start_time": "2025-09-29T18:53:26.439132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def appendfiles(filenamepattern: str) -> str:\n",
    "    \"\"\"\n",
    "    Integra todos los TXT que matchean pattern + '_*.txt' en un solo archivo.\n",
    "    Devuelve el nombre del archivo integrado (sin ruta).\n",
    "    \"\"\"\n",
    "    searchpattern = str(BASE_DIR / f\"{filenamepattern}_*.txt\")\n",
    "    filestoread = glob.glob(searchpattern)\n",
    "\n",
    "    outputfile_name = f\"Integrated_{filenamepattern}_files.txt\"\n",
    "    output_path = BASE_DIR / outputfile_name\n",
    "\n",
    "    print(\"Buscando:\", searchpattern)\n",
    "    print(\"Archivos:\", filestoread)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as outputfile:\n",
    "        for name in filestoread:\n",
    "            with open(name, \"r\", encoding=\"utf-8\") as f:\n",
    "                outputfile.write(f.read())\n",
    "                print(\"Agregado:\", name)\n",
    "\n",
    "    print(\"Integrado =>\", outputfile_name)\n",
    "    return outputfile_name\n",
    "\n",
    "\n",
    "def cleanfile(filename: str, ignorelines=None) -> str:\n",
    "    \"\"\"\n",
    "    Elimina líneas que contengan cualquiera de los patrones indicados.\n",
    "    Devuelve el nombre del archivo limpio (sin ruta).\n",
    "    \"\"\"\n",
    "    if ignorelines is None:\n",
    "        ignorelines = [\"SubNetwork,\", \"instance(s)\", \"NodeId\"]\n",
    "\n",
    "    inputfile = BASE_DIR / filename\n",
    "    cleanfile_name = f\"Clean_{filename}\"\n",
    "    cleanfile_path = BASE_DIR / cleanfile_name\n",
    "\n",
    "    with open(inputfile, 'r', encoding=\"utf-8\") as f_in:\n",
    "        lines = f_in.readlines()\n",
    "\n",
    "    kept = []\n",
    "    for line in lines:\n",
    "        if any(p in line for p in ignorelines):\n",
    "            continue\n",
    "        kept.append(line)\n",
    "\n",
    "    with open(cleanfile_path, 'w', encoding=\"utf-8\") as f_out:\n",
    "        f_out.writelines(kept)\n",
    "\n",
    "    print(f\"Limpieza OK -> {cleanfile_name} ({len(kept)} líneas)\")\n",
    "    return cleanfile_name\n",
    "\n",
    "\n",
    "def convert_to_excel(cleanfile_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Lee TXT tab-delimited sin encabezados y guarda a Excel.\n",
    "    Devuelve el nombre del archivo Excel (sin ruta).\n",
    "    \"\"\"\n",
    "    cleanfile_path = BASE_DIR / cleanfile_name\n",
    "    out_xlsx = f\"Converted_{cleanfile_name}.xlsx\"\n",
    "    out_path = BASE_DIR / out_xlsx\n",
    "\n",
    "    df = pd.read_csv(cleanfile_path, delimiter='\\t', header=None)\n",
    "    df.to_excel(out_path, index=False, header=None)\n",
    "    print(f\"Convertido a Excel -> {out_xlsx}  (shape={df.shape})\")\n",
    "    return out_xlsx\n",
    "\n"
   ],
   "id": "6b2453460000e987",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T18:54:05.380883Z",
     "start_time": "2025-09-29T18:53:29.739878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# EUtranCellFDD\n",
    "eu_txt = appendfiles('EUtranCellFDD')\n",
    "eu_clean = cleanfile(eu_txt)\n",
    "eu_xlsx = convert_to_excel(eu_clean)\n",
    "\n",
    "# ENodeBFunction\n",
    "nb_txt = appendfiles('ENodeBFunction')\n",
    "nb_clean = cleanfile(nb_txt)\n",
    "nb_xlsx = convert_to_excel(nb_clean)\n",
    "\n",
    "# nodeid\n",
    "nd_txt = appendfiles('nodeid')\n",
    "nd_clean = cleanfile(nd_txt)\n",
    "nd_xlsx = convert_to_excel(nd_clean)\n",
    "\n",
    "# MME\n",
    "mme_txt = appendfiles('MME')\n",
    "mme_clean = cleanfile(mme_txt)\n",
    "mme_xlsx = convert_to_excel(mme_clean)\n",
    "#NblotCell\n",
    "nbiot_xlsx = appendfiles('NbIotCell')\n",
    "nbiot_clean = cleanfile(nbiot_xlsx)\n",
    "nbiot_xlsx = convert_to_excel(nbiot_clean)\n",
    "\n"
   ],
   "id": "822c1f2141799a7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\EUtranCellFDD_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\EUtranCellFDD_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\EUtranCellFDD_9.txt']\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\EUtranCellFDD_14.txt\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\EUtranCellFDD_9.txt\n",
      "Integrado => Integrated_EUtranCellFDD_files.txt\n",
      "Limpieza OK -> Clean_Integrated_EUtranCellFDD_files.txt (51830 líneas)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SCaracoza\\AppData\\Local\\Temp\\ipykernel_3540\\2274692285.py:62: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(cleanfile_path, delimiter='\\t', header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertido a Excel -> Converted_Clean_Integrated_EUtranCellFDD_files.txt.xlsx  (shape=(51814, 23))\n",
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\ENodeBFunction_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\ENodeBFunction_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\ENodeBFunction_9.txt']\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\ENodeBFunction_14.txt\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\ENodeBFunction_9.txt\n",
      "Integrado => Integrated_ENodeBFunction_files.txt\n",
      "Limpieza OK -> Clean_Integrated_ENodeBFunction_files.txt (6953 líneas)\n",
      "Convertido a Excel -> Converted_Clean_Integrated_ENodeBFunction_files.txt.xlsx  (shape=(6937, 3))\n",
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\nodeid_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\nodeid_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\nodeid_9.txt']\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\nodeid_14.txt\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\nodeid_9.txt\n",
      "Integrado => Integrated_nodeid_files.txt\n",
      "Limpieza OK -> Clean_Integrated_nodeid_files.txt (2658 líneas)\n",
      "Convertido a Excel -> Converted_Clean_Integrated_nodeid_files.txt.xlsx  (shape=(2648, 4))\n",
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\MME_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\MME_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\MME_9.txt']\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\MME_14.txt\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\MME_9.txt\n",
      "Integrado => Integrated_MME_files.txt\n",
      "Limpieza OK -> Clean_Integrated_MME_files.txt (39456 líneas)\n",
      "Convertido a Excel -> Converted_Clean_Integrated_MME_files.txt.xlsx  (shape=(39438, 9))\n",
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\NbIotCell_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\NbIotCell_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\NbIotCell_9.txt']\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\NbIotCell_14.txt\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\NbIotCell_9.txt\n",
      "Integrado => Integrated_NbIotCell_files.txt\n",
      "Limpieza OK -> Clean_Integrated_NbIotCell_files.txt (7529 líneas)\n",
      "Convertido a Excel -> Converted_Clean_Integrated_NbIotCell_files.txt.xlsx  (shape=(7517, 14))\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T18:54:44.797291Z",
     "start_time": "2025-09-29T18:54:14.045715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Archivo base desde la conversión de EUtranCellFDD\n",
    "wb = load_workbook(BASE_DIR / eu_xlsx)  # ej. Converted_Clean_Integrated_EUtranCellFDD_files.txt.xlsx\n",
    "ws = wb.active\n",
    "\n",
    "ultima_fila = ws.max_row\n",
    "\n",
    "# Mover B -> +30 columnas (B1:B{ultima_fila} => AF1:AF{ultima_fila})\n",
    "rango = f\"B1:B{ultima_fila}\"\n",
    "ws.move_range(rango, rows=0, cols=30)\n",
    "\n",
    "# Mover C:AF -> -1 columna (C..AF => B..AE)\n",
    "rango = f\"C1:AE{ultima_fila}\"\n",
    "ws.move_range(rango, rows=0, cols=-1)\n",
    "\n",
    "wb.save(BASE_DIR / \"Modified_workfile.xlsx\")\n",
    "print(\"Reacomodo OK -> Modified_workfile.xlsx\")\n",
    "\n"
   ],
   "id": "d88444e0446c642e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reacomodo OK -> Modified_workfile.xlsx\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T18:55:30.708057Z",
     "start_time": "2025-09-29T18:55:01.263877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Leemos el archivo reacomodado SIN headers\n",
    "df_base = pd.read_excel(BASE_DIR / \"Modified_workfile.xlsx\", header=None)\n",
    "\n",
    "# Verificación de columnas\n",
    "n_cols = df_base.shape[1]\n",
    "print(\"Columnas detectadas en Modified_workfile.xlsx:\", n_cols)\n",
    "\n",
    "# Si tu tabla reacomodada debe tener exactamente len(HEADERS) columnas:\n",
    "expected = len(HEADERS)\n",
    "if n_cols < expected:\n",
    "    # agrega columnas vacías para completar\n",
    "    for i in range(expected - n_cols):\n",
    "        df_base[f\"__tmp_empty_{i}\"] = pd.NA\n",
    "    n_cols = expected\n",
    "\n",
    "# Asigna nombres: si hay más columnas que headers, nómbralas para NO perderlas\n",
    "if n_cols > expected:\n",
    "    extra_names = [\"\"for i in range(n_cols - expected)]\n",
    "    df_base.columns = HEADERS + extra_names\n",
    "else:\n",
    "    df_base.columns = HEADERS\n",
    "\n",
    "# (Opcional) guardado de control sin formato\n",
    "df_base.to_excel(BASE_DIR / \"Modified_with_headers.xlsx\", index=False)\n",
    "print(\"Headers asignados en pandas -> Modified_with_headers_pandas.xlsx\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "d6659a16606a67c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas detectadas en Modified_workfile.xlsx: 32\n",
      "Headers asignados en pandas -> Modified_with_headers_pandas.xlsx\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T18:56:08.135870Z",
     "start_time": "2025-09-29T18:55:44.076885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_base = df_base.loc[:, [c for c in df_base.columns if c in HEADERS]]\n",
    "for c in HEADERS:\n",
    "    if c not in df_base.columns:\n",
    "        df_base[c] = pd.NA\n",
    "df_base = df_base[HEADERS]\n",
    "\n",
    "# --- 2) Cargar NbIot sin headers (A..N => 14 cols) ---\n",
    "df_nbiot = pd.read_excel(BASE_DIR / nbiot_xlsx, header=None, dtype=str)\n",
    "if df_nbiot.shape[1] < 14:\n",
    "    raise ValueError(\"NbIotCell debe tener al menos 14 columnas (A..N).\")\n",
    "df_nbiot = df_nbiot.iloc[:, :14]\n",
    "\n",
    "# --- 3) Construir bloque NbIot alineado por POSICIÓN (tu mapeo) ---\n",
    "# A->A, B->AF, C->B, D->C, E->E, F->F, G->G, H->K, I->L, J->P, K->R, L->D, M->S, N->T\n",
    "df_nb = pd.DataFrame(pd.NA, index=df_nbiot.index, columns=HEADERS)\n",
    "df_nb[\"eNodeB Name\"]               = df_nbiot.iloc[:, 0]   # A -> A\n",
    "df_nb[\"CellName\"]                  = df_nbiot.iloc[:, 2]   # C -> B\n",
    "df_nb[\"activePlmnList_mcc\"]        = df_nbiot.iloc[:, 3]   # D -> C\n",
    "df_nb[\"administrativeState\"]       = df_nbiot.iloc[:, 4]   # E -> E\n",
    "df_nb[\"cellBarred\"]                = df_nbiot.iloc[:, 5]   # F -> F\n",
    "df_nb[\"cellId\"]                    = df_nbiot.iloc[:, 6]   # G -> G\n",
    "df_nb[\"earfcndl\"]                  = df_nbiot.iloc[:, 7]   # H -> K\n",
    "df_nb[\"earfcnul\"]                  = df_nbiot.iloc[:, 8]   # I -> L\n",
    "df_nb[\"operationalState\"]          = df_nbiot.iloc[:, 9]   # J -> P\n",
    "df_nb[\"physicalLayerCellIdGroup\"]    = df_nbiot.iloc[:,10]   # K -> R\n",
    "df_nb[\"additionalPlmnList_mcc\"]    = df_nbiot.iloc[:,11]   # L -> D\n",
    "df_nb[\"sectorCarrierRef\"]          = df_nbiot.iloc[:,12]   # M -> S\n",
    "df_nb[\"tac\"]                       = df_nbiot.iloc[:,13]   # N -> T\n",
    "\n",
    "# --- 4) Pegar debajo y guardar ---\n",
    "df_out = pd.concat([df_base, df_nb], ignore_index=True)\n",
    "df_out.to_excel(BASE_DIR / \"Modified_with_headers.xlsx\", index=False)\n",
    "print(\"OK: NbIotCell agregado debajo, con B->AF y el resto según mapeo por posición.\")"
   ],
   "id": "941410eb832d68a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: NbIotCell agregado debajo, con B->AF y el resto según mapeo por posición.\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T19:01:02.788968Z",
     "start_time": "2025-09-29T18:56:57.850999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- eNBId desde ENodeBFunction ---\n",
    "df_nodeb = pd.read_excel(BASE_DIR / nb_xlsx, header=None, usecols=[0, 1, 2])\n",
    "# Lee el archivo Excel convertido de ENodeBFunction (ruta 'nb_xlsx'),\n",
    "# sin encabezado (header=None), y solo las 3 primeras columnas (0,1,2).\n",
    "\n",
    "df_nodeb.columns = [\"NodeId\", \"ENodeBFunctionId\", \"eNBIdnew\"]\n",
    "# Asigna nombres a las 3 columnas: NodeId (clave), ENodeBFunctionId (solo informativo),\n",
    "# y eNBIdnew (el valor que queremos traer por JOIN).\n",
    "\n",
    "df_out[\"eNodeB Name\"] = df_out[\"eNodeB Name\"].astype(str)\n",
    "# Asegura que la columna clave en df_out sea string (evita mismatches de tipo).\n",
    "\n",
    "df_nodeb[\"NodeId\"] = df_nodeb[\"NodeId\"].astype(str)\n",
    "# Asegura que la clave en el catálogo (NodeId) también sea string.\n",
    "\n",
    "df_nodeb = df_nodeb.drop_duplicates(subset=[\"NodeId\"], keep=\"first\")\n",
    "# Si hay filas duplicadas por NodeId en el catálogo, conserva la primera\n",
    "# para evitar que el merge genere duplicados.\n",
    "\n",
    "df_tmp = df_out.merge(df_nodeb[[\"NodeId\", \"eNBIdnew\"]],\n",
    "                       left_on=\"eNodeB Name\", right_on=\"NodeId\", how=\"left\")\n",
    "# LEFT JOIN: por cada fila de df_out, busca en df_nodeb la fila con el mismo NodeId.\n",
    "# - Clave izquierda: eNodeB Name (df_out)\n",
    "# - Clave derecha: NodeId (df_nodeb)\n",
    "# - how=\"left\": conserva todas las filas de df_out aunque no haya match.\n",
    "\n",
    "df_out[\"eNBId\"] = df_tmp[\"eNBIdnew\"]\n",
    "# Copia (asigna) a df_out la columna eNBId con el valor traído (eNBIdnew).\n",
    "# Nota: si no hubo match, quedará NaN.\n",
    "\n",
    "# --- eNodeB Name Unique (solo cuando cambia) ---\n",
    "#df_out es mi df main\n",
    "_name = df_out[\"eNodeB Name\"].astype(str).fillna(\"\").str.strip()\n",
    "# Toma la columna eNodeB Name, la convierte a str, reemplaza NaN por\"\",\n",
    "# y recorta espacios en extremos para comparar bien.\n",
    "\n",
    "is_new = _name.ne(_name.shift())\n",
    "# Crea una serie booleana True/False que vale True cuando\n",
    "# el nombre actual es diferente al de la fila anterior (inicio de bloque).\n",
    "\n",
    "df_out[\"eNodeB Name Unique\"] = np.where(is_new & _name.ne(\"\"), df_out[\"eNodeB Name\"], \"\")\n",
    "# Si cambia el nombre (is_new=True) y no está vacío: escribe el nombre.\n",
    "# En caso contrario: deja cadena vacía\"\" (equivalente a tu SI(A2=A1,\"\",A2)).\n",
    "\n",
    "# --- LAT/LON/AT&T_Site_Name desde All_Ericsson_4G_{YYYYMM} (mes anterior) ---\n",
    "# YYYYMM del mes anterior\n",
    "# 1) YYYYMM del mes anterior\n",
    "# 1) YYYYMM del mes anterior\n",
    "today = date.today()\n",
    "prev_year  = today.year if today.month > 1 else today.year - 1\n",
    "prev_month = today.month - 1 or 12\n",
    "yyyymm = f\"{prev_year}{prev_month:02d}\"\n",
    "\n",
    "# 2) Fuente principal: All_Ericsson\n",
    "ae_path = BASE_DIR / f\"All_Ericsson_4G_{yyyymm}.xlsx\"\n",
    "ae_df = pd.read_excel(ae_path, usecols=[\"eNodeB Name\", \"LAT\", \"LON\", \"AT&T_Site_Name\"])\n",
    "ae_df[\"eNodeB Name\"] = ae_df[\"eNodeB Name\"].astype(str).str.strip()\n",
    "ae_df = ae_df.drop_duplicates(subset=[\"eNodeB Name\"], keep=\"first\")\n",
    "\n",
    "df_out[\"eNodeB Name\"] = df_out[\"eNodeB Name\"].astype(str).str.strip()\n",
    "\n",
    "merged = df_out.merge(ae_df, on=\"eNodeB Name\", how=\"left\", suffixes=(\"\", \"_ae\"))\n",
    "def _is_blank(s):\n",
    "    # NaN o string vacío (incluye espacios)\n",
    "    return s.isna() | s.astype(str).str.strip().eq(\"\")\n",
    "\n",
    "for col in [\"LAT\", \"LON\", \"AT&T_Site_Name\"]:\n",
    "    m = _is_blank(merged[col])\n",
    "    merged.loc[m, col] = merged.loc[m, col + \"_ae\"]\n",
    "    merged.drop(columns=[col + \"_ae\"], inplace=True)\n",
    "\n",
    "# (Opcional) armoniza dtypes sin downcasting silencioso\n",
    "merged = merged.infer_objects(copy=False)\n",
    "def coalesce_to_single_column(df, variants, target):\n",
    "    \"\"\"\n",
    "    Une varias columnas posibles (variants) en una sola columna 'target',\n",
    "    tomando el primer no-nulo por fila y eliminando las columnas sobrantes.\n",
    "    \"\"\"\n",
    "    present = [c for c in variants if c in df.columns]\n",
    "    if not present:\n",
    "        return df\n",
    "    if target in present:\n",
    "        cols = [target] + [c for c in present if c != target]\n",
    "    else:\n",
    "        cols = present\n",
    "    merged_series = df[cols].bfill(axis=1).iloc[:, 0]\n",
    "    df[target] = merged_series\n",
    "    to_drop = [c for c in present if c != target]\n",
    "    df.drop(columns=to_drop, inplace=True, errors=\"ignore\")\n",
    "    return df\n",
    "\n",
    "faltan = (\n",
    "    _is_blank(merged[\"LAT\"]) |\n",
    "    _is_blank(merged[\"LON\"]) |\n",
    "    _is_blank(merged[\"AT&T_Site_Name\"])\n",
    ")\n",
    "\n",
    "if not faltan.any():\n",
    "    # All_Ericsson llenó todo → no usar EPT\n",
    "    df_out = merged\n",
    "    print(\"All_Ericsson cubrió 100% (LAT/LON/AT&T_Site_Name). Se omite fallback EPT.\")\n",
    "else:\n",
    "    print(f\"Quedan {int(faltan.sum())} filas con faltantes. Se aplica fallback EPT…\")\n",
    "\n",
    "    # 3) Fallback EPT (solo completa donde aún falte)\n",
    "    ept_matches = glob.glob(str(BASE_DIR / \"EPT_ATT_UMTS_LTE_*.xlsx\"))\n",
    "    if ept_matches:\n",
    "        ept_file = ept_matches[0]\n",
    "        ept_sheets = [\"EPT_3G_LTE_OUTDOOR\", \"PLAN_OUTDOOR\", \"EPT_3G_LTE_INDOOR\", \"PLAN_INDOOR\", \"Eventos_Especiales\"]\n",
    "\n",
    "        frames = []\n",
    "        for sh in ept_sheets:\n",
    "            try:\n",
    "                tmp = pd.read_excel(ept_file, sheet_name=sh, engine=\"openpyxl\")\n",
    "                frames.append(tmp)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if frames:\n",
    "            ept_df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "            # --- Unificar columnas variantes ---\n",
    "            ept_df = coalesce_to_single_column(ept_df, [\"Cell Name\", \"CellName\", \"ATT_CELL_ID_Name\"], \"Cell Name\")\n",
    "            ept_df = coalesce_to_single_column(ept_df, [\"Latitud\"], \"LAT\")\n",
    "            ept_df = coalesce_to_single_column(ept_df, [\"Longitud\"], \"LON\")\n",
    "            ept_df = coalesce_to_single_column(ept_df, [\"AT&T_Site_Name\"], \"AT&T_Site_Name\")\n",
    "\n",
    "           # Forzar LAT/LON a numérico (evita FutureWarning)\n",
    "            for c in [\"LAT\", \"LON\"]:\n",
    "                if c in ept_df.columns:\n",
    "                    ept_df[c] = (\n",
    "                        ept_df[c]\n",
    "                        .astype(str)\n",
    "                        .str.strip(\"[]\")\n",
    "                        .str.replace(\",\", \"\", regex=False)\n",
    "                    )\n",
    "                    ept_df[c] = pd.to_numeric(ept_df[c], errors=\"coerce\")\n",
    "\n",
    "            # Limpiar espacios en SiteName y Cell Name\n",
    "            for c in [\"Cell Name\", \"AT&T_Site_Name\"]:\n",
    "                if c in ept_df.columns:\n",
    "                    ept_df[c] = ept_df[c].astype(str).str.strip()\n",
    "\n",
    "            ept_df = ept_df.drop_duplicates(subset=[\"Cell Name\"], keep=\"first\")\n",
    "\n",
    "            # --- asegurar llave en tu base ---\n",
    "            created_temp_key = False\n",
    "            if \"Cell Name\" not in merged.columns:\n",
    "                if \"CellName\" in merged.columns:\n",
    "                    merged[\"Cell Name\"] = merged[\"CellName\"]\n",
    "                    created_temp_key = True\n",
    "                else:\n",
    "                    raise KeyError(\"No se encontró 'Cell Name' ni 'CellName' en el DataFrame base para hacer el join con EPT.\")\n",
    "\n",
    "            merged[\"Cell Name\"] = merged[\"Cell Name\"].astype(str).str.strip()\n",
    "\n",
    "            # Lookup desde EPT\n",
    "            ept_lookup = ept_df[[\"Cell Name\", \"LAT\", \"LON\", \"AT&T_Site_Name\"]].rename(columns={\n",
    "                \"LAT\": \"LAT_ept\",\n",
    "                \"LON\": \"LON_ept\",\n",
    "                \"AT&T_Site_Name\": \"AT&T_Site_Name_ept\"\n",
    "            })\n",
    "\n",
    "            # Merge\n",
    "            merged = merged.merge(ept_lookup, on=\"Cell Name\", how=\"left\")\n",
    "\n",
    "            # Completar SOLO faltantes\n",
    "            need_lat  = _is_blank(merged[\"LAT\"]) if \"LAT\" in merged.columns else pd.Series(False, index=merged.index)\n",
    "            need_lon  = _is_blank(merged[\"LON\"]) if \"LON\" in merged.columns else pd.Series(False, index=merged.index)\n",
    "            need_site = _is_blank(merged[\"AT&T_Site_Name\"]) if \"AT&T_Site_Name\" in merged.columns else pd.Series(False, index=merged.index)\n",
    "\n",
    "            if \"LAT_ept\" in merged: merged.loc[need_lat,  \"LAT\"] = merged.loc[need_lat,  \"LAT_ept\"]\n",
    "            if \"LON_ept\" in merged: merged.loc[need_lon,  \"LON\"] = merged.loc[need_lon,  \"LON_ept\"]\n",
    "            if \"AT&T_Site_Name_ept\" in merged: merged.loc[need_site, \"AT&T_Site_Name\"] = merged.loc[need_site, \"AT&T_Site_Name_ept\"]\n",
    "\n",
    "            # Limpieza\n",
    "            merged.drop(columns=[c for c in [\"LAT_ept\", \"LON_ept\", \"AT&T_Site_Name_ept\"] if c in merged.columns], inplace=True)\n",
    "            if created_temp_key:\n",
    "                merged.drop(columns=[\"Cell Name\"], inplace=True)\n",
    "\n",
    "            print(\"Fallback EPT aplicado por 'Cell Name':\", ept_file)\n",
    "        else:\n",
    "            print(\"No se pudieron leer hojas del EPT; se omite fallback.\")\n",
    "    else:\n",
    "        print(\"No se encontró archivo EPT_ATT_UMTS_LTE_*.xlsx; se omite fallback.\")\n",
    "\n",
    "# 4) Guardar para que tu celda de formateo final (headers/estilos) tome este resultado\n",
    "final_path = BASE_DIR / \"Datos_Modified.xlsx\"\n",
    "merged.to_excel(final_path, index=False)\n",
    "print(\"Guardado con All_Ericsson + fallback EPT ->\", final_path)\n",
    "\n",
    "# --- Calcular PCI = IF(R blank, Q, Q*3 + R) y guardar ---\n",
    "q_col = \"physicalLayerCellIdGroup\"\n",
    "r_col = \"physicalLayerSubCellId\"\n",
    "\n",
    "merged[q_col] = pd.to_numeric(merged[q_col], errors=\"coerce\")\n",
    "merged[r_col] = pd.to_numeric(merged[r_col].astype(str).str.strip().replace({\"\": None}), errors=\"coerce\")\n",
    "\n",
    "merged[\"PCI\"] = pd.Series(\n",
    "    np.where(merged[r_col].isna(),\n",
    "             merged[q_col],\n",
    "             merged[q_col]*3 + merged[r_col]),\n",
    "    index=merged.index,\n",
    "    dtype=\"Int64\"\n",
    ")\n",
    "\n",
    "# --- Marcar \"MOCN Activo por Celda\" (match EXACTO) y guardar ---\n",
    "# Cadena exacta contra la que quieres comparar\n",
    "pattern = \"[{mncLength=3, mcc=334, mnc=90}, {mncLength=2, mcc=334, mnc=3}, {mncLength=2, mcc=1, mnc=1}, {mncLength=2, mcc=1, mnc=1}, {mncLength=2, mcc=1, mnc=1}]\"\n",
    "\n",
    "# Crear/actualizar la columna \"MOCN Activo por Celda\"\n",
    "merged[\"MOCN Activo por Celda\"] = np.where(\n",
    "    merged[\"additionalPlmnList_mcc\"].astype(str).str.strip() == pattern,\n",
    "    \"Si\",\n",
    "    \"No\"\n",
    ")\n",
    "# --- \"Al menos una celda de MOCN encendida\" basado en match eNodeB Name ∈ {AT&T_Site_Name con MOCN=SI} ---\n",
    "# === \"Al menos una celda de MOCN encendida\" basado en eNodeB Name con MOCN=Si ===\n",
    "truthy = {\"si\", \"sí\", \"yes\", \"true\", \"1\"}\n",
    "\n",
    "# Normalizar columnas necesarias\n",
    "enb = merged[\"eNodeB Name\"].astype(str).str.strip()\n",
    "mocn = merged[\"MOCN Activo por Celda\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "# 1) Conjunto de eNodeB Name que tienen MOCN = \"Si\" (o equivalentes en truthy)\n",
    "enbs_con_mocn = set(enb[mocn.isin(truthy)])\n",
    "\n",
    "# 2) Marcar en cada fila si su eNodeB Name está en ese conjunto\n",
    "merged[\"Al menos una celda de MOCN encendida\"] = np.where(\n",
    "    enb.isin(enbs_con_mocn),\n",
    "    \"Si\",\n",
    "    \"No\"\n",
    ")\n",
    "# --- eNBId desde MME ---\n",
    "df_MME = pd.read_excel(BASE_DIR / mme_xlsx, header=None, usecols=[0,1,2], dtype=str)\n",
    "df_MME.columns = [\"NodeId\", \"eNodeBFunction\", \"TermPointToMmeId\"]\n",
    "df_MME[\"NodeId\"] = df_MME[\"NodeId\"].astype(str).str.strip()\n",
    "\n",
    "# Filtrar solo NodeId de longitud 7\n",
    "df_MME_7 = df_MME[df_MME[\"TermPointToMmeId\"].str.len() == 7].copy()\n",
    "\n",
    "# Conteo tipo COUNTIF: cuántas veces aparece cada NodeId\n",
    "mme_counts = df_MME_7[\"NodeId\"].value_counts()\n",
    "\n",
    "# Normalizar clave y mapear conteo a tu base por eNodeB Name\n",
    "df_out[\"eNodeB Name\"] = df_out[\"eNodeB Name\"].astype(str).str.strip()\n",
    "merged[\"MME TEF\"] = (\n",
    "    df_out[\"eNodeB Name\"].map(mme_counts).fillna(0).astype(\"Int64\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "df_out = merged\n",
    "# Actualiza df_out con el DataFrame enriquecido.\n",
    "\n",
    "# --- Guardar preliminar (lo tomará la [7] para formateo final) ---\n",
    "final_path = BASE_DIR / \"Datos_Modified.xlsx\"\n",
    "# Define la ruta del Excel preliminar (sin estilos).\n",
    "\n",
    "df_out.to_excel(final_path, index=False)\n",
    "# Escribe el Excel con todas las columnas (aquí todavía sin formato openpyxl).\n",
    "\n",
    "print(\"Guardado enriquecido ->\", final_path, \"shape=\", df_out.shape)\n",
    "# Log: confirma guardado y muestra dimensiones finales.\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "dd34249ef17f6f73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quedan 1 filas con faltantes. Se aplica fallback EPT…\n",
      "Fallback EPT aplicado por 'Cell Name': C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\EPT_ATT_UMTS_LTE_2025-08-14.xlsx\n",
      "Guardado con All_Ericsson + fallback EPT -> C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\Datos_Modified.xlsx\n",
      "Guardado enriquecido -> C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\Datos_Modified.xlsx shape= (59331, 31)\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T19:03:04.321042Z",
     "start_time": "2025-09-29T19:01:50.239787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "final_excel = BASE_DIR / \"Datos_Modified.xlsx\"\n",
    "tmp_excel = BASE_DIR / \"~tmp_Datos_Modified.xlsx\"\n",
    "\n",
    "# Releer, forzar columnas y orden\n",
    "df_out = pd.read_excel(final_excel)\n",
    "\n",
    "# Garantiza que TODAS las columnas existan\n",
    "for col in HEADERS:\n",
    "    if col not in df_out.columns:\n",
    "        df_out[col] = pd.NA\n",
    "\n",
    "# Reordena exactamente como HEADERS\n",
    "df_out = df_out[HEADERS]\n",
    "\n",
    "# Escribe temporal\n",
    "df_out.to_excel(tmp_excel, index=False)\n",
    "\n",
    "# Reaplicar estilo vertical de headers\n",
    "wb = load_workbook(tmp_excel)\n",
    "ws = wb.active\n",
    "\n",
    "# Congelar encabezado\n",
    "ws.freeze_panes = \"A2\"\n",
    "\n",
    "# Aplicar estilo a fila 1\n",
    "for col_idx, header in enumerate(HEADERS, start=1):\n",
    "    cell = ws.cell(row=1, column=col_idx)\n",
    "    cell.value = header\n",
    "    cell.font = Font(name=\"Aptos Narrow\", size=11)  # bold=True si quieres negrita\n",
    "    cell.alignment = Alignment(textRotation=90, horizontal=\"center\", vertical=\"bottom\", wrap_text=True)\n",
    "\n",
    "wb.save(final_excel)\n",
    "\n",
    "# Limpia temporal\n",
    "try:\n",
    "    tmp_excel.unlink()\n",
    "except Exception as e:\n",
    "    print(\"No se pudo borrar temporal:\", e)\n",
    "\n",
    "print(\"Ajuste final OK -> Headers verticales y columnas forzadas/ordenadas.\")\n",
    "\n"
   ],
   "id": "cdc94f3d0b991d80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajuste final OK -> Headers verticales y columnas forzadas/ordenadas.\n"
     ]
    }
   ],
   "execution_count": 49
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
