{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-01T17:51:05.135198Z",
     "start_time": "2025-10-01T17:51:04.023330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Alignment, Font\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "# Ruta base (ajústala si cambia)\n",
    "BASE_DIR = Path(r\"C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\3G\")\n",
    "\n",
    "# Lista de 31 encabezados, en el orden en que los quieres (eNBId queda en W si mantienes este orden)\n",
    "HEADERS = [\n",
    "     \"RNC\",\"administrativeState\",\"cId\",\"iubLinkRef\",\n",
    "    \"localCellId\",\"lac\",\"maximumTransmissionPower\",\"maxPwrMax\",\n",
    "    \"mocnCellProfileRef\",\"operationalState\",\"primaryCpichPower\",\"primaryScramblingCode\",\n",
    "    \"rac\",\"sac\",\"tCell\",\"uarfcnDl\",\n",
    "    \"uarfcnUl\",\"uraList\",\"UtranCellId\",\n",
    "    \"NodeB\",\"NodeBUnique\",\"LAT\",\"LON\",\"AT&T_Site_Name\",\n",
    "    \"Node_B_ID\"\n",
    "]\n",
    "\n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T17:51:10.926063Z",
     "start_time": "2025-10-01T17:51:10.918555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def appendfiles(filenamepattern: str) -> str:\n",
    "    \"\"\"\n",
    "    Integra todos los TXT que matchean pattern + '_*.txt' en un solo archivo.\n",
    "    Devuelve el nombre del archivo integrado (sin ruta).\n",
    "    \"\"\"\n",
    "    searchpattern = str(BASE_DIR / f\"{filenamepattern}_*.txt\")\n",
    "    filestoread = glob.glob(searchpattern)\n",
    "\n",
    "    outputfile_name = f\"Integrated_{filenamepattern}_files.txt\"\n",
    "    output_path = BASE_DIR / outputfile_name\n",
    "\n",
    "    print(\"Buscando:\", searchpattern)\n",
    "    print(\"Archivos:\", filestoread)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as outputfile:\n",
    "        for name in filestoread:\n",
    "            with open(name, \"r\", encoding=\"utf-8\") as f:\n",
    "                outputfile.write(f.read())\n",
    "                print(\"Agregado:\", name)\n",
    "\n",
    "    print(\"Integrado =>\", outputfile_name)\n",
    "    return outputfile_name\n",
    "\n",
    "\n",
    "def cleanfile(filename: str, ignorelines=None) -> str:\n",
    "    \"\"\"\n",
    "    Elimina líneas que contengan cualquiera de los patrones indicados.\n",
    "    Devuelve el nombre del archivo limpio (sin ruta).\n",
    "    \"\"\"\n",
    "    if ignorelines is None:\n",
    "        ignorelines = [\"SubNetwork,\", \"instance(s)\", \"NodeId\"]\n",
    "\n",
    "    inputfile = BASE_DIR / filename\n",
    "    cleanfile_name = f\"Clean_{filename}\"\n",
    "    cleanfile_path = BASE_DIR / cleanfile_name\n",
    "\n",
    "    with open(inputfile, 'r', encoding=\"utf-8\") as f_in:\n",
    "        lines = f_in.readlines()\n",
    "\n",
    "    kept = []\n",
    "    for line in lines:\n",
    "        if any(p in line for p in ignorelines):\n",
    "            continue\n",
    "        kept.append(line)\n",
    "\n",
    "    with open(cleanfile_path, 'w', encoding=\"utf-8\") as f_out:\n",
    "        f_out.writelines(kept)\n",
    "\n",
    "    print(f\"Limpieza OK -> {cleanfile_name} ({len(kept)} líneas)\")\n",
    "    return cleanfile_name\n",
    "\n",
    "\n",
    "def convert_to_excel(cleanfile_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Lee TXT tab-delimited sin encabezados y guarda a Excel.\n",
    "    Devuelve el nombre del archivo Excel (sin ruta).\n",
    "    \"\"\"\n",
    "    cleanfile_path = BASE_DIR / cleanfile_name\n",
    "    out_xlsx = f\"Converted_{cleanfile_name}.xlsx\"\n",
    "    out_path = BASE_DIR / out_xlsx\n",
    "\n",
    "    df = pd.read_csv(cleanfile_path, delimiter='\\t', header=None)\n",
    "    df.to_excel(out_path, index=False, header=None)\n",
    "    print(f\"Convertido a Excel -> {out_xlsx}  (shape={df.shape})\")\n",
    "    return out_xlsx\n",
    "\n"
   ],
   "id": "6b2453460000e987",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T17:51:22.521657Z",
     "start_time": "2025-10-01T17:51:14.651594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# EUtranCellFDD\n",
    "eu_txt = appendfiles('UtranCell')\n",
    "eu_clean = cleanfile(eu_txt)\n",
    "eu_xlsx = convert_to_excel(eu_clean)\n",
    "\n",
    "# nodeid\n",
    "nd_txt = appendfiles('nodeid')\n",
    "nd_clean = cleanfile(nd_txt)\n",
    "nd_xlsx = convert_to_excel(nd_clean)\n",
    "\n",
    "\n"
   ],
   "id": "822c1f2141799a7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\3G\\UtranCell_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\3G\\\\UtranCell_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\3G\\\\UtranCell_9.txt']\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\3G\\UtranCell_14.txt\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\3G\\UtranCell_9.txt\n",
      "Integrado => Integrated_UtranCell_files.txt\n",
      "Limpieza OK -> Clean_Integrated_UtranCell_files.txt (24714 líneas)\n",
      "Convertido a Excel -> Converted_Clean_Integrated_UtranCell_files.txt.xlsx  (shape=(24706, 21))\n",
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\3G\\nodeid_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\3G\\\\nodeid_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\3G\\\\nodeid_9.txt']\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\3G\\nodeid_14.txt\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\3G\\nodeid_9.txt\n",
      "Integrado => Integrated_nodeid_files.txt\n",
      "Limpieza OK -> Clean_Integrated_nodeid_files.txt (2658 líneas)\n",
      "Convertido a Excel -> Converted_Clean_Integrated_nodeid_files.txt.xlsx  (shape=(2648, 4))\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T17:51:37.364770Z",
     "start_time": "2025-10-01T17:51:25.026799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Archivo base desde la conversión de EUtranCellFDD\n",
    "wb = load_workbook(BASE_DIR / eu_xlsx)  # ej. Converted_Clean_Integrated_UtranCell_files.txt.xlsx\n",
    "ws = wb.active\n",
    "\n",
    "ultima_fila = ws.max_row\n",
    "\n",
    "# Mover B -> +30 columnas (B1:B{ultima_fila} => AF1:AF{ultima_fila})\n",
    "rango = f\"B1:B{ultima_fila}\"\n",
    "ws.move_range(rango, rows=0, cols=30)\n",
    "\n",
    "# Mover C:AF -> -1 columna (C..AF => B..AE)\n",
    "rango = f\"C1:AE{ultima_fila}\"\n",
    "ws.move_range(rango, rows=0, cols=-1)\n",
    "\n",
    "wb.save(BASE_DIR / \"Modified_workfile.xlsx\")\n",
    "print(\"Reacomodo OK -> Modified_workfile.xlsx\")\n",
    "\n"
   ],
   "id": "d88444e0446c642e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reacomodo OK -> Modified_workfile.xlsx\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T17:51:53.660176Z",
     "start_time": "2025-10-01T17:51:41.800103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Leemos el archivo reacomodado SIN headers\n",
    "df_base = pd.read_excel(BASE_DIR / \"Modified_workfile.xlsx\", header=None)\n",
    "\n",
    "# --- mover B -> S (índices base 0: B=1, S=18) ---\n",
    "# Asegura que exista al menos la columna S\n",
    "while df_base.shape[1] < 20:   # necesitas al menos 20 columnas para llegar a la S (19 base 0)\n",
    "    df_base[df_base.shape[1]] = pd.NA\n",
    "\n",
    "src_idx = 1      # B\n",
    "dst_idx = 19     # S\n",
    "\n",
    "# Tomamos la serie de B y la quitamos\n",
    "colB = df_base.iloc[:, src_idx]\n",
    "df_base.drop(df_base.columns[src_idx], axis=1, inplace=True)\n",
    "\n",
    "# Ajusta destino si venía después del origen\n",
    "if dst_idx > src_idx:\n",
    "    dst_idx -= 1\n",
    "\n",
    "# Inserta la antigua B en la posición S (desplaza a la derecha lo que estaba ahí)\n",
    "df_base.insert(dst_idx, colB.name, colB)\n",
    "# --- fin mover B -> S ---\n",
    "\n",
    "# --- Ajuste de columnas según HEADERS ---\n",
    "expected = len(HEADERS)\n",
    "\n",
    "# Si faltan columnas, rellena con vacías\n",
    "if df_base.shape[1] < expected:\n",
    "    for i in range(expected - df_base.shape[1]):\n",
    "        df_base[df_base.shape[1]] = pd.NA\n",
    "\n",
    "# Si sobran columnas, recorta\n",
    "if df_base.shape[1] > expected:\n",
    "    extra_cols = df_base.shape[1] - expected\n",
    "    print(f\"⚠️ Se detectaron {extra_cols} columnas extra. Serán descartadas.\")\n",
    "    df_base = df_base.iloc[:, :expected]\n",
    "\n",
    "# Asigna nombres finales\n",
    "df_base.columns = HEADERS\n",
    "\n",
    "# Guarda el resultado\n",
    "out_path = BASE_DIR / \"Modified_with_headers.xlsx\"\n",
    "df_base.to_excel(out_path, index=False)\n",
    "print(\"Archivo final generado:\", out_path)\n"
   ],
   "id": "d6659a16606a67c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Se detectaron 7 columnas extra. Serán descartadas.\n",
      "Archivo final generado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\3G\\Modified_with_headers.xlsx\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T17:52:06.174783Z",
     "start_time": "2025-10-01T17:51:57.564207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Columnas con cadenas estilo \"k1=v1,k2=v2,...\"\n",
    "cols_mo = [\n",
    "    \"iubLinkRef\",       # ejemplo: ... ,IubLink=Iub_0420  -> Iub_0420\n",
    "    \"lac\",              # ... ,LocationArea=41416         -> 41416\n",
    "    \"mocnCellProfileRef\",# ... ,MocnCellProfile=Telefonica -> Telefonica\n",
    "    \"rac\",              # ... ,RoutingArea=146            -> 146\n",
    "    \"sac\",              # ... ,ServiceArea=65055          -> 65055\n",
    "    \"uraList\",          # [ ...,Ura=146]                  -> 146\n",
    "]\n",
    "\n",
    "def extract_last_token_after_equal(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Toma una Serie de strings y devuelve el segmento que está después del último '='.\n",
    "    Limpiaespacios, corchetes y comas/paréntesis finales si los hubiera.\n",
    "    \"\"\"\n",
    "    out = (\n",
    "        s.astype(str)\n",
    "         # divide solo una vez desde la derecha: ['prefix', 'ultimo_valor']\n",
    "         .str.rsplit('=', n=1).str[-1]\n",
    "         .str.strip()                  # quita espacios\n",
    "         .str.strip('[]()')            # quita corchetes/paréntesis de extremos\n",
    "         .str.replace(r'[,\\;]$', '', regex=True)  # quita coma/; final si existe\n",
    "    )\n",
    "    # Cuando no había '=', rsplit devuelve todo; si quieres NA en esos casos:\n",
    "    has_equal = s.astype(str).str.contains('=', regex=False)\n",
    "    out = out.where(has_equal, other=pd.NA)\n",
    "    return out\n",
    "\n",
    "# Creamos columnas normalizadas (puedes sobreescribir las originales si prefieres)\n",
    "for c in cols_mo:\n",
    "    df_base[c] = extract_last_token_after_equal(df_base[c])\n",
    "\n",
    "# Convierte a numérico donde corresponde (las que deben ser números):\n",
    "cols_numericas = [\"lac\", \"rac\", \"sac\", \"uraList\"]\n",
    "for c in cols_numericas:\n",
    "    if c in df_base.columns:\n",
    "        df_base[c] = pd.to_numeric(df_base[c], errors=\"coerce\")\n",
    "\n",
    "# Guarda resultado (opcional)\n",
    "df_base.to_excel(BASE_DIR / \"Modified_with_headers_extracted.xlsx\", index=False)\n",
    "\n",
    "print(\"Extracción OK -> valores finales asignados en\", cols_mo)"
   ],
   "id": "2aab45472a73685",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracción OK -> valores finales asignados en ['iubLinkRef', 'lac', 'mocnCellProfileRef', 'rac', 'sac', 'uraList']\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T22:50:44.775350Z",
     "start_time": "2025-10-01T22:47:33.268866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------- Helpers ----------\n",
    "def _is_blank(s: pd.Series) -> pd.Series:\n",
    "    return s.isna() | s.astype(str).str.strip().eq(\"\")\n",
    "\n",
    "def coalesce_to_single_column(df, variants, target):\n",
    "    present = [c for c in variants if c in df.columns]\n",
    "    if not present:\n",
    "        return df\n",
    "    cols = [target] + [c for c in present if c != target] if target in present else present\n",
    "    df[target] = df[cols].bfill(axis=1).iloc[:, 0]\n",
    "    to_drop = [c for c in present if c != target]\n",
    "    df.drop(columns=to_drop, inplace=True, errors=\"ignore\")\n",
    "    return df\n",
    "\n",
    "def prev_month_yyyymm(today=None):\n",
    "    if today is None:\n",
    "        today = date.today()\n",
    "    y = today.year\n",
    "    m = today.month - 1\n",
    "    if m == 0:\n",
    "        y -= 1\n",
    "        m = 12\n",
    "    return f\"{y}{m:02d}\"\n",
    "\n",
    "# ---------- Config ----------\n",
    "#prev_yymm = prev_month_yyyymm()\n",
    "prev_yymm = f\"202508\"\n",
    "\n",
    "ae_path   = BASE_DIR / f\"All_Ericsson_3G_{prev_yymm}.xlsx\"\n",
    "ept_glob  = str(BASE_DIR / \"EPT_ATT_UMTS_LTE_*.xlsx\")\n",
    "\n",
    "# ========== ETAPA 0: punto de partida ==========\n",
    "merged = df_base.copy()  #NOn volver a usar df_base después de aquí\n",
    "\n",
    "# ========== ETAPA 1: AE por UtranCellId (NodeB, LAT, LON, AT&T_Site_Name) + fallback EPT por UtranCellId ==========\n",
    "# --- All_Ericsson por UtranCellId ---\n",
    "if ae_path.exists():\n",
    "    ae_cols = [\"UtranCellId\", \"NodeB\", \"LAT\", \"LON\", \"AT&T_Site_Name\"]\n",
    "    ae_df = pd.read_excel(ae_path, usecols=ae_cols)\n",
    "    if \"UtranCellId\" not in ae_df.columns:\n",
    "        raise KeyError(f\"{ae_path} no tiene 'UtranCellId'.\")\n",
    "\n",
    "    ae_df[\"UtranCellId\"] = ae_df[\"UtranCellId\"].astype(str).str.strip()\n",
    "    ae_df = ae_df.drop_duplicates(subset=[\"UtranCellId\"], keep=\"first\")\n",
    "\n",
    "    # LAT/LON a numérico si existen\n",
    "    for c in [\"LAT\", \"LON\"]:\n",
    "        if c in ae_df.columns:\n",
    "            ae_df[c] = pd.to_numeric(ae_df[c], errors=\"coerce\")\n",
    "\n",
    "    # Normaliza llave en base y merge\n",
    "    merged[\"UtranCellId\"] = merged[\"UtranCellId\"].astype(str).str.strip()\n",
    "    merged = merged.merge(ae_df, on=\"UtranCellId\", how=\"left\", suffixes=(\"\", \"_ae\"))\n",
    "\n",
    "    # Coalesce SOLO LAT/LON/Site desde AE (NO tocar NodeB aquí)\n",
    "    for col in [\"LAT\", \"LON\", \"AT&T_Site_Name\"]:\n",
    "        aux = f\"{col}_ae\"\n",
    "        if aux in merged.columns:\n",
    "            mask = _is_blank(merged[col]) if col in merged.columns else pd.Series(True, index=merged.index)\n",
    "            merged[col] = merged[col].where(~mask, merged[aux])\n",
    "            merged.drop(columns=[aux], inplace=True, errors=\"ignore\")\n",
    "else:\n",
    "    print(f\"⚠️ No se encontró {ae_path}. Se salta AE en etapa 1.\")\n",
    "\n",
    "# --- Fallback EPT por UtranCellId (completar solo faltantes de LAT/LON/Site y obtener candidato NodeB_ept) ---\n",
    "faltan1 = (\n",
    "    _is_blank(merged.get(\"NodeB\", pd.Series(False, index=merged.index))) |\n",
    "    _is_blank(merged.get(\"LAT\",   pd.Series(False, index=merged.index))) |\n",
    "    _is_blank(merged.get(\"LON\",   pd.Series(False, index=merged.index))) |\n",
    "    _is_blank(merged.get(\"AT&T_Site_Name\", pd.Series(False, index=merged.index)))\n",
    ")\n",
    "\n",
    "if faltan1.any():\n",
    "    ept_matches = glob.glob(ept_glob)\n",
    "    if ept_matches:\n",
    "        ept_file = ept_matches[0]\n",
    "        ept_sheets = [\"EPT_3G_LTE_OUTDOOR\", \"PLAN_OUTDOOR\", \"EPT_3G_LTE_INDOOR\", \"PLAN_INDOOR\", \"Eventos_Especiales\"]\n",
    "\n",
    "        frames = []\n",
    "        for sh in ept_sheets:\n",
    "            try:\n",
    "                tmp = pd.read_excel(ept_file, sheet_name=sh, engine=\"openpyxl\")\n",
    "                frames.append(tmp)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if frames:\n",
    "            ept_df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "            # Unificar nombres a los destinos esperados para lookup por UtranCellId\n",
    "            ept_df = coalesce_to_single_column(ept_df, [\"ATT_CELL_ID_Name\", \"UtranCellId\"], \"UtranCellId\")\n",
    "            ept_df = coalesce_to_single_column(ept_df, [\"AT&T_Node_Name\", \"NodeB\"], \"NodeB\")\n",
    "            ept_df = coalesce_to_single_column(ept_df, [\"Latitud\", \"LAT\"], \"LAT\")\n",
    "            ept_df = coalesce_to_single_column(ept_df, [\"Longitud\", \"LON\"], \"LON\")\n",
    "            ept_df = coalesce_to_single_column(ept_df, [\"AT&T_Site_Name\"], \"AT&T_Site_Name\")\n",
    "\n",
    "            if \"UtranCellId\" not in ept_df.columns:\n",
    "                raise KeyError(\"EPT no tiene 'UtranCellId'/'ATT_CELL_ID_Name' para fallback (etapa 1).\")\n",
    "\n",
    "            ept_df[\"UtranCellId\"] = ept_df[\"UtranCellId\"].astype(str).str.strip()\n",
    "            for c in [\"LAT\", \"LON\"]:\n",
    "                if c in ept_df.columns:\n",
    "                    ept_df[c] = ept_df[c].astype(str).str.strip(\"[]\").str.replace(\",\", \"\", regex=False)\n",
    "                    ept_df[c] = pd.to_numeric(ept_df[c], errors=\"coerce\")\n",
    "\n",
    "            ept_df = ept_df.drop_duplicates(subset=[\"UtranCellId\"], keep=\"first\")\n",
    "\n",
    "            # Build lookup (mantener NodeB_ept para construir NodeB final)\n",
    "            cols_keep = [c for c in [\"UtranCellId\", \"NodeB\", \"LAT\", \"LON\", \"AT&T_Site_Name\"] if c in ept_df.columns]\n",
    "            ept_lookup = ept_df[cols_keep].rename(columns={\n",
    "                \"NodeB\": \"NodeB_ept\",\n",
    "                \"LAT\": \"LAT_ept\",\n",
    "                \"LON\": \"LON_ept\",\n",
    "                \"AT&T_Site_Name\": \"AT&T_Site_Name_ept\",\n",
    "            })\n",
    "\n",
    "            merged = merged.merge(ept_lookup, on=\"UtranCellId\", how=\"left\")\n",
    "\n",
    "            # Completar SOLO faltantes de LAT/LON/Site desde EPT\n",
    "            need_lat  = _is_blank(merged[\"LAT\"])   if \"LAT\"   in merged.columns else pd.Series(False, index=merged.index)\n",
    "            need_lon  = _is_blank(merged[\"LON\"])   if \"LON\"   in merged.columns else pd.Series(False, index=merged.index)\n",
    "            need_site = _is_blank(merged[\"AT&T_Site_Name\"]) if \"AT&T_Site_Name\" in merged.columns else pd.Series(False, index=merged.index)\n",
    "\n",
    "            if \"LAT_ept\" in merged.columns:               merged.loc[need_lat,  \"LAT\"] = merged.loc[need_lat,  \"LAT_ept\"]\n",
    "            if \"LON_ept\" in merged.columns:               merged.loc[need_lon,  \"LON\"] = merged.loc[need_lon,  \"LON_ept\"]\n",
    "            if \"AT&T_Site_Name_ept\" in merged.columns:    merged.loc[need_site, \"AT&T_Site_Name\"] = merged.loc[need_site, \"AT&T_Site_Name_ept\"]\n",
    "\n",
    "            # Limpia auxiliares de LAT/LON/Site (conserva NodeB_ept para armar NodeB final)\n",
    "            merged.drop(columns=[c for c in [\"LAT_ept\",\"LON_ept\",\"AT&T_Site_Name_ept\"] if c in merged.columns],\n",
    "                        inplace=True, errors=\"ignore\")\n",
    "        else:\n",
    "            print(\"⚠️ EPT sin hojas legibles; no se aplicó fallback en etapa 1.\")\n",
    "    else:\n",
    "        print(\"⚠️ No se encontró archivo EPT para etapa 1.\")\n",
    "else:\n",
    "    print(\"AE cubrió 100% en etapa 1; no se necesita EPT.\")\n",
    "\n",
    "# --- Construcción de NodeB SOLO desde AE/EPT (sin mover su posición y sin FutureWarning) ---\n",
    "cands = []\n",
    "if \"NodeB_ae\" in merged.columns:  cands.append(merged[\"NodeB_ae\"])\n",
    "if \"NodeB_ept\" in merged.columns: cands.append(merged[\"NodeB_ept\"])\n",
    "\n",
    "if cands:\n",
    "    nodeb_final = cands[0]\n",
    "    for s in cands[1:]:\n",
    "        nodeb_final = nodeb_final.combine_first(s)   # evita FutureWarning\n",
    "else:\n",
    "    nodeb_final = pd.Series(pd.NA, index=merged.index)\n",
    "\n",
    "nodeb_final = nodeb_final.fillna(\"\")   # <- deja vacío en lugar de NaN\n",
    "\n",
    "\n",
    "# Asignar sobre 'NodeB' manteniendo su posición; si no existe, insertarla en la posición de HEADERS\n",
    "if \"NodeB\" in merged.columns:\n",
    "    merged.loc[:, \"NodeB\"] = nodeb_final\n",
    "else:\n",
    "    idx_nodeb = HEADERS.index(\"NodeB\") if \"NodeB\" in HEADERS else len(merged.columns)\n",
    "    merged.insert(idx_nodeb, \"NodeB\", nodeb_final)\n",
    "\n",
    "# Limpia auxiliares de NodeB\n",
    "merged.drop(columns=[c for c in [\"NodeB_ae\",\"NodeB_ept\"] if c in merged.columns],\n",
    "            inplace=True, errors=\"ignore\")\n",
    "# (Opcional) Guardar intermedio por trazabilidad\n",
    "intermediate_path = BASE_DIR / \"Datos_Modified_etapa1.xlsx\"\n",
    "try:\n",
    "    merged.to_excel(intermediate_path, index=False)\n",
    "    print(\"Guardado intermedio (etapa 1) →\", intermediate_path)\n",
    "except Exception as e:\n",
    "    print(\"No se guardó intermedio etapa 1:\", e)\n",
    "\n",
    "\n",
    "\n",
    "# --- eNodeB Name Unique (solo cuando cambia) ---\n",
    "#df_out es mi df main\n",
    "_name = merged[\"NodeB\"].astype(str).fillna(\"\").str.strip()\n",
    "# Toma la columna eNodeB Name, la convierte a str, reemplaza NaN por\"\",\n",
    "# y recorta espacios en extremos para comparar bien.\n",
    "\n",
    "is_new = _name.ne(_name.shift())\n",
    "# Crea una serie booleana True/False que vale True cuando\n",
    "# el nombre actual es diferente al de la fila anterior (inicio de bloque).\n",
    "\n",
    "merged[\"NodeBUnique\"] = np.where(is_new & _name.ne(\"\"), merged[\"NodeB\"], \"\")\n",
    "# Si cambia el nombre (is_new=True) y no está vacío: escribe el nombre.\n",
    "# En caso contrario: deja cadena vacía\"\" (equivalente a tu SI(A2=A1,\"\",A2)).\n",
    "\n"
   ],
   "id": "6b81fe1cc0743f72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado intermedio (etapa 1) → C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\3G\\Datos_Modified_etapa1.xlsx\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T22:58:49.487316Z",
     "start_time": "2025-10-01T22:55:41.702856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========== ETAPA 2: AE por (RNC+NodeB) → Node_B_ID + fallback EPT por (RNC+AT&T_Node_Name) ==========\n",
    "# Importante: seguir sobre 'merged' (NO volver a df_base)\n",
    "\n",
    "# AE por RNC+NodeB\n",
    "if ae_path.exists():\n",
    "    ae_cols2 = [\"RNC\", \"NodeB\", \"Node_B_ID\"]\n",
    "    ae_df2 = pd.read_excel(ae_path, usecols=[c for c in ae_cols2 if c])\n",
    "    missing = [c for c in [\"RNC\",\"NodeB\"] if c not in ae_df2.columns]\n",
    "    if missing:\n",
    "        print(f\"⚠️ AE sin columnas {missing} para etapa 2. Se salta AE etapa 2.\")\n",
    "    else:\n",
    "        ae_df2[\"RNC\"]   = ae_df2[\"RNC\"].astype(str).str.strip()\n",
    "        ae_df2[\"NodeB\"] = ae_df2[\"NodeB\"].astype(str).str.strip()\n",
    "        ae_df2 = ae_df2.drop_duplicates(subset=[\"RNC\",\"NodeB\"], keep=\"first\")\n",
    "        ae_df2 = ae_df2.rename(columns={\"Node_B_ID\": \"Node_B_ID_ae\"})\n",
    "\n",
    "        merged[\"RNC\"]   = merged[\"RNC\"].astype(str).str.strip()\n",
    "        merged[\"NodeB\"] = merged[\"NodeB\"].astype(str).str.strip()\n",
    "\n",
    "        merged = merged.merge(ae_df2, on=[\"RNC\",\"NodeB\"], how=\"left\")\n",
    "        # Completar SOLO Node_B_ID desde AE\n",
    "        if \"Node_B_ID_ae\" in merged.columns:\n",
    "            if \"Node_B_ID\" in merged.columns:\n",
    "                mask = _is_blank(merged[\"Node_B_ID\"])\n",
    "                merged.loc[mask, \"Node_B_ID\"] = merged.loc[mask, \"Node_B_ID_ae\"]\n",
    "            else:\n",
    "                merged[\"Node_B_ID\"] = merged[\"Node_B_ID_ae\"]\n",
    "            merged.drop(columns=[\"Node_B_ID_ae\"], inplace=True, errors=\"ignore\")\n",
    "else:\n",
    "    print(f\"⚠️ No se encontró {ae_path} para etapa 2.\")\n",
    "\n",
    "# Fallback EPT por (RNC + AT&T_Node_Name) → Node_B_ID\n",
    "faltan2 = _is_blank(merged.get(\"Node_B_ID\", pd.Series(False, index=merged.index)))\n",
    "if faltan2.any():\n",
    "    ept_matches = glob.glob(ept_glob)\n",
    "    if ept_matches:\n",
    "        ept_file = ept_matches[0]\n",
    "        ept_sheets = [\"EPT_3G_LTE_OUTDOOR\", \"PLAN_OUTDOOR\", \"EPT_3G_LTE_INDOOR\", \"PLAN_INDOOR\", \"Eventos_Especiales\"]\n",
    "        frames = []\n",
    "        for sh in ept_sheets:\n",
    "            try:\n",
    "                tmp = pd.read_excel(ept_file, sheet_name=sh, engine=\"openpyxl\")\n",
    "                frames.append(tmp)\n",
    "            except Exception:\n",
    "                pass\n",
    "        if frames:\n",
    "            ept_df2 = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "            ept_df2 = coalesce_to_single_column(ept_df2, [\"RNC\"], \"RNC\")\n",
    "            ept_df2 = coalesce_to_single_column(ept_df2, [\"AT&T_Node_Name\", \"NodeB\", \"Node_B_Name\"], \"AT&T_Node_Name\")\n",
    "            ept_df2 = coalesce_to_single_column(ept_df2, [\"Node_B_ID\", \"NodeB_ID\", \"NodeB Id\", \"Node_B Id\"], \"Node_B_ID\")\n",
    "\n",
    "            missing = [c for c in [\"RNC\",\"AT&T_Node_Name\",\"Node_B_ID\"] if c not in ept_df2.columns]\n",
    "            if missing:\n",
    "                raise KeyError(f\"EPT carece de columnas para etapa 2: {missing}\")\n",
    "\n",
    "            for c in [\"RNC\",\"AT&T_Node_Name\"]:\n",
    "                ept_df2[c] = ept_df2[c].astype(str).str.strip()\n",
    "\n",
    "            ept_lookup2 = (\n",
    "                ept_df2[[\"RNC\",\"AT&T_Node_Name\",\"Node_B_ID\"]]\n",
    "                .dropna(subset=[\"RNC\",\"AT&T_Node_Name\"])\n",
    "                .drop_duplicates(subset=[\"RNC\",\"AT&T_Node_Name\"], keep=\"first\")\n",
    "                .rename(columns={\"Node_B_ID\":\"Node_B_ID_ept\"})\n",
    "            )\n",
    "\n",
    "            merged[\"RNC\"]   = merged[\"RNC\"].astype(str).str.strip()\n",
    "            merged[\"NodeB\"] = merged[\"NodeB\"].astype(str).str.strip()\n",
    "\n",
    "            merged = merged.merge(\n",
    "                ept_lookup2,\n",
    "                left_on=[\"RNC\",\"NodeB\"],\n",
    "                right_on=[\"RNC\",\"AT&T_Node_Name\"],\n",
    "                how=\"left\"\n",
    "            )\n",
    "\n",
    "            need_id = _is_blank(merged[\"Node_B_ID\"]) if \"Node_B_ID\" in merged.columns else pd.Series(False, index=merged.index)\n",
    "            if \"Node_B_ID_ept\" in merged.columns:\n",
    "                merged.loc[need_id, \"Node_B_ID\"] = merged.loc[need_id, \"Node_B_ID_ept\"]\n",
    "\n",
    "            merged.drop(columns=[c for c in [\"Node_B_ID_ept\",\"AT&T_Node_Name\"] if c in merged.columns],\n",
    "                        inplace=True, errors=\"ignore\")\n",
    "        else:\n",
    "            print(\"⚠️ EPT sin hojas leíbles; no se aplicó fallback en etapa 2.\")\n",
    "    else:\n",
    "        print(\"⚠️ No se encontró archivo EPT para etapa 2.\")\n",
    "else:\n",
    "    print(\"AE cubrió 100% Node_B_ID en etapa 2; no se necesita EPT.\")\n",
    "\n",
    "# ========== ETAPA FINAL: conformar columnas y guardar ==========\n",
    "# Asegura tener exactamente los HEADERS definidos (sin extras)\n",
    "final_cols = HEADERS[:]  # HEADERS ya incluye \"Node_B_ID\"\n",
    "for c in final_cols:\n",
    "    if c not in merged.columns:\n",
    "        merged[c] = pd.NA\n",
    "merged = merged[final_cols]\n",
    "\n",
    "final_path = BASE_DIR / \"Datos_Modified.xlsx\"\n",
    "merged.to_excel(final_path, index=False)\n",
    "print(\"✅ Guardado FINAL →\", final_path)"
   ],
   "id": "507e1c39921b21ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Guardado FINAL → C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\3G\\Datos_Modified.xlsx\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T23:01:25.765705Z",
     "start_time": "2025-10-01T23:00:55.054934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "final_excel = BASE_DIR / \"Datos_Modified.xlsx\"\n",
    "tmp_excel = BASE_DIR / \"~tmp_Datos_Modified.xlsx\"\n",
    "\n",
    "# Releer, forzar columnas y orden\n",
    "df_out = pd.read_excel(final_excel)\n",
    "\n",
    "# Garantiza que TODAS las columnas existan\n",
    "for col in HEADERS:\n",
    "    if col not in df_out.columns:\n",
    "        df_out[col] = pd.NA\n",
    "\n",
    "# Reordena exactamente como HEADERS\n",
    "df_out = df_out[HEADERS]\n",
    "\n",
    "# Escribe temporal\n",
    "df_out.to_excel(tmp_excel, index=False)\n",
    "\n",
    "# Reaplicar estilo vertical de headers\n",
    "wb = load_workbook(tmp_excel)\n",
    "ws = wb.active\n",
    "\n",
    "# Congelar encabezado\n",
    "ws.freeze_panes = \"A2\"\n",
    "\n",
    "# Aplicar estilo a fila 1\n",
    "for col_idx, header in enumerate(HEADERS, start=1):\n",
    "    cell = ws.cell(row=1, column=col_idx)\n",
    "    cell.value = header\n",
    "    cell.font = Font(name=\"Aptos Narrow\", size=11)  # bold=True si quieres negrita\n",
    "    cell.alignment = Alignment(textRotation=90, horizontal=\"center\", vertical=\"bottom\", wrap_text=True)\n",
    "\n",
    "wb.save(final_excel)\n",
    "\n",
    "# Limpia temporal\n",
    "try:\n",
    "    tmp_excel.unlink()\n",
    "except Exception as e:\n",
    "    print(\"No se pudo borrar temporal:\", e)\n",
    "\n",
    "print(\"Ajuste final OK -> Headers verticales y columnas forzadas/ordenadas.\")\n",
    "\n"
   ],
   "id": "cdc94f3d0b991d80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se pudo borrar temporal: [WinError 32] El proceso no tiene acceso al archivo porque está siendo utilizado por otro proceso: 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\3G\\\\~tmp_Datos_Modified.xlsx'\n",
      "Ajuste final OK -> Headers verticales y columnas forzadas/ordenadas.\n"
     ]
    }
   ],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
