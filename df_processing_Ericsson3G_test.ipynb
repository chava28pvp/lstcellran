{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-04T01:14:39.914570Z",
     "start_time": "2025-10-04T01:14:39.910841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Alignment, Font, PatternFill\n",
    "from openpyxl.utils import get_column_letter\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "# Ruta base (ajústala si cambia)\n",
    "BASE_DIR = Path(r\"C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\3G\")\n",
    "\n",
    "# Lista de 31 encabezados, en el orden en que los quieres (eNBId queda en W si mantienes este orden)\n",
    "HEADERS = [\n",
    "     \"RNC\",\"administrativeState\",\"cId\",\"iubLinkRef\",\n",
    "    \"localCellId\",\"lac\",\"maximumTransmissionPower\",\"maxPwrMax\",\n",
    "    \"mocnCellProfileRef\",\"operationalState\",\"primaryCpichPower\",\"primaryScramblingCode\",\n",
    "    \"rac\",\"sac\",\"tCell\",\"uarfcnDl\",\n",
    "    \"uarfcnUl\",\"uraList\",\"UtranCellId\",\n",
    "    \"NodeB\",\"NodeBUnique\",\"LAT\",\"LON\",\"AT&T_Site_Name\",\n",
    "    \"Node_B_ID\", \"Gestor\"\n",
    "]\n",
    "\n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T01:14:44.550762Z",
     "start_time": "2025-10-04T01:14:44.543072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def appendfiles(filenamepattern: str) -> str:\n",
    "    \"\"\"\n",
    "    Integra todos los TXT que matchean pattern + '_*.txt' en un solo archivo,\n",
    "    agregando como ÚLTIMA COLUMNA el 'Gestor' derivado del sufijo numérico del archivo.\n",
    "    Devuelve el nombre del archivo integrado (sin ruta).\n",
    "    \"\"\"\n",
    "    searchpattern = str(BASE_DIR / f\"{filenamepattern}_*.txt\")\n",
    "    filestoread = glob.glob(searchpattern)\n",
    "\n",
    "    outputfile_name = f\"Integrated_{filenamepattern}_files.txt\"\n",
    "    output_path = BASE_DIR / outputfile_name\n",
    "\n",
    "    print(\"Buscando:\", searchpattern)\n",
    "    print(\"Archivos:\", filestoread)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as outputfile:\n",
    "        for name in filestoread:\n",
    "            # Extrae número después del guion bajo: *_NN.txt\n",
    "            m = re.search(r\"_(\\d+)\\.txt$\", os.path.basename(name), flags=re.IGNORECASE)\n",
    "            gestor = m.group(1) if m else \"\"\n",
    "\n",
    "            with open(name, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    # evita líneas vacías puras\n",
    "                    if not line.strip():\n",
    "                        continue\n",
    "                    # agrega el Gestor como última columna\n",
    "                    line = line.rstrip(\"\\n\")\n",
    "                    outputfile.write(f\"{line}\\t{gestor}\\n\")\n",
    "            print(\"Agregado:\", name)\n",
    "\n",
    "    print(\"Integrado =>\", outputfile_name)\n",
    "    return outputfile_name\n",
    "\n",
    "\n",
    "\n",
    "def cleanfile(filename: str, ignorelines=None) -> str:\n",
    "    \"\"\"\n",
    "    Elimina líneas que contengan cualquiera de los patrones indicados.\n",
    "    Devuelve el nombre del archivo limpio (sin ruta).\n",
    "    \"\"\"\n",
    "    if ignorelines is None:\n",
    "        ignorelines = [\"SubNetwork,\", \"instance(s)\", \"NodeId\"]\n",
    "\n",
    "    inputfile = BASE_DIR / filename\n",
    "    cleanfile_name = f\"Clean_{filename}\"\n",
    "    cleanfile_path = BASE_DIR / cleanfile_name\n",
    "\n",
    "    with open(inputfile, 'r', encoding=\"utf-8\") as f_in:\n",
    "        lines = f_in.readlines()\n",
    "\n",
    "    kept = []\n",
    "    for line in lines:\n",
    "        if any(p in line for p in ignorelines):\n",
    "            continue\n",
    "        kept.append(line)\n",
    "\n",
    "    with open(cleanfile_path, 'w', encoding=\"utf-8\") as f_out:\n",
    "        f_out.writelines(kept)\n",
    "\n",
    "    print(f\"Limpieza OK -> {cleanfile_name} ({len(kept)} líneas)\")\n",
    "    return cleanfile_name\n",
    "\n",
    "\n",
    "def convert_to_excel(cleanfile_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Lee TXT tab-delimited sin encabezados y guarda a Excel.\n",
    "    Devuelve el nombre del archivo Excel (sin ruta).\n",
    "    \"\"\"\n",
    "    cleanfile_path = BASE_DIR / cleanfile_name\n",
    "    out_xlsx = f\"Converted_{cleanfile_name}.xlsx\"\n",
    "    out_path = BASE_DIR / out_xlsx\n",
    "\n",
    "    df = pd.read_csv(cleanfile_path, delimiter='\\t', header=None)\n",
    "    df.to_excel(out_path, index=False, header=None)\n",
    "    print(f\"Convertido a Excel -> {out_xlsx}  (shape={df.shape})\")\n",
    "    return out_xlsx\n",
    "\n"
   ],
   "id": "6b2453460000e987",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T01:14:56.870628Z",
     "start_time": "2025-10-04T01:14:47.958054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# EUtranCellFDD\n",
    "eu_txt = appendfiles('UtranCell')\n",
    "eu_clean = cleanfile(eu_txt)\n",
    "eu_xlsx = convert_to_excel(eu_clean)\n",
    "\n",
    "# nodeid\n",
    "nd_txt = appendfiles('nodeid')\n",
    "nd_clean = cleanfile(nd_txt)\n",
    "nd_xlsx = convert_to_excel(nd_clean)\n",
    "\n",
    "\n"
   ],
   "id": "822c1f2141799a7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\3G\\UtranCell_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\3G\\\\UtranCell_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\3G\\\\UtranCell_9.txt']\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\3G\\UtranCell_14.txt\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\3G\\UtranCell_9.txt\n",
      "Integrado => Integrated_UtranCell_files.txt\n",
      "Limpieza OK -> Clean_Integrated_UtranCell_files.txt (24706 líneas)\n",
      "Convertido a Excel -> Converted_Clean_Integrated_UtranCell_files.txt.xlsx  (shape=(24706, 22))\n",
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\3G\\nodeid_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\3G\\\\nodeid_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\3G\\\\nodeid_9.txt']\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\3G\\nodeid_14.txt\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\3G\\nodeid_9.txt\n",
      "Integrado => Integrated_nodeid_files.txt\n",
      "Limpieza OK -> Clean_Integrated_nodeid_files.txt (2648 líneas)\n",
      "Convertido a Excel -> Converted_Clean_Integrated_nodeid_files.txt.xlsx  (shape=(2648, 5))\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T01:20:26.546324Z",
     "start_time": "2025-10-04T01:20:14.126649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Archivo base desde la conversión de EUtranCellFDD\n",
    "wb = load_workbook(BASE_DIR / eu_xlsx)  # ej. Converted_Clean_Integrated_UtranCell_files.txt.xlsx\n",
    "ws = wb.active\n",
    "\n",
    "# Elimina la columna B (índice 2). 'amount=1' borra solo una columna.\n",
    "# openpyxl recorrerá C→B, D→C, etc.\n",
    "if ws.max_column >= 2:\n",
    "    ws.delete_cols(idx=2, amount=1)\n",
    "\n",
    "wb.save(BASE_DIR / \"Modified_workfile.xlsx\")\n",
    "print(\"Reacomodo OK -> Modified_workfile.xlsx (columna B eliminada)\")\n"
   ],
   "id": "d88444e0446c642e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reacomodo OK -> Modified_workfile.xlsx (columna B eliminada)\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T01:24:30.986779Z",
     "start_time": "2025-10-04T01:24:16.431792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Leemos el archivo reacomodado SIN headers\n",
    "df_base = pd.read_excel(BASE_DIR / \"Modified_workfile.xlsx\", header=None)\n",
    "\n",
    "# --- mover B -> S (índices base 0: B=1, S=19) ---\n",
    "while df_base.shape[1] < 20:\n",
    "    df_base[df_base.shape[1]] = pd.NA\n",
    "\n",
    "src_idx = 1  # B\n",
    "dst_idx = 19  # S\n",
    "colB = df_base.iloc[:, src_idx].copy()\n",
    "df_base.drop(df_base.columns[src_idx], axis=1, inplace=True)\n",
    "if dst_idx > src_idx:\n",
    "    dst_idx -= 1\n",
    "df_base.insert(dst_idx, colB.name, colB)\n",
    "# --- fin mover B -> S ---\n",
    "\n",
    "# === 1) Captura y ELIMINA la última columna cruda (Gestor del TXT) ===\n",
    "gestor_series = df_base.iloc[:, -1].copy()\n",
    "df_base = df_base.iloc[:, :-1]  # quita la última columna para que no ocupe otra cabecera\n",
    "\n",
    "# === 2) Mapea el resto por POSICIÓN usando HEADERS sin 'Gestor' ===\n",
    "headers_wo_gestor = [h for h in HEADERS if h != \"Gestor\"]\n",
    "expected_wo_gestor = len(headers_wo_gestor)\n",
    "\n",
    "# Completa o recorta al tamaño esperado (sin Gestor)\n",
    "if df_base.shape[1] < expected_wo_gestor:\n",
    "    for _ in range(expected_wo_gestor - df_base.shape[1]):\n",
    "        df_base[df_base.shape[1]] = pd.NA\n",
    "elif df_base.shape[1] > expected_wo_gestor:\n",
    "    extra_cols = df_base.shape[1] - expected_wo_gestor\n",
    "    print(f\"⚠️ Se detectaron {extra_cols} columnas extra (sin Gestor). Serán descartadas.\")\n",
    "    df_base = df_base.iloc[:, :expected_wo_gestor]\n",
    "\n",
    "# Asigna nombres por posición para estas columnas\n",
    "df_base.columns = headers_wo_gestor\n",
    "\n",
    "# === 3) Inserta de nuevo 'Gestor' con la serie capturada ===\n",
    "df_base[\"Gestor\"] = gestor_series.values\n",
    "\n",
    "# === 4) Reordena exactamente según HEADERS completos (columna Z para Gestor) ===\n",
    "df_base = df_base[HEADERS]\n",
    "\n",
    "# === 5) Guardar ===\n",
    "out_path = BASE_DIR / \"Modified_with_headers.xlsx\"\n",
    "df_base.to_excel(out_path, index=False)\n",
    "print(\"✅ Archivo final generado:\", out_path)\n"
   ],
   "id": "d6659a16606a67c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo final generado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\3G\\Modified_with_headers.xlsx\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T01:25:46.669231Z",
     "start_time": "2025-10-04T01:25:37.391416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Columnas con cadenas estilo \"k1=v1,k2=v2,...\"\n",
    "cols_mo = [\n",
    "    \"iubLinkRef\",       # ejemplo: ... ,IubLink=Iub_0420  -> Iub_0420\n",
    "    \"lac\",              # ... ,LocationArea=41416         -> 41416\n",
    "    \"mocnCellProfileRef\",# ... ,MocnCellProfile=Telefonica -> Telefonica\n",
    "    \"rac\",              # ... ,RoutingArea=146            -> 146\n",
    "    \"sac\",              # ... ,ServiceArea=65055          -> 65055\n",
    "    \"uraList\",          # [ ...,Ura=146]                  -> 146\n",
    "]\n",
    "\n",
    "def extract_last_token_after_equal(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Toma una Serie de strings y devuelve el segmento que está después del último '='.\n",
    "    Limpiaespacios, corchetes y comas/paréntesis finales si los hubiera.\n",
    "    \"\"\"\n",
    "    out = (\n",
    "        s.astype(str)\n",
    "         # divide solo una vez desde la derecha: ['prefix', 'ultimo_valor']\n",
    "         .str.rsplit('=', n=1).str[-1]\n",
    "         .str.strip()                  # quita espacios\n",
    "         .str.strip('[]()')            # quita corchetes/paréntesis de extremos\n",
    "         .str.replace(r'[,\\;]$', '', regex=True)  # quita coma/; final si existe\n",
    "    )\n",
    "    # Cuando no había '=', rsplit devuelve todo; si quieres NA en esos casos:\n",
    "    has_equal = s.astype(str).str.contains('=', regex=False)\n",
    "    out = out.where(has_equal, other=pd.NA)\n",
    "    return out\n",
    "\n",
    "# Creamos columnas normalizadas (puedes sobreescribir las originales si prefieres)\n",
    "for c in cols_mo:\n",
    "    df_base[c] = extract_last_token_after_equal(df_base[c])\n",
    "\n",
    "# Convierte a numérico donde corresponde (las que deben ser números):\n",
    "cols_numericas = [\"lac\", \"rac\", \"sac\", \"uraList\"]\n",
    "for c in cols_numericas:\n",
    "    if c in df_base.columns:\n",
    "        df_base[c] = pd.to_numeric(df_base[c], errors=\"coerce\")\n",
    "\n",
    "# Guarda resultado (opcional)\n",
    "df_base.to_excel(BASE_DIR / \"Modified_with_headers_extracted.xlsx\", index=False)\n",
    "\n",
    "print(\"Extracción OK -> valores finales asignados en\", cols_mo)"
   ],
   "id": "2aab45472a73685",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracción OK -> valores finales asignados en ['iubLinkRef', 'lac', 'mocnCellProfileRef', 'rac', 'sac', 'uraList']\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T01:29:26.455189Z",
     "start_time": "2025-10-04T01:26:15.689413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------- Helpers ----------\n",
    "def _is_blank(s: pd.Series) -> pd.Series:\n",
    "    return s.isna() | s.astype(str).str.strip().eq(\"\")\n",
    "\n",
    "def coalesce_to_single_column(df, variants, target):\n",
    "    present = [c for c in variants if c in df.columns]\n",
    "    if not present:\n",
    "        return df\n",
    "    cols = [target] + [c for c in present if c != target] if target in present else present\n",
    "    df[target] = df[cols].bfill(axis=1).iloc[:, 0]\n",
    "    to_drop = [c for c in present if c != target]\n",
    "    df.drop(columns=to_drop, inplace=True, errors=\"ignore\")\n",
    "    return df\n",
    "\n",
    "def prev_month_yyyymm(today=None):\n",
    "    if today is None:\n",
    "        today = date.today()\n",
    "    y = today.year\n",
    "    m = today.month - 1\n",
    "    if m == 0:\n",
    "        y -= 1\n",
    "        m = 12\n",
    "    return f\"{y}{m:02d}\"\n",
    "\n",
    "# ---------- Config ----------\n",
    "#prev_yymm = prev_month_yyyymm()\n",
    "prev_yymm = f\"202508\"\n",
    "\n",
    "ae_path   = BASE_DIR / f\"All_Ericsson_3G_{prev_yymm}.xlsx\"\n",
    "ept_glob  = str(BASE_DIR / \"EPT_ATT_UMTS_LTE_*.xlsx\")\n",
    "\n",
    "# ========== ETAPA 0: punto de partida ==========\n",
    "merged = df_base.copy()  #NOn volver a usar df_base después de aquí\n",
    "\n",
    "# ========== ETAPA 1: AE por UtranCellId (NodeB, LAT, LON, AT&T_Site_Name) + fallback EPT por UtranCellId ==========\n",
    "# --- All_Ericsson por UtranCellId ---\n",
    "if ae_path.exists():\n",
    "    ae_cols = [\"UtranCellId\", \"NodeB\", \"LAT\", \"LON\", \"AT&T_Site_Name\"]\n",
    "    ae_df = pd.read_excel(ae_path, usecols=ae_cols)\n",
    "    if \"UtranCellId\" not in ae_df.columns:\n",
    "        raise KeyError(f\"{ae_path} no tiene 'UtranCellId'.\")\n",
    "\n",
    "    ae_df[\"UtranCellId\"] = ae_df[\"UtranCellId\"].astype(str).str.strip()\n",
    "    ae_df = ae_df.drop_duplicates(subset=[\"UtranCellId\"], keep=\"first\")\n",
    "\n",
    "    # LAT/LON a numérico si existen\n",
    "    for c in [\"LAT\", \"LON\"]:\n",
    "        if c in ae_df.columns:\n",
    "            ae_df[c] = pd.to_numeric(ae_df[c], errors=\"coerce\")\n",
    "\n",
    "    # Normaliza llave en base y merge\n",
    "    merged[\"UtranCellId\"] = merged[\"UtranCellId\"].astype(str).str.strip()\n",
    "    merged = merged.merge(ae_df, on=\"UtranCellId\", how=\"left\", suffixes=(\"\", \"_ae\"))\n",
    "\n",
    "    # Coalesce SOLO LAT/LON/Site desde AE (NO tocar NodeB aquí)\n",
    "    for col in [\"LAT\", \"LON\", \"AT&T_Site_Name\"]:\n",
    "        aux = f\"{col}_ae\"\n",
    "        if aux in merged.columns:\n",
    "            mask = _is_blank(merged[col]) if col in merged.columns else pd.Series(True, index=merged.index)\n",
    "            merged[col] = merged[col].where(~mask, merged[aux])\n",
    "            merged.drop(columns=[aux], inplace=True, errors=\"ignore\")\n",
    "else:\n",
    "    print(f\"⚠️ No se encontró {ae_path}. Se salta AE en etapa 1.\")\n",
    "\n",
    "# --- Fallback EPT por UtranCellId (completar solo faltantes de LAT/LON/Site y obtener candidato NodeB_ept) ---\n",
    "faltan1 = (\n",
    "    _is_blank(merged.get(\"NodeB\", pd.Series(False, index=merged.index))) |\n",
    "    _is_blank(merged.get(\"LAT\",   pd.Series(False, index=merged.index))) |\n",
    "    _is_blank(merged.get(\"LON\",   pd.Series(False, index=merged.index))) |\n",
    "    _is_blank(merged.get(\"AT&T_Site_Name\", pd.Series(False, index=merged.index)))\n",
    ")\n",
    "\n",
    "if faltan1.any():\n",
    "    ept_matches = glob.glob(ept_glob)\n",
    "    if ept_matches:\n",
    "        ept_file = ept_matches[0]\n",
    "        ept_sheets = [\"EPT_3G_LTE_OUTDOOR\", \"PLAN_OUTDOOR\", \"EPT_3G_LTE_INDOOR\", \"PLAN_INDOOR\", \"Eventos_Especiales\"]\n",
    "\n",
    "        frames = []\n",
    "        for sh in ept_sheets:\n",
    "            try:\n",
    "                tmp = pd.read_excel(ept_file, sheet_name=sh, engine=\"openpyxl\")\n",
    "                frames.append(tmp)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if frames:\n",
    "            ept_df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "            # Unificar nombres a los destinos esperados para lookup por UtranCellId\n",
    "            ept_df = coalesce_to_single_column(ept_df, [\"ATT_CELL_ID_Name\", \"UtranCellId\"], \"UtranCellId\")\n",
    "            ept_df = coalesce_to_single_column(ept_df, [\"AT&T_Node_Name\", \"NodeB\"], \"NodeB\")\n",
    "            ept_df = coalesce_to_single_column(ept_df, [\"Latitud\", \"LAT\"], \"LAT\")\n",
    "            ept_df = coalesce_to_single_column(ept_df, [\"Longitud\", \"LON\"], \"LON\")\n",
    "            ept_df = coalesce_to_single_column(ept_df, [\"AT&T_Site_Name\"], \"AT&T_Site_Name\")\n",
    "\n",
    "            if \"UtranCellId\" not in ept_df.columns:\n",
    "                raise KeyError(\"EPT no tiene 'UtranCellId'/'ATT_CELL_ID_Name' para fallback (etapa 1).\")\n",
    "\n",
    "            ept_df[\"UtranCellId\"] = ept_df[\"UtranCellId\"].astype(str).str.strip()\n",
    "            for c in [\"LAT\", \"LON\"]:\n",
    "                if c in ept_df.columns:\n",
    "                    ept_df[c] = ept_df[c].astype(str).str.strip(\"[]\").str.replace(\",\", \"\", regex=False)\n",
    "                    ept_df[c] = pd.to_numeric(ept_df[c], errors=\"coerce\")\n",
    "\n",
    "            ept_df = ept_df.drop_duplicates(subset=[\"UtranCellId\"], keep=\"first\")\n",
    "\n",
    "            # Build lookup (mantener NodeB_ept para construir NodeB final)\n",
    "            cols_keep = [c for c in [\"UtranCellId\", \"NodeB\", \"LAT\", \"LON\", \"AT&T_Site_Name\"] if c in ept_df.columns]\n",
    "            ept_lookup = ept_df[cols_keep].rename(columns={\n",
    "                \"NodeB\": \"NodeB_ept\",\n",
    "                \"LAT\": \"LAT_ept\",\n",
    "                \"LON\": \"LON_ept\",\n",
    "                \"AT&T_Site_Name\": \"AT&T_Site_Name_ept\",\n",
    "            })\n",
    "\n",
    "            merged = merged.merge(ept_lookup, on=\"UtranCellId\", how=\"left\")\n",
    "\n",
    "            # Completar SOLO faltantes de LAT/LON/Site desde EPT\n",
    "            need_lat  = _is_blank(merged[\"LAT\"])   if \"LAT\"   in merged.columns else pd.Series(False, index=merged.index)\n",
    "            need_lon  = _is_blank(merged[\"LON\"])   if \"LON\"   in merged.columns else pd.Series(False, index=merged.index)\n",
    "            need_site = _is_blank(merged[\"AT&T_Site_Name\"]) if \"AT&T_Site_Name\" in merged.columns else pd.Series(False, index=merged.index)\n",
    "\n",
    "            if \"LAT_ept\" in merged.columns:               merged.loc[need_lat,  \"LAT\"] = merged.loc[need_lat,  \"LAT_ept\"]\n",
    "            if \"LON_ept\" in merged.columns:               merged.loc[need_lon,  \"LON\"] = merged.loc[need_lon,  \"LON_ept\"]\n",
    "            if \"AT&T_Site_Name_ept\" in merged.columns:    merged.loc[need_site, \"AT&T_Site_Name\"] = merged.loc[need_site, \"AT&T_Site_Name_ept\"]\n",
    "\n",
    "            # Limpia auxiliares de LAT/LON/Site (conserva NodeB_ept para armar NodeB final)\n",
    "            merged.drop(columns=[c for c in [\"LAT_ept\",\"LON_ept\",\"AT&T_Site_Name_ept\"] if c in merged.columns],\n",
    "                        inplace=True, errors=\"ignore\")\n",
    "        else:\n",
    "            print(\"⚠️ EPT sin hojas legibles; no se aplicó fallback en etapa 1.\")\n",
    "    else:\n",
    "        print(\"⚠️ No se encontró archivo EPT para etapa 1.\")\n",
    "else:\n",
    "    print(\"AE cubrió 100% en etapa 1; no se necesita EPT.\")\n",
    "\n",
    "# --- Construcción de NodeB SOLO desde AE/EPT (sin mover su posición y sin FutureWarning) ---\n",
    "cands = []\n",
    "if \"NodeB_ae\" in merged.columns:  cands.append(merged[\"NodeB_ae\"])\n",
    "if \"NodeB_ept\" in merged.columns: cands.append(merged[\"NodeB_ept\"])\n",
    "\n",
    "if cands:\n",
    "    nodeb_final = cands[0]\n",
    "    for s in cands[1:]:\n",
    "        nodeb_final = nodeb_final.combine_first(s)   # evita FutureWarning\n",
    "else:\n",
    "    nodeb_final = pd.Series(pd.NA, index=merged.index)\n",
    "\n",
    "nodeb_final = nodeb_final.fillna(\"\")   # <- deja vacío en lugar de NaN\n",
    "\n",
    "\n",
    "# Asignar sobre 'NodeB' manteniendo su posición; si no existe, insertarla en la posición de HEADERS\n",
    "if \"NodeB\" in merged.columns:\n",
    "    merged.loc[:, \"NodeB\"] = nodeb_final\n",
    "else:\n",
    "    idx_nodeb = HEADERS.index(\"NodeB\") if \"NodeB\" in HEADERS else len(merged.columns)\n",
    "    merged.insert(idx_nodeb, \"NodeB\", nodeb_final)\n",
    "\n",
    "# Limpia auxiliares de NodeB\n",
    "merged.drop(columns=[c for c in [\"NodeB_ae\",\"NodeB_ept\"] if c in merged.columns],\n",
    "            inplace=True, errors=\"ignore\")\n",
    "# (Opcional) Guardar intermedio por trazabilidad\n",
    "intermediate_path = BASE_DIR / \"Datos_Modified_etapa1.xlsx\"\n",
    "try:\n",
    "    merged.to_excel(intermediate_path, index=False)\n",
    "    print(\"Guardado intermedio (etapa 1) →\", intermediate_path)\n",
    "except Exception as e:\n",
    "    print(\"No se guardó intermedio etapa 1:\", e)\n",
    "\n",
    "\n"
   ],
   "id": "6b81fe1cc0743f72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado intermedio (etapa 1) → C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\3G\\Datos_Modified_etapa1.xlsx\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T01:33:07.645207Z",
     "start_time": "2025-10-04T01:29:49.816652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========== ETAPA 2: AE por (RNC+NodeB) → Node_B_ID + fallback EPT por (RNC+AT&T_Node_Name) ==========\n",
    "# Importante: seguir sobre 'merged' (NO volver a df_base)\n",
    "\n",
    "# AE por RNC+NodeB\n",
    "if ae_path.exists():\n",
    "    ae_cols2 = [\"RNC\", \"NodeB\", \"Node_B_ID\"]\n",
    "    ae_df2 = pd.read_excel(ae_path, usecols=[c for c in ae_cols2 if c])\n",
    "    missing = [c for c in [\"RNC\",\"NodeB\"] if c not in ae_df2.columns]\n",
    "    if missing:\n",
    "        print(f\"⚠️ AE sin columnas {missing} para etapa 2. Se salta AE etapa 2.\")\n",
    "    else:\n",
    "        ae_df2[\"RNC\"]   = ae_df2[\"RNC\"].astype(str).str.strip()\n",
    "        ae_df2[\"NodeB\"] = ae_df2[\"NodeB\"].astype(str).str.strip()\n",
    "        ae_df2 = ae_df2.drop_duplicates(subset=[\"RNC\",\"NodeB\"], keep=\"first\")\n",
    "        ae_df2 = ae_df2.rename(columns={\"Node_B_ID\": \"Node_B_ID_ae\"})\n",
    "\n",
    "        merged[\"RNC\"]   = merged[\"RNC\"].astype(str).str.strip()\n",
    "        merged[\"NodeB\"] = merged[\"NodeB\"].astype(str).str.strip()\n",
    "\n",
    "        merged = merged.merge(ae_df2, on=[\"RNC\",\"NodeB\"], how=\"left\")\n",
    "        # Completar SOLO Node_B_ID desde AE\n",
    "        if \"Node_B_ID_ae\" in merged.columns:\n",
    "            if \"Node_B_ID\" in merged.columns:\n",
    "                mask = _is_blank(merged[\"Node_B_ID\"])\n",
    "                merged.loc[mask, \"Node_B_ID\"] = merged.loc[mask, \"Node_B_ID_ae\"]\n",
    "            else:\n",
    "                merged[\"Node_B_ID\"] = merged[\"Node_B_ID_ae\"]\n",
    "            merged.drop(columns=[\"Node_B_ID_ae\"], inplace=True, errors=\"ignore\")\n",
    "else:\n",
    "    print(f\"⚠️ No se encontró {ae_path} para etapa 2.\")\n",
    "\n",
    "# Fallback EPT por (RNC + AT&T_Node_Name) → Node_B_ID\n",
    "faltan2 = _is_blank(merged.get(\"Node_B_ID\", pd.Series(False, index=merged.index)))\n",
    "if faltan2.any():\n",
    "    ept_matches = glob.glob(ept_glob)\n",
    "    if ept_matches:\n",
    "        ept_file = ept_matches[0]\n",
    "        ept_sheets = [\"EPT_3G_LTE_OUTDOOR\", \"PLAN_OUTDOOR\", \"EPT_3G_LTE_INDOOR\", \"PLAN_INDOOR\", \"Eventos_Especiales\"]\n",
    "        frames = []\n",
    "        for sh in ept_sheets:\n",
    "            try:\n",
    "                tmp = pd.read_excel(ept_file, sheet_name=sh, engine=\"openpyxl\")\n",
    "                frames.append(tmp)\n",
    "            except Exception:\n",
    "                pass\n",
    "        if frames:\n",
    "            ept_df2 = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "            ept_df2 = coalesce_to_single_column(ept_df2, [\"RNC\"], \"RNC\")\n",
    "            ept_df2 = coalesce_to_single_column(ept_df2, [\"AT&T_Node_Name\", \"NodeB\", \"Node_B_Name\"], \"AT&T_Node_Name\")\n",
    "            ept_df2 = coalesce_to_single_column(ept_df2, [\"Node_B_ID\", \"NodeB_ID\", \"NodeB Id\", \"Node_B Id\"], \"Node_B_ID\")\n",
    "\n",
    "            missing = [c for c in [\"RNC\",\"AT&T_Node_Name\",\"Node_B_ID\"] if c not in ept_df2.columns]\n",
    "            if missing:\n",
    "                raise KeyError(f\"EPT carece de columnas para etapa 2: {missing}\")\n",
    "\n",
    "            for c in [\"RNC\",\"AT&T_Node_Name\"]:\n",
    "                ept_df2[c] = ept_df2[c].astype(str).str.strip()\n",
    "\n",
    "            ept_lookup2 = (\n",
    "                ept_df2[[\"RNC\",\"AT&T_Node_Name\",\"Node_B_ID\"]]\n",
    "                .dropna(subset=[\"RNC\",\"AT&T_Node_Name\"])\n",
    "                .drop_duplicates(subset=[\"RNC\",\"AT&T_Node_Name\"], keep=\"first\")\n",
    "                .rename(columns={\"Node_B_ID\":\"Node_B_ID_ept\"})\n",
    "            )\n",
    "\n",
    "            merged[\"RNC\"]   = merged[\"RNC\"].astype(str).str.strip()\n",
    "            merged[\"NodeB\"] = merged[\"NodeB\"].astype(str).str.strip()\n",
    "\n",
    "            merged = merged.merge(\n",
    "                ept_lookup2,\n",
    "                left_on=[\"RNC\",\"NodeB\"],\n",
    "                right_on=[\"RNC\",\"AT&T_Node_Name\"],\n",
    "                how=\"left\"\n",
    "            )\n",
    "\n",
    "            need_id = _is_blank(merged[\"Node_B_ID\"]) if \"Node_B_ID\" in merged.columns else pd.Series(False, index=merged.index)\n",
    "            if \"Node_B_ID_ept\" in merged.columns:\n",
    "                merged.loc[need_id, \"Node_B_ID\"] = merged.loc[need_id, \"Node_B_ID_ept\"]\n",
    "\n",
    "            merged.drop(columns=[c for c in [\"Node_B_ID_ept\",\"AT&T_Node_Name\"] if c in merged.columns],\n",
    "                        inplace=True, errors=\"ignore\")\n",
    "        else:\n",
    "            print(\"⚠️ EPT sin hojas leíbles; no se aplicó fallback en etapa 2.\")\n",
    "    else:\n",
    "        print(\"⚠️ No se encontró archivo EPT para etapa 2.\")\n",
    "else:\n",
    "    print(\"AE cubrió 100% Node_B_ID en etapa 2; no se necesita EPT.\")\n",
    "\n",
    "# ========== ETAPA FINAL: conformar columnas y guardar ==========\n",
    "# Asegura tener exactamente los HEADERS definidos (sin extras)\n",
    "final_cols = HEADERS[:]  # HEADERS ya incluye \"Node_B_ID\"\n",
    "for c in final_cols:\n",
    "    if c not in merged.columns:\n",
    "        merged[c] = pd.NA\n",
    "merged = merged[final_cols]\n",
    "\n",
    "final_path = BASE_DIR / \"Datos_Modified.xlsx\"\n",
    "merged.to_excel(final_path, index=False)\n",
    "print(\"✅ Guardado FINAL →\", final_path)"
   ],
   "id": "507e1c39921b21ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Guardado FINAL → C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\3G\\Datos_Modified.xlsx\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T01:33:39.263648Z",
     "start_time": "2025-10-04T01:33:26.093668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# === Configura tu carpeta base (una CARPETA, no un archivo) ===\n",
    "BASE_DIR = Path(r\"C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\3G\")\n",
    "\n",
    "# Si te pasaron un archivo completo en BASE_DIR, normalízalo:\n",
    "if BASE_DIR.suffix.lower() == \".xlsx\":\n",
    "    src = BASE_DIR\n",
    "    BASE_DIR = src.parent\n",
    "else:\n",
    "    # Caso normal: BASE_DIR es carpeta; el archivo esperado es Datos_Modified.xlsx\n",
    "    src = BASE_DIR / \"Datos_Modified.xlsx\"\n",
    "\n",
    "dst = BASE_DIR / \"Datos_Modified_order.xlsx\"\n",
    "\n",
    "# Validaciones útiles\n",
    "if not BASE_DIR.exists():\n",
    "    raise FileNotFoundError(f\"La carpeta base no existe: {BASE_DIR}\")\n",
    "\n",
    "if src.is_dir():\n",
    "    raise IsADirectoryError(\n",
    "        f\"'{src}' es una carpeta. Debes apuntar a un archivo .xlsx, por ejemplo: {src / 'Datos_Modified.xlsx'}\"\n",
    "    )\n",
    "\n",
    "if not src.exists():\n",
    "    similares = list(BASE_DIR.glob(\"*Datos_Modified*.xlsx\"))\n",
    "    sugerencia = f\"\\nSugeridos: {similares}\" if similares else \"\"\n",
    "    raise FileNotFoundError(f\"No encontré el archivo de entrada: {src}{sugerencia}\")\n",
    "\n",
    "# === Leer\n",
    "try:\n",
    "    df = pd.read_excel(src, engine=\"openpyxl\")\n",
    "except PermissionError as e:\n",
    "    raise PermissionError(\n",
    "        f\"No pude leer '{src}'. ¿Está abierto en Excel? Ciérralo e intenta de nuevo.\"\n",
    "    ) from e\n",
    "\n",
    "if \"NodeB\" not in df.columns:\n",
    "    raise KeyError(\"El archivo no contiene la columna 'NodeB'.\")\n",
    "\n",
    "# === Ordenar por NodeB (A→Z, case-insensitive, ignorando espacios)\n",
    "df_sorted = df.sort_values(\n",
    "    by=\"NodeB\",\n",
    "    key=lambda s: s.astype(\"string\").str.strip().str.casefold(),\n",
    "    kind=\"mergesort\",\n",
    "    na_position=\"last\",\n",
    ")\n",
    "\n",
    "# === Calcular NodeBUnique: solo la primera aparición de cada NodeB\n",
    "key = df_sorted[\"NodeB\"].astype(\"string\").str.strip().str.casefold()\n",
    "first_hit = ~key.duplicated(keep=\"first\") & key.fillna(\"\").ne(\"\")\n",
    "df_sorted[\"NodeBUnique\"] = df_sorted[\"NodeB\"].where(first_hit, \"\")\n",
    "\n",
    "# (Si prefieres NaN en lugar de cadena vacía, usa pd.NA en la línea anterior)\n",
    "\n",
    "# === Guardar\n",
    "try:\n",
    "    df_sorted.to_excel(dst, index=False, na_rep=\"\")\n",
    "except PermissionError as e:\n",
    "    raise PermissionError(\n",
    "        f\"No pude escribir '{dst}'. Si existe, asegúrate de que NO esté abierto en Excel.\"\n",
    "    ) from e\n",
    "\n",
    "print(\"✅ Archivo ordenado con NodeBUnique guardado en:\", dst)\n"
   ],
   "id": "ceb644283d156d8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo ordenado con NodeBUnique guardado en: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\3G\\Datos_Modified_order.xlsx\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T01:34:33.561718Z",
     "start_time": "2025-10-04T01:33:57.124925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "final_excel = BASE_DIR / \"Datos_Modified_order.xlsx\"\n",
    "tmp_excel   = BASE_DIR / \"~tmp_Datos_Modified_order.xlsx\"\n",
    "\n",
    "# === 1) Releer, asegurar columnas y orden ===\n",
    "df_out = pd.read_excel(final_excel)\n",
    "\n",
    "for col in HEADERS:\n",
    "    if col not in df_out.columns:\n",
    "        df_out[col] = pd.NA\n",
    "\n",
    "df_out = df_out[HEADERS]\n",
    "\n",
    "# Escribe temporal sin 'nan'\n",
    "df_out.to_excel(tmp_excel, index=False, na_rep=\"\")\n",
    "\n",
    "# === 2) Estilos y ajustes selectivos ===\n",
    "wb = load_workbook(tmp_excel)\n",
    "ws = wb.active\n",
    "\n",
    "# Congelar encabezado y aplicar autofiltro\n",
    "ws.freeze_panes = \"A2\"\n",
    "ws.auto_filter.ref = ws.dimensions\n",
    "\n",
    "# Estilo de encabezados\n",
    "header_fill = PatternFill(fill_type=\"solid\", start_color=\"FFBFBFBF\", end_color=\"FFBFBFBF\")\n",
    "ws.row_dimensions[1].height = 90  # más alto para leer el header rotado\n",
    "\n",
    "for col_idx, header in enumerate(HEADERS, start=1):\n",
    "    c = ws.cell(row=1, column=col_idx)\n",
    "    c.value = header\n",
    "    c.font = Font(name=\"Aptos Narrow\", size=9, bold=True)\n",
    "    c.alignment = Alignment(textRotation=90, horizontal=\"center\", vertical=\"bottom\", wrap_text=True)\n",
    "    c.fill = header_fill\n",
    "\n",
    "# ======== SOLO AJUSTAR ALGUNAS COLUMNAS ========\n",
    "# Opción A: anchos FIJOS por columna (recomendado si ya sabes los tamaños)\n",
    "width_overrides = {\n",
    "    \"NodeB\": 15,\n",
    "    \"AT&T_Site_Name\": 15,\n",
    "    \"RNC\": 14,\n",
    "    \"NodeBUnique\": 15,\n",
    "    \"UtranCellId\":20,\n",
    "    \"LAT\": 15,\n",
    "    \"LON\":15\n",
    "}\n",
    "\n",
    "for col_name, width in width_overrides.items():\n",
    "    if col_name in HEADERS:\n",
    "        col_letter = get_column_letter(HEADERS.index(col_name) + 1)\n",
    "        ws.column_dimensions[col_letter].width = width\n",
    "\n",
    "# --- Opción B (opcional): auto-fit SOLO para algunas columnas ---\n",
    "#   Si prefieres autoajustar *solo* algunas (y no tocar el resto), usa esta lista:\n",
    "AUTO_FIT = []  # por ejemplo: [\"AT&T_Site_Name\", \"NodeB\"]\n",
    "\n",
    "for col_name in AUTO_FIT:\n",
    "    if col_name in HEADERS:\n",
    "        col_idx = HEADERS.index(col_name) + 1\n",
    "        col_letter = get_column_letter(col_idx)\n",
    "        # calcula ancho por contenido + header (percentil 95), con límites\n",
    "        lens = df_out[col_name].astype(str).replace(\"nan\", \"\").str.len()\n",
    "        p95 = int(lens.quantile(0.95)) if len(lens) else 0\n",
    "        header_len = len(str(col_name))\n",
    "        width = min(max(8, max(p95, header_len) + 2), 50)\n",
    "        ws.column_dimensions[col_letter].width = width\n",
    "\n",
    "# (Opcional) shrink-to-fit en cuerpo para que el texto largo se vea mejor sin cambiar ancho:\n",
    "for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=ws.max_column):\n",
    "    for cell in row:\n",
    "        cell.alignment = Alignment(vertical=\"center\")\n",
    "\n",
    "# === 3) Guardar y limpiar tmp ===\n",
    "wb.save(final_excel)\n",
    "\n",
    "try:\n",
    "    tmp_excel.unlink()\n",
    "except Exception as e:\n",
    "    print(\"No se pudo borrar temporal:\", e)\n",
    "\n",
    "print(\"Ajuste final OK → headers rotados y grises; anchos aplicados solo a columnas seleccionadas.\")\n"
   ],
   "id": "cdc94f3d0b991d80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se pudo borrar temporal: [WinError 32] El proceso no tiene acceso al archivo porque está siendo utilizado por otro proceso: 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\3G\\\\~tmp_Datos_Modified_order.xlsx'\n",
      "Ajuste final OK → headers rotados y grises; anchos aplicados solo a columnas seleccionadas.\n"
     ]
    }
   ],
   "execution_count": 79
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
