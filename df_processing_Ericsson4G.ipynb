{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-24T21:46:00.889649Z",
     "start_time": "2025-09-24T21:46:00.854905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Alignment, Font\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "# Ruta base (ajústala si cambia)\n",
    "BASE_DIR = Path(r\"C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\")\n",
    "\n",
    "# Lista de 31 encabezados, en el orden en que los quieres (eNBId queda en W si mantienes este orden)\n",
    "HEADERS = [\n",
    "    \"eNodeB Name\", \"CellName\", \"activePlmnList_mcc\", \"additionalPlmnList_mcc\",\n",
    "    \"administrativeState\", \"cellBarred\", \"cellId\", \"cellSubscriptionCapacity\",\n",
    "    \"channelSelectionSetSize\", \"dlChannelBandwidth\", \"earfcndl\", \"earfcnul\",\n",
    "    \"freqBand\", \"noOfPucchCqiUsers\", \"noOfPucchSrUsers\", \"operationalState\",\n",
    "    \"physicalLayerCellIdGroup\", \"physicalLayerSubCellId\", \"sectorCarrierRef\",\n",
    "    \"tac\", \"timeOfLastModification\", \"ulChannelBandwidth\",\n",
    "    \"eNBId\",  # <= posición deseada para W\n",
    "    \"eNodeB Name Unique\", \"LAT\", \"LON\", \"PCI\", \"AT&T_Site_Name\",\n",
    "    \"MOCN Activo por Celda\", \"Al menos una celda de MOCN encendida\", \"MME TEF\"\n",
    "]\n",
    "\n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T17:14:38.966489Z",
     "start_time": "2025-09-23T17:14:38.959201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def appendfiles(filenamepattern: str) -> str:\n",
    "    \"\"\"\n",
    "    Integra todos los TXT que matchean pattern + '_*.txt' en un solo archivo.\n",
    "    Devuelve el nombre del archivo integrado (sin ruta).\n",
    "    \"\"\"\n",
    "    searchpattern = str(BASE_DIR / f\"{filenamepattern}_*.txt\")\n",
    "    filestoread = glob.glob(searchpattern)\n",
    "\n",
    "    outputfile_name = f\"Integrated_{filenamepattern}_files.txt\"\n",
    "    output_path = BASE_DIR / outputfile_name\n",
    "\n",
    "    print(\"Buscando:\", searchpattern)\n",
    "    print(\"Archivos:\", filestoread)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as outputfile:\n",
    "        for name in filestoread:\n",
    "            with open(name, \"r\", encoding=\"utf-8\") as f:\n",
    "                outputfile.write(f.read())\n",
    "                print(\"Agregado:\", name)\n",
    "\n",
    "    print(\"Integrado =>\", outputfile_name)\n",
    "    return outputfile_name\n",
    "\n",
    "\n",
    "def cleanfile(filename: str, ignorelines=None) -> str:\n",
    "    \"\"\"\n",
    "    Elimina líneas que contengan cualquiera de los patrones indicados.\n",
    "    Devuelve el nombre del archivo limpio (sin ruta).\n",
    "    \"\"\"\n",
    "    if ignorelines is None:\n",
    "        ignorelines = [\"SubNetwork,\", \"instance(s)\", \"NodeId\"]\n",
    "\n",
    "    inputfile = BASE_DIR / filename\n",
    "    cleanfile_name = f\"Clean_{filename}\"\n",
    "    cleanfile_path = BASE_DIR / cleanfile_name\n",
    "\n",
    "    with open(inputfile, 'r', encoding=\"utf-8\") as f_in:\n",
    "        lines = f_in.readlines()\n",
    "\n",
    "    kept = []\n",
    "    for line in lines:\n",
    "        if any(p in line for p in ignorelines):\n",
    "            continue\n",
    "        kept.append(line)\n",
    "\n",
    "    with open(cleanfile_path, 'w', encoding=\"utf-8\") as f_out:\n",
    "        f_out.writelines(kept)\n",
    "\n",
    "    print(f\"Limpieza OK -> {cleanfile_name} ({len(kept)} líneas)\")\n",
    "    return cleanfile_name\n",
    "\n",
    "\n",
    "def convert_to_excel(cleanfile_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Lee TXT tab-delimited sin encabezados y guarda a Excel.\n",
    "    Devuelve el nombre del archivo Excel (sin ruta).\n",
    "    \"\"\"\n",
    "    cleanfile_path = BASE_DIR / cleanfile_name\n",
    "    out_xlsx = f\"Converted_{cleanfile_name}.xlsx\"\n",
    "    out_path = BASE_DIR / out_xlsx\n",
    "\n",
    "    df = pd.read_csv(cleanfile_path, delimiter='\\t', header=None)\n",
    "    df.to_excel(out_path, index=False, header=None)\n",
    "    print(f\"Convertido a Excel -> {out_xlsx}  (shape={df.shape})\")\n",
    "    return out_xlsx\n",
    "\n"
   ],
   "id": "6b2453460000e987",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T23:23:58.713938Z",
     "start_time": "2025-09-24T23:23:36.381330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# EUtranCellFDD\n",
    "eu_txt = appendfiles('EUtranCellFDD')\n",
    "eu_clean = cleanfile(eu_txt)\n",
    "eu_xlsx = convert_to_excel(eu_clean)\n",
    "\n",
    "# ENodeBFunction\n",
    "nb_txt = appendfiles('ENodeBFunction')\n",
    "nb_clean = cleanfile(nb_txt)\n",
    "nb_xlsx = convert_to_excel(nb_clean)\n",
    "\n",
    "# nodeid\n",
    "nd_txt = appendfiles('nodeid')\n",
    "nd_clean = cleanfile(nd_txt)\n",
    "nd_xlsx = convert_to_excel(nd_clean)\n",
    "\n",
    "# MME\n",
    "mme_txt = appendfiles('MME')\n",
    "mme_clean = cleanfile(mme_txt)\n",
    "mme_xlsx = convert_to_excel(mme_clean)\n",
    "\n"
   ],
   "id": "822c1f2141799a7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\EUtranCellFDD_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\EUtranCellFDD_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\EUtranCellFDD_9.txt']\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\EUtranCellFDD_14.txt\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\EUtranCellFDD_9.txt\n",
      "Integrado => Integrated_EUtranCellFDD_files.txt\n",
      "Limpieza OK -> Clean_Integrated_EUtranCellFDD_files.txt (51830 líneas)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SCaracoza\\AppData\\Local\\Temp\\ipykernel_26912\\2274692285.py:62: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(cleanfile_path, delimiter='\\t', header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertido a Excel -> Converted_Clean_Integrated_EUtranCellFDD_files.txt.xlsx  (shape=(51814, 23))\n",
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\ENodeBFunction_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\ENodeBFunction_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\ENodeBFunction_9.txt']\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\ENodeBFunction_14.txt\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\ENodeBFunction_9.txt\n",
      "Integrado => Integrated_ENodeBFunction_files.txt\n",
      "Limpieza OK -> Clean_Integrated_ENodeBFunction_files.txt (6953 líneas)\n",
      "Convertido a Excel -> Converted_Clean_Integrated_ENodeBFunction_files.txt.xlsx  (shape=(6937, 3))\n",
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\nodeid_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\nodeid_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\nodeid_9.txt']\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\nodeid_14.txt\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\nodeid_9.txt\n",
      "Integrado => Integrated_nodeid_files.txt\n",
      "Limpieza OK -> Clean_Integrated_nodeid_files.txt (2658 líneas)\n",
      "Convertido a Excel -> Converted_Clean_Integrated_nodeid_files.txt.xlsx  (shape=(2648, 4))\n",
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\MME_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\MME_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\MME_9.txt']\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\MME_14.txt\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\MME_9.txt\n",
      "Integrado => Integrated_MME_files.txt\n",
      "Limpieza OK -> Clean_Integrated_MME_files.txt (39456 líneas)\n",
      "Convertido a Excel -> Converted_Clean_Integrated_MME_files.txt.xlsx  (shape=(39438, 9))\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T22:44:53.745443Z",
     "start_time": "2025-09-24T22:44:23.858590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Archivo base desde la conversión de EUtranCellFDD\n",
    "wb = load_workbook(BASE_DIR / eu_xlsx)  # ej. Converted_Clean_Integrated_EUtranCellFDD_files.txt.xlsx\n",
    "ws = wb.active\n",
    "\n",
    "ultima_fila = ws.max_row\n",
    "\n",
    "# Mover B -> +30 columnas (B1:B{ultima_fila} => AF1:AF{ultima_fila})\n",
    "rango = f\"B1:B{ultima_fila}\"\n",
    "ws.move_range(rango, rows=0, cols=30)\n",
    "\n",
    "# Mover C:AF -> -1 columna (C..AF => B..AE)\n",
    "rango = f\"C1:AE{ultima_fila}\"\n",
    "ws.move_range(rango, rows=0, cols=-1)\n",
    "\n",
    "wb.save(BASE_DIR / \"Modified_workfile.xlsx\")\n",
    "print(\"Reacomodo OK -> Modified_workfile.xlsx\")\n",
    "\n"
   ],
   "id": "d88444e0446c642e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reacomodo OK -> Modified_workfile.xlsx\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T22:59:22.925986Z",
     "start_time": "2025-09-24T22:58:54.266121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Leemos el archivo reacomodado SIN headers\n",
    "df_base = pd.read_excel(BASE_DIR / \"Modified_workfile.xlsx\", header=None)\n",
    "\n",
    "# Verificación de columnas\n",
    "n_cols = df_base.shape[1]\n",
    "print(\"Columnas detectadas en Modified_workfile.xlsx:\", n_cols)\n",
    "\n",
    "# Si tu tabla reacomodada debe tener exactamente len(HEADERS) columnas:\n",
    "expected = len(HEADERS)\n",
    "if n_cols < expected:\n",
    "    # agrega columnas vacías para completar\n",
    "    for i in range(expected - n_cols):\n",
    "        df_base[f\"__tmp_empty_{i}\"] = pd.NA\n",
    "    n_cols = expected\n",
    "\n",
    "# Asigna nombres: si hay más columnas que headers, nómbralas para NO perderlas\n",
    "if n_cols > expected:\n",
    "    extra_names = [\"\"for i in range(n_cols - expected)]\n",
    "    df_base.columns = HEADERS + extra_names\n",
    "else:\n",
    "    df_base.columns = HEADERS\n",
    "\n",
    "# (Opcional) guardado de control sin formato\n",
    "df_base.to_excel(BASE_DIR / \"Modified_with_headers.xlsx\", index=False)\n",
    "print(\"Headers asignados en pandas -> Modified_with_headers_pandas.xlsx\")\n",
    "\n"
   ],
   "id": "46be44fb1152b0a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas detectadas en Modified_workfile.xlsx: 32\n",
      "Headers asignados en pandas -> Modified_with_headers_pandas.xlsx\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T23:38:18.680971Z",
     "start_time": "2025-09-24T23:37:42.986053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- eNBId desde ENodeBFunction ---\n",
    "df_nodeb = pd.read_excel(BASE_DIR / nb_xlsx, header=None, usecols=[0, 1, 2])\n",
    "# Lee el archivo Excel convertido de ENodeBFunction (ruta 'nb_xlsx'),\n",
    "# sin encabezado (header=None), y solo las 3 primeras columnas (0,1,2).\n",
    "\n",
    "df_nodeb.columns = [\"NodeId\", \"ENodeBFunctionId\", \"eNBIdnew\"]\n",
    "# Asigna nombres a las 3 columnas: NodeId (clave), ENodeBFunctionId (solo informativo),\n",
    "# y eNBIdnew (el valor que queremos traer por JOIN).\n",
    "\n",
    "df_base[\"eNodeB Name\"] = df_base[\"eNodeB Name\"].astype(str)\n",
    "# Asegura que la columna clave en df_base sea string (evita mismatches de tipo).\n",
    "\n",
    "df_nodeb[\"NodeId\"] = df_nodeb[\"NodeId\"].astype(str)\n",
    "# Asegura que la clave en el catálogo (NodeId) también sea string.\n",
    "\n",
    "df_nodeb = df_nodeb.drop_duplicates(subset=[\"NodeId\"], keep=\"first\")\n",
    "# Si hay filas duplicadas por NodeId en el catálogo, conserva la primera\n",
    "# para evitar que el merge genere duplicados.\n",
    "\n",
    "df_tmp = df_base.merge(df_nodeb[[\"NodeId\", \"eNBIdnew\"]],\n",
    "                       left_on=\"eNodeB Name\", right_on=\"NodeId\", how=\"left\")\n",
    "# LEFT JOIN: por cada fila de df_base, busca en df_nodeb la fila con el mismo NodeId.\n",
    "# - Clave izquierda: eNodeB Name (df_base)\n",
    "# - Clave derecha: NodeId (df_nodeb)\n",
    "# - how=\"left\": conserva todas las filas de df_base aunque no haya match.\n",
    "\n",
    "df_base[\"eNBId\"] = df_tmp[\"eNBIdnew\"]\n",
    "# Copia (asigna) a df_base la columna eNBId con el valor traído (eNBIdnew).\n",
    "# Nota: si no hubo match, quedará NaN.\n",
    "\n",
    "# --- eNodeB Name Unique (solo cuando cambia) ---\n",
    "_name = df_base[\"eNodeB Name\"].astype(str).fillna(\"\").str.strip()\n",
    "# Toma la columna eNodeB Name, la convierte a str, reemplaza NaN por\"\",\n",
    "# y recorta espacios en extremos para comparar bien.\n",
    "\n",
    "is_new = _name.ne(_name.shift())\n",
    "# Crea una serie booleana True/False que vale True cuando\n",
    "# el nombre actual es diferente al de la fila anterior (inicio de bloque).\n",
    "\n",
    "df_base[\"eNodeB Name Unique\"] = np.where(is_new & _name.ne(\"\"), df_base[\"eNodeB Name\"], \"\")\n",
    "# Si cambia el nombre (is_new=True) y no está vacío: escribe el nombre.\n",
    "# En caso contrario: deja cadena vacía\"\" (equivalente a tu SI(A2=A1,\"\",A2)).\n",
    "\n",
    "# --- LAT/LON/AT&T_Site_Name desde All_Ericsson_4G_{YYYYMM} (mes anterior) ---\n",
    "# YYYYMM del mes anterior\n",
    "today = date.today()\n",
    "prev_year  = today.year if today.month > 1 else today.year - 1\n",
    "prev_month = today.month - 1 or 12\n",
    "yyyymm = f\"{prev_year}{prev_month:02d}\"\n",
    "\n",
    "# Ruta\n",
    "ae_path = BASE_DIR / f\"All_Ericsson_4G_{yyyymm}.xlsx\"\n",
    "\n",
    "# Cargar las columnas\n",
    "ae_df = pd.read_excel(ae_path, usecols=[\"eNodeB Name\", \"LAT\", \"LON\", \"AT&T_Site_Name\"])\n",
    "ae_df[\"eNodeB Name\"] = ae_df[\"eNodeB Name\"].astype(str).str.strip()\n",
    "ae_df = ae_df.drop_duplicates(subset=[\"eNodeB Name\"], keep=\"first\")\n",
    "\n",
    "# Merge y rellenado solo de NaN con valores del archivo All_Ericsson\n",
    "merged = df_base.merge(ae_df, on=\"eNodeB Name\", how=\"left\", suffixes=(\"\", \"_ae\"))\n",
    "for col in [\"LAT\", \"LON\", \"AT&T_Site_Name\"]:\n",
    "    merged[col] = merged[col].fillna(merged[col + \"_ae\"])\n",
    "    merged.drop(columns=[col + \"_ae\"], inplace=True)\n",
    "\n",
    "# --- Calcular PCI = IF(R blank, Q, Q*3 + R) y guardar ---\n",
    "q_col = \"physicalLayerCellIdGroup\"\n",
    "r_col = \"physicalLayerSubCellId\"\n",
    "\n",
    "merged[q_col] = pd.to_numeric(merged[q_col], errors=\"coerce\")\n",
    "merged[r_col] = pd.to_numeric(merged[r_col].astype(str).str.strip().replace({\"\": None}), errors=\"coerce\")\n",
    "\n",
    "merged[\"PCI\"] = pd.Series(\n",
    "    np.where(merged[r_col].isna(),\n",
    "             merged[q_col],\n",
    "             merged[q_col]*3 + merged[r_col]),\n",
    "    index=merged.index,\n",
    "    dtype=\"Int64\"\n",
    ")\n",
    "\n",
    "# --- Marcar \"MOCN Activo por Celda\" (match EXACTO) y guardar ---\n",
    "# Cadena exacta contra la que quieres comparar\n",
    "pattern = \"[{mncLength=3, mcc=334, mnc=90}, {mncLength=2, mcc=334, mnc=3}, {mncLength=2, mcc=1, mnc=1}, {mncLength=2, mcc=1, mnc=1}, {mncLength=2, mcc=1, mnc=1}]\"\n",
    "\n",
    "# Crear/actualizar la columna \"MOCN Activo por Celda\"\n",
    "merged[\"MOCN Activo por Celda\"] = np.where(\n",
    "    merged[\"additionalPlmnList_mcc\"].astype(str).str.strip() == pattern,\n",
    "    \"Si\",\n",
    "    \"No\"\n",
    ")\n",
    "# --- \"Al menos una celda de MOCN encendida\" basado en match eNodeB Name ∈ {AT&T_Site_Name con MOCN=SI} ---\n",
    "mocn_si_sites = (\n",
    "    df_base.loc[df_base[\"MOCN Activo por Celda\"] == \"Si\", \"AT&T_Site_Name\"]\n",
    "          .astype(str).str.strip()\n",
    ")\n",
    "mocn_si_sites = set(mocn_si_sites[mocn_si_sites.ne(\"\")])\n",
    "\n",
    "# 2) Para cada fila: SI si eNodeB Name (normalizado) aparece en ese conjunto; si no, NO\n",
    "merged[\"Al menos una celda de MOCN encendida\"] = np.where(\n",
    "    merged[\"eNodeB Name\"].astype(str).str.strip().isin(mocn_si_sites),\n",
    "    \"Si\",\n",
    "    \"No\"\n",
    ")\n",
    "# --- eNBId desde MME ---\n",
    "df_MME = pd.read_excel(BASE_DIR / mme_xlsx, header=None, usecols=[0,1,2], dtype=str)\n",
    "df_MME.columns = [\"NodeId\", \"eNodeBFunction\", \"TermPointToMmeId\"]\n",
    "df_MME[\"NodeId\"] = df_MME[\"NodeId\"].astype(str).str.strip()\n",
    "\n",
    "# Filtrar solo NodeId de longitud 7\n",
    "df_MME_7 = df_MME[df_MME[\"TermPointToMmeId\"].str.len() == 7].copy()\n",
    "\n",
    "# Conteo tipo COUNTIF: cuántas veces aparece cada NodeId\n",
    "mme_counts = df_MME_7[\"NodeId\"].value_counts()\n",
    "\n",
    "# Normalizar clave y mapear conteo a tu base por eNodeB Name\n",
    "df_base[\"eNodeB Name\"] = df_base[\"eNodeB Name\"].astype(str).str.strip()\n",
    "merged[\"MME TEF\"] = (\n",
    "    df_base[\"eNodeB Name\"].map(mme_counts).fillna(0).astype(\"Int64\")\n",
    ")\n",
    "\n",
    "\n",
    "df_base = merged\n",
    "# Actualiza df_base con el DataFrame enriquecido.\n",
    "\n",
    "# --- Guardar preliminar (lo tomará la [7] para formateo final) ---\n",
    "final_path = BASE_DIR / \"Datos_Modified.xlsx\"\n",
    "# Define la ruta del Excel preliminar (sin estilos).\n",
    "\n",
    "df_base.to_excel(final_path, index=False)\n",
    "# Escribe el Excel con todas las columnas (aquí todavía sin formato openpyxl).\n",
    "\n",
    "print(\"Guardado enriquecido ->\", final_path, \"shape=\", df_base.shape)\n",
    "# Log: confirma guardado y muestra dimensiones finales.\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "dd34249ef17f6f73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado enriquecido -> C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\Datos_Modified.xlsx shape= (51814, 32)\n"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T23:22:35.117510Z",
     "start_time": "2025-09-23T23:21:31.844003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "final_excel = BASE_DIR / \"Datos_Modified.xlsx\"\n",
    "tmp_excel = BASE_DIR / \"~tmp_Datos_Modified.xlsx\"\n",
    "\n",
    "# Releer, forzar columnas y orden\n",
    "df_out = pd.read_excel(final_excel)\n",
    "\n",
    "# Garantiza que TODAS las columnas existan\n",
    "for col in HEADERS:\n",
    "    if col not in df_out.columns:\n",
    "        df_out[col] = pd.NA\n",
    "\n",
    "# Reordena exactamente como HEADERS\n",
    "df_out = df_out[HEADERS]\n",
    "\n",
    "# Escribe temporal\n",
    "df_out.to_excel(tmp_excel, index=False)\n",
    "\n",
    "# Reaplicar estilo vertical de headers\n",
    "wb = load_workbook(tmp_excel)\n",
    "ws = wb.active\n",
    "\n",
    "# Congelar encabezado\n",
    "ws.freeze_panes = \"A2\"\n",
    "\n",
    "# Aplicar estilo a fila 1\n",
    "for col_idx, header in enumerate(HEADERS, start=1):\n",
    "    cell = ws.cell(row=1, column=col_idx)\n",
    "    cell.value = header\n",
    "    cell.font = Font(name=\"Aptos Narrow\", size=11)  # bold=True si quieres negrita\n",
    "    cell.alignment = Alignment(textRotation=90, horizontal=\"center\", vertical=\"bottom\", wrap_text=True)\n",
    "\n",
    "# (Opcional) Ajustar ancho mínimo por header vertical\n",
    "# for col in ws.iter_cols(min_row=1, max_row=1):\n",
    "#     ws.column_dimensions[col[0].column_letter].width = 6\n",
    "\n",
    "wb.save(final_excel)\n",
    "\n",
    "# Limpia temporal\n",
    "try:\n",
    "    tmp_excel.unlink()\n",
    "except Exception as e:\n",
    "    print(\"No se pudo borrar temporal:\", e)\n",
    "\n",
    "print(\"Ajuste final OK -> Headers verticales y columnas forzadas/ordenadas.\")\n",
    "\n"
   ],
   "id": "cdc94f3d0b991d80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajuste final OK -> Headers verticales y columnas forzadas/ordenadas.\n"
     ]
    }
   ],
   "execution_count": 57
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
