{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-23T19:42:04.723622Z",
     "start_time": "2025-09-23T19:42:04.701413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Alignment, Font\n",
    "import numpy as np\n",
    "\n",
    "# Ruta base (ajústala si cambia)\n",
    "BASE_DIR = Path(r\"C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\")\n",
    "\n",
    "# Lista de 31 encabezados, en el orden en que los quieres (eNBId queda en W si mantienes este orden)\n",
    "HEADERS = [\n",
    "    \"eNodeB Name\", \"CellName\", \"activePlmnList_mcc\", \"additionalPlmnList_mcc\",\n",
    "    \"administrativeState\", \"cellBarred\", \"cellId\", \"cellSubscriptionCapacity\",\n",
    "    \"channelSelectionSetSize\", \"dlChannelBandwidth\", \"earfcndl\", \"earfcnul\",\n",
    "    \"freqBand\", \"noOfPucchCqiUsers\", \"noOfPucchSrUsers\", \"operationalState\",\n",
    "    \"physicalLayerCellIdGroup\", \"physicalLayerSubCellId\", \"sectorCarrierRef\",\n",
    "    \"tac\", \"timeOfLastModification\", \"ulChannelBandwidth\",\n",
    "    \"eNBId\",  # <= posición deseada para W\n",
    "    \"eNodeB Name Unique\", \"LAT\", \"LON\", \"PCI\", \"AT&T_Site_Name\",\n",
    "    \"MOCN Activo por Celda\", \"Al menos una celda de MOCN encendida\", \"MME TEF\"\n",
    "]\n",
    "\n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T17:14:38.966489Z",
     "start_time": "2025-09-23T17:14:38.959201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def appendfiles(filenamepattern: str) -> str:\n",
    "    \"\"\"\n",
    "    Integra todos los TXT que matchean pattern + '_*.txt' en un solo archivo.\n",
    "    Devuelve el nombre del archivo integrado (sin ruta).\n",
    "    \"\"\"\n",
    "    searchpattern = str(BASE_DIR / f\"{filenamepattern}_*.txt\")\n",
    "    filestoread = glob.glob(searchpattern)\n",
    "\n",
    "    outputfile_name = f\"Integrated_{filenamepattern}_files.txt\"\n",
    "    output_path = BASE_DIR / outputfile_name\n",
    "\n",
    "    print(\"Buscando:\", searchpattern)\n",
    "    print(\"Archivos:\", filestoread)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as outputfile:\n",
    "        for name in filestoread:\n",
    "            with open(name, \"r\", encoding=\"utf-8\") as f:\n",
    "                outputfile.write(f.read())\n",
    "                print(\"Agregado:\", name)\n",
    "\n",
    "    print(\"Integrado =>\", outputfile_name)\n",
    "    return outputfile_name\n",
    "\n",
    "\n",
    "def cleanfile(filename: str, ignorelines=None) -> str:\n",
    "    \"\"\"\n",
    "    Elimina líneas que contengan cualquiera de los patrones indicados.\n",
    "    Devuelve el nombre del archivo limpio (sin ruta).\n",
    "    \"\"\"\n",
    "    if ignorelines is None:\n",
    "        ignorelines = [\"SubNetwork,\", \"instance(s)\", \"NodeId\"]\n",
    "\n",
    "    inputfile = BASE_DIR / filename\n",
    "    cleanfile_name = f\"Clean_{filename}\"\n",
    "    cleanfile_path = BASE_DIR / cleanfile_name\n",
    "\n",
    "    with open(inputfile, 'r', encoding=\"utf-8\") as f_in:\n",
    "        lines = f_in.readlines()\n",
    "\n",
    "    kept = []\n",
    "    for line in lines:\n",
    "        if any(p in line for p in ignorelines):\n",
    "            continue\n",
    "        kept.append(line)\n",
    "\n",
    "    with open(cleanfile_path, 'w', encoding=\"utf-8\") as f_out:\n",
    "        f_out.writelines(kept)\n",
    "\n",
    "    print(f\"Limpieza OK -> {cleanfile_name} ({len(kept)} líneas)\")\n",
    "    return cleanfile_name\n",
    "\n",
    "\n",
    "def convert_to_excel(cleanfile_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Lee TXT tab-delimited sin encabezados y guarda a Excel.\n",
    "    Devuelve el nombre del archivo Excel (sin ruta).\n",
    "    \"\"\"\n",
    "    cleanfile_path = BASE_DIR / cleanfile_name\n",
    "    out_xlsx = f\"Converted_{cleanfile_name}.xlsx\"\n",
    "    out_path = BASE_DIR / out_xlsx\n",
    "\n",
    "    df = pd.read_csv(cleanfile_path, delimiter='\\t', header=None)\n",
    "    df.to_excel(out_path, index=False, header=None)\n",
    "    print(f\"Convertido a Excel -> {out_xlsx}  (shape={df.shape})\")\n",
    "    return out_xlsx\n",
    "\n"
   ],
   "id": "6b2453460000e987",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T17:15:02.575077Z",
     "start_time": "2025-09-23T17:14:44.227355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# EUtranCellFDD\n",
    "eu_txt = appendfiles('EUtranCellFDD')\n",
    "eu_clean = cleanfile(eu_txt)\n",
    "eu_xlsx = convert_to_excel(eu_clean)\n",
    "\n",
    "# ENodeBFunction\n",
    "nb_txt = appendfiles('ENodeBFunction')\n",
    "nb_clean = cleanfile(nb_txt)\n",
    "nb_xlsx = convert_to_excel(nb_clean)\n",
    "\n",
    "# nodeid (si lo sigues usando más adelante)\n",
    "nd_txt = appendfiles('nodeid')\n",
    "nd_clean = cleanfile(nd_txt)\n",
    "nd_xlsx = convert_to_excel(nd_clean)\n",
    "\n"
   ],
   "id": "822c1f2141799a7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\EUtranCellFDD_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\EUtranCellFDD_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\EUtranCellFDD_9.txt']\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\EUtranCellFDD_14.txt\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\EUtranCellFDD_9.txt\n",
      "Integrado => Integrated_EUtranCellFDD_files.txt\n",
      "Limpieza OK -> Clean_Integrated_EUtranCellFDD_files.txt (51830 líneas)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SCaracoza\\AppData\\Local\\Temp\\ipykernel_26912\\2274692285.py:62: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(cleanfile_path, delimiter='\\t', header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertido a Excel -> Converted_Clean_Integrated_EUtranCellFDD_files.txt.xlsx  (shape=(51814, 23))\n",
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\ENodeBFunction_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\ENodeBFunction_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\ENodeBFunction_9.txt']\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\ENodeBFunction_14.txt\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\ENodeBFunction_9.txt\n",
      "Integrado => Integrated_ENodeBFunction_files.txt\n",
      "Limpieza OK -> Clean_Integrated_ENodeBFunction_files.txt (6953 líneas)\n",
      "Convertido a Excel -> Converted_Clean_Integrated_ENodeBFunction_files.txt.xlsx  (shape=(6937, 3))\n",
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\nodeid_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\nodeid_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\nodeid_9.txt']\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\nodeid_14.txt\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\nodeid_9.txt\n",
      "Integrado => Integrated_nodeid_files.txt\n",
      "Limpieza OK -> Clean_Integrated_nodeid_files.txt (2658 líneas)\n",
      "Convertido a Excel -> Converted_Clean_Integrated_nodeid_files.txt.xlsx  (shape=(2648, 4))\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T17:15:47.531433Z",
     "start_time": "2025-09-23T17:15:09.640157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Archivo base desde la conversión de EUtranCellFDD\n",
    "wb = load_workbook(BASE_DIR / eu_xlsx)  # ej. Converted_Clean_Integrated_EUtranCellFDD_files.txt.xlsx\n",
    "ws = wb.active\n",
    "\n",
    "ultima_fila = ws.max_row\n",
    "\n",
    "# Mover B -> +30 columnas (B1:B{ultima_fila} => AF1:AF{ultima_fila})\n",
    "rango = f\"B1:B{ultima_fila}\"\n",
    "ws.move_range(rango, rows=0, cols=30)\n",
    "\n",
    "# Mover C:AF -> -1 columna (C..AF => B..AE)\n",
    "rango = f\"C1:AF{ultima_fila}\"\n",
    "ws.move_range(rango, rows=0, cols=-1)\n",
    "\n",
    "wb.save(BASE_DIR / \"Modified_workfile.xlsx\")\n",
    "print(\"Reacomodo OK -> Modified_workfile.xlsx\")\n",
    "\n"
   ],
   "id": "d88444e0446c642e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reacomodo OK -> Modified_workfile.xlsx\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T19:44:08.302067Z",
     "start_time": "2025-09-23T19:43:33.974657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Leemos el archivo reacomodado SIN headers\n",
    "df_base = pd.read_excel(BASE_DIR / \"Modified_workfile.xlsx\", header=None)\n",
    "\n",
    "# Verificación de columnas\n",
    "n_cols = df_base.shape[1]\n",
    "print(\"Columnas detectadas en Modified_workfile.xlsx:\", n_cols)\n",
    "\n",
    "# Si tu tabla reacomodada debe tener exactamente len(HEADERS) columnas:\n",
    "expected = len(HEADERS)\n",
    "if n_cols < expected:\n",
    "    # si faltan columnas, agregamos columnas vacías para completar\n",
    "    for i in range(expected - n_cols):\n",
    "        df_base[f\"__tmp_empty_{i}\"] = pd.NA\n",
    "elif n_cols > expected:\n",
    "    # si sobran columnas, las recortamos (ajusta si necesitas conservar alguna)\n",
    "    df_base = df_base.iloc[:, :expected]\n",
    "\n",
    "# Asignar los nombres canónicos\n",
    "df_base.columns = HEADERS\n",
    "\n",
    "# (Opcional) guardado de control sin formato\n",
    "df_base.to_excel(BASE_DIR / \"Modified_with_headers.xlsx\", index=False)\n",
    "print(\"Headers asignados en pandas -> Modified_with_headers_pandas.xlsx\")\n",
    "\n"
   ],
   "id": "46be44fb1152b0a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas detectadas en Modified_workfile.xlsx: 31\n",
      "Headers asignados en pandas -> Modified_with_headers_pandas.xlsx\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T23:42:06.033378Z",
     "start_time": "2025-09-23T23:41:30.153253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- eNBId desde ENodeBFunction ---\n",
    "df_nodeb = pd.read_excel(BASE_DIR / nb_xlsx, header=None, usecols=[0, 1, 2])\n",
    "# Lee el archivo Excel convertido de ENodeBFunction (ruta 'nb_xlsx'),\n",
    "# sin encabezado (header=None), y solo las 3 primeras columnas (0,1,2).\n",
    "\n",
    "df_nodeb.columns = [\"NodeId\", \"ENodeBFunctionId\", \"eNBIdnew\"]\n",
    "# Asigna nombres a las 3 columnas: NodeId (clave), ENodeBFunctionId (solo informativo),\n",
    "# y eNBIdnew (el valor que queremos traer por JOIN).\n",
    "\n",
    "df_base[\"eNodeB Name\"] = df_base[\"eNodeB Name\"].astype(str)\n",
    "# Asegura que la columna clave en df_base sea string (evita mismatches de tipo).\n",
    "\n",
    "df_nodeb[\"NodeId\"] = df_nodeb[\"NodeId\"].astype(str)\n",
    "# Asegura que la clave en el catálogo (NodeId) también sea string.\n",
    "\n",
    "df_nodeb = df_nodeb.drop_duplicates(subset=[\"NodeId\"], keep=\"first\")\n",
    "# Si hay filas duplicadas por NodeId en el catálogo, conserva la primera\n",
    "# para evitar que el merge genere duplicados.\n",
    "\n",
    "df_tmp = df_base.merge(df_nodeb[[\"NodeId\", \"eNBIdnew\"]],\n",
    "                       left_on=\"eNodeB Name\", right_on=\"NodeId\", how=\"left\")\n",
    "# LEFT JOIN: por cada fila de df_base, busca en df_nodeb la fila con el mismo NodeId.\n",
    "# - Clave izquierda: eNodeB Name (df_base)\n",
    "# - Clave derecha: NodeId (df_nodeb)\n",
    "# - how=\"left\": conserva todas las filas de df_base aunque no haya match.\n",
    "\n",
    "df_base[\"eNBId\"] = df_tmp[\"eNBIdnew\"]\n",
    "# Copia (asigna) a df_base la columna eNBId con el valor traído (eNBIdnew).\n",
    "# Nota: si no hubo match, quedará NaN.\n",
    "\n",
    "# --- eNodeB Name Unique (solo cuando cambia) ---\n",
    "_name = df_base[\"eNodeB Name\"].astype(str).fillna(\"\").str.strip()\n",
    "# Toma la columna eNodeB Name, la convierte a str, reemplaza NaN por \"\",\n",
    "# y recorta espacios en extremos para comparar bien.\n",
    "\n",
    "is_new = _name.ne(_name.shift())\n",
    "# Crea una serie booleana True/False que vale True cuando\n",
    "# el nombre actual es diferente al de la fila anterior (inicio de bloque).\n",
    "\n",
    "df_base[\"eNodeB Name Unique\"] = np.where(is_new & _name.ne(\"\"), df_base[\"eNodeB Name\"], \"\")\n",
    "# Si cambia el nombre (is_new=True) y no está vacío: escribe el nombre.\n",
    "# En caso contrario: deja cadena vacía \"\" (equivalente a tu SI(A2=A1,\"\",A2)).\n",
    "\n",
    "# --- LAT/LON/AT&T_Site_Name desde All_Ericsson_4G_{YYYYMM} (mes anterior) ---\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "# Importa utilidades: 'date' para calcular mes anterior y 're' para regex en búsqueda de archivos.\n",
    "\n",
    "today = date.today()\n",
    "# Fecha de hoy (del sistema donde corre el notebook).\n",
    "\n",
    "prev_year, prev_month = (today.year, today.month - 1) if today.month > 1 else (today.year - 1, 12)\n",
    "# Calcula el mes anterior: si hoy es enero, retrocede al diciembre del año previo.\n",
    "\n",
    "yyyymm = f\"{prev_year}{prev_month:02d}\"\n",
    "# Formatea YYYYMM (p.ej. 202508 para agosto 2025).\n",
    "\n",
    "patterns = [f\"All_Ericsson_4G_{yyyymm}*.xlsx\", f\"All_Ericsson_4G_{yyyymm}*.xls\", f\"All_Ericsson_4G_{yyyymm}*\"]\n",
    "# Define patrones de búsqueda para localizar el archivo del mes anterior (por si tiene sufijo o variaciones).\n",
    "\n",
    "cands = []\n",
    "for pat in patterns:\n",
    "    cands += list(BASE_DIR.glob(pat))\n",
    "# Busca en BASE_DIR archivos que cumplan los patrones anteriores y acumúlalos en cands.\n",
    "\n",
    "if not cands:\n",
    "    all_cands = [p for p in BASE_DIR.glob(\"All_Ericsson_4G_*\")\n",
    "                 if (p.suffix.lower() in {\".xlsx\", \".xls\", \".csv\", \".xlsm\", \".xlsb\"}) or re.search(\n",
    "            r\"All_Ericsson_4G_\\d{6}\", p.name)]\n",
    "\n",
    "\n",
    "    # Si no hay match exacto del mes anterior:\n",
    "    # Toma cualquier archivo que parezca All_Ericsson_4G_* y tenga extensión válida\n",
    "    # o lleve YYYYMM en el nombre.\n",
    "\n",
    "    def ext_ym(p):\n",
    "        m = re.search(r\"All_Ericsson_4G_(\\d{6})\", p.name);\n",
    "        return int(m.group(1)) if m else -1\n",
    "\n",
    "\n",
    "    # Función auxiliar para extraer YYYYMM como entero (o -1 si no lo encuentra).\n",
    "\n",
    "    ae_path = sorted(all_cands, key=ext_ym)[-1] if all_cands else None\n",
    "    # Ordena por YYYYMM y toma el más reciente disponible (fallback).\n",
    "else:\n",
    "    ae_path = cands[0]\n",
    "# Si hubo candidatos exactos del mes anterior, toma el primero.\n",
    "\n",
    "if ae_path is None:\n",
    "    raise FileNotFoundError(f\"No se encontró All_Ericsson_4G para {yyyymm} en {BASE_DIR}\")\n",
    "# Si de plano no hay ningún archivo All_Ericsson_4G válido, detén el proceso con error claro.\n",
    "\n",
    "print(\"Usando All_Ericsson:\", ae_path.name)\n",
    "# Log informativo: qué archivo fue seleccionado.\n",
    "\n",
    "usecols = [\"eNodeB Name\", \"LAT\", \"LON\", \"AT&T_Site_Name\"]\n",
    "# Columnas que nos interesan traer del archivo All_Ericsson.\n",
    "\n",
    "if ae_path.suffix.lower() == \".csv\":\n",
    "    ae_df = pd.read_csv(ae_path, usecols=usecols)\n",
    "else:\n",
    "    ae_df = pd.read_excel(ae_path, usecols=usecols)\n",
    "# Lee el archivo All_Ericsson según su extensión (CSV o Excel) trayendo solo las columnas necesarias.\n",
    "\n",
    "ae_df[\"eNodeB Name\"] = ae_df[\"eNodeB Name\"].astype(str).str.strip()\n",
    "# Normaliza eNodeB Name en el catálogo All_Ericsson (str + strip).\n",
    "\n",
    "ae_df = ae_df.drop_duplicates(subset=[\"eNodeB Name\"], keep=\"first\")\n",
    "# Si hubiese duplicados por eNodeB Name en All_Ericsson, quédate con el primero.\n",
    "\n",
    "df_base[\"eNodeB Name\"] = df_base[\"eNodeB Name\"].astype(str).str.strip()\n",
    "# Normaliza eNodeB Name en tu base (asegura match correcto).\n",
    "\n",
    "merged = df_base.merge(ae_df, on=\"eNodeB Name\", how=\"left\", suffixes=(\"\", \"_ae\"))\n",
    "# LEFT JOIN entre tu base y el All_Ericsson por eNodeB Name.\n",
    "# Trae columnas LAT, LON y AT&T_Site_Name con sufijo \"_ae\" si colisionan.\n",
    "\n",
    "for col in [\"LAT\", \"LON\", \"AT&T_Site_Name\"]:\n",
    "    if col + \"_ae\" in merged.columns:\n",
    "        merged[col] = merged[col].fillna(merged[col + \"_ae\"])\n",
    "        merged.drop(columns=[col + \"_ae\"], inplace=True)\n",
    "# Para cada columna objetivo:\n",
    "# - Si ya existía en df_base, solo rellena los NaN con el valor del catálogo (_ae).\n",
    "# - Luego elimina la columna auxiliar *_ae.\n",
    "# (Si quisieras sobrescribir siempre, harías: merged[col] = merged[col + \"_ae\"]).\n",
    "\n",
    "df_base = merged\n",
    "# Actualiza df_base con el DataFrame enriquecido.\n",
    "\n",
    "# --- Guardar preliminar (lo tomará la [7] para formateo final) ---\n",
    "final_path = BASE_DIR / \"Datos_Modified.xlsx\"\n",
    "# Define la ruta del Excel preliminar (sin estilos).\n",
    "\n",
    "df_base.to_excel(final_path, index=False)\n",
    "# Escribe el Excel con todas las columnas (aquí todavía sin formato openpyxl).\n",
    "\n",
    "print(\"Guardado enriquecido ->\", final_path, \"shape=\", df_base.shape)\n",
    "# Log: confirma guardado y muestra dimensiones finales.\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "dd34249ef17f6f73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando All_Ericsson: All_Ericsson_4G_202508.xlsx\n",
      "Guardado enriquecido -> C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\Datos_Modified.xlsx shape= (51814, 31)\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T23:22:35.117510Z",
     "start_time": "2025-09-23T23:21:31.844003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "final_excel = BASE_DIR / \"Datos_Modified.xlsx\"\n",
    "tmp_excel = BASE_DIR / \"~tmp_Datos_Modified.xlsx\"\n",
    "\n",
    "# Releer, forzar columnas y orden\n",
    "df_out = pd.read_excel(final_excel)\n",
    "\n",
    "# Garantiza que TODAS las columnas existan\n",
    "for col in HEADERS:\n",
    "    if col not in df_out.columns:\n",
    "        df_out[col] = pd.NA\n",
    "\n",
    "# Reordena exactamente como HEADERS\n",
    "df_out = df_out[HEADERS]\n",
    "\n",
    "# Escribe temporal\n",
    "df_out.to_excel(tmp_excel, index=False)\n",
    "\n",
    "# Reaplicar estilo vertical de headers\n",
    "wb = load_workbook(tmp_excel)\n",
    "ws = wb.active\n",
    "\n",
    "# Congelar encabezado\n",
    "ws.freeze_panes = \"A2\"\n",
    "\n",
    "# Aplicar estilo a fila 1\n",
    "for col_idx, header in enumerate(HEADERS, start=1):\n",
    "    cell = ws.cell(row=1, column=col_idx)\n",
    "    cell.value = header\n",
    "    cell.font = Font(name=\"Aptos Narrow\", size=11)  # bold=True si quieres negrita\n",
    "    cell.alignment = Alignment(textRotation=90, horizontal=\"center\", vertical=\"bottom\", wrap_text=True)\n",
    "\n",
    "# (Opcional) Ajustar ancho mínimo por header vertical\n",
    "# for col in ws.iter_cols(min_row=1, max_row=1):\n",
    "#     ws.column_dimensions[col[0].column_letter].width = 6\n",
    "\n",
    "wb.save(final_excel)\n",
    "\n",
    "# Limpia temporal\n",
    "try:\n",
    "    tmp_excel.unlink()\n",
    "except Exception as e:\n",
    "    print(\"No se pudo borrar temporal:\", e)\n",
    "\n",
    "print(\"Ajuste final OK -> Headers verticales y columnas forzadas/ordenadas.\")\n",
    "\n"
   ],
   "id": "cdc94f3d0b991d80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajuste final OK -> Headers verticales y columnas forzadas/ordenadas.\n"
     ]
    }
   ],
   "execution_count": 57
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
