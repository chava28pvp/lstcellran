{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-26T16:08:49.123948Z",
     "start_time": "2025-09-26T16:08:49.109734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Alignment, Font\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "# Ruta base (ajústala si cambia)\n",
    "BASE_DIR = Path(r\"C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\")\n",
    "\n",
    "# Lista de 31 encabezados, en el orden en que los quieres (eNBId queda en W si mantienes este orden)\n",
    "HEADERS = [\n",
    "     \"eNodeB Name\",\"CellName\",\"activePlmnList_mcc\",\"additionalPlmnList_mcc\",\n",
    "    \"administrativeState\",\"cellBarred\",\"cellId\",\"cellSubscriptionCapacity\",\n",
    "    \"channelSelectionSetSize\",\"dlChannelBandwidth\",\"earfcndl\",\"earfcnul\",\n",
    "    \"freqBand\",\"noOfPucchCqiUsers\",\"noOfPucchSrUsers\",\"operationalState\",\n",
    "    \"physicalLayerCellIdGroup\",\"physicalLayerSubCellId\",\"sectorCarrierRef\",\n",
    "    \"tac\",\"timeOfLastModification\",\"ulChannelBandwidth\",\n",
    "    \"eNBId\",\"eNodeB Name Unique\",\"LAT\",\"LON\",\"PCI\",\"AT&T_Site_Name\",\n",
    "    \"MOCN Activo por Celda\",\"Al menos una celda de MOCN encendida\",\"MME TEF\",\n",
    "    \"ENodeBFunctionId\"\n",
    "]\n",
    "\n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T16:08:51.263303Z",
     "start_time": "2025-09-26T16:08:51.235035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def appendfiles(filenamepattern: str) -> str:\n",
    "    \"\"\"\n",
    "    Integra todos los TXT que matchean pattern + '_*.txt' en un solo archivo.\n",
    "    Devuelve el nombre del archivo integrado (sin ruta).\n",
    "    \"\"\"\n",
    "    searchpattern = str(BASE_DIR / f\"{filenamepattern}_*.txt\")\n",
    "    filestoread = glob.glob(searchpattern)\n",
    "\n",
    "    outputfile_name = f\"Integrated_{filenamepattern}_files.txt\"\n",
    "    output_path = BASE_DIR / outputfile_name\n",
    "\n",
    "    print(\"Buscando:\", searchpattern)\n",
    "    print(\"Archivos:\", filestoread)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as outputfile:\n",
    "        for name in filestoread:\n",
    "            with open(name, \"r\", encoding=\"utf-8\") as f:\n",
    "                outputfile.write(f.read())\n",
    "                print(\"Agregado:\", name)\n",
    "\n",
    "    print(\"Integrado =>\", outputfile_name)\n",
    "    return outputfile_name\n",
    "\n",
    "\n",
    "def cleanfile(filename: str, ignorelines=None) -> str:\n",
    "    \"\"\"\n",
    "    Elimina líneas que contengan cualquiera de los patrones indicados.\n",
    "    Devuelve el nombre del archivo limpio (sin ruta).\n",
    "    \"\"\"\n",
    "    if ignorelines is None:\n",
    "        ignorelines = [\"SubNetwork,\", \"instance(s)\", \"NodeId\"]\n",
    "\n",
    "    inputfile = BASE_DIR / filename\n",
    "    cleanfile_name = f\"Clean_{filename}\"\n",
    "    cleanfile_path = BASE_DIR / cleanfile_name\n",
    "\n",
    "    with open(inputfile, 'r', encoding=\"utf-8\") as f_in:\n",
    "        lines = f_in.readlines()\n",
    "\n",
    "    kept = []\n",
    "    for line in lines:\n",
    "        if any(p in line for p in ignorelines):\n",
    "            continue\n",
    "        kept.append(line)\n",
    "\n",
    "    with open(cleanfile_path, 'w', encoding=\"utf-8\") as f_out:\n",
    "        f_out.writelines(kept)\n",
    "\n",
    "    print(f\"Limpieza OK -> {cleanfile_name} ({len(kept)} líneas)\")\n",
    "    return cleanfile_name\n",
    "\n",
    "\n",
    "def convert_to_excel(cleanfile_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Lee TXT tab-delimited sin encabezados y guarda a Excel.\n",
    "    Devuelve el nombre del archivo Excel (sin ruta).\n",
    "    \"\"\"\n",
    "    cleanfile_path = BASE_DIR / cleanfile_name\n",
    "    out_xlsx = f\"Converted_{cleanfile_name}.xlsx\"\n",
    "    out_path = BASE_DIR / out_xlsx\n",
    "\n",
    "    df = pd.read_csv(cleanfile_path, delimiter='\\t', header=None)\n",
    "    df.to_excel(out_path, index=False, header=None)\n",
    "    print(f\"Convertido a Excel -> {out_xlsx}  (shape={df.shape})\")\n",
    "    return out_xlsx\n",
    "\n"
   ],
   "id": "6b2453460000e987",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T16:09:21.580299Z",
     "start_time": "2025-09-26T16:08:55.053241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# EUtranCellFDD\n",
    "eu_txt = appendfiles('EUtranCellFDD')\n",
    "eu_clean = cleanfile(eu_txt)\n",
    "eu_xlsx = convert_to_excel(eu_clean)\n",
    "\n",
    "# ENodeBFunction\n",
    "nb_txt = appendfiles('ENodeBFunction')\n",
    "nb_clean = cleanfile(nb_txt)\n",
    "nb_xlsx = convert_to_excel(nb_clean)\n",
    "\n",
    "# nodeid\n",
    "nd_txt = appendfiles('nodeid')\n",
    "nd_clean = cleanfile(nd_txt)\n",
    "nd_xlsx = convert_to_excel(nd_clean)\n",
    "\n",
    "# MME\n",
    "mme_txt = appendfiles('MME')\n",
    "mme_clean = cleanfile(mme_txt)\n",
    "mme_xlsx = convert_to_excel(mme_clean)\n",
    "#NblotCell\n",
    "nbiot_xlsx = appendfiles('NbIotCell')\n",
    "nbiot_clean = cleanfile(nbiot_xlsx)\n",
    "nbiot_xlsx = convert_to_excel(nbiot_clean)\n",
    "\n"
   ],
   "id": "822c1f2141799a7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\EUtranCellFDD_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\EUtranCellFDD_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\EUtranCellFDD_9.txt']\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\EUtranCellFDD_14.txt\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\EUtranCellFDD_9.txt\n",
      "Integrado => Integrated_EUtranCellFDD_files.txt\n",
      "Limpieza OK -> Clean_Integrated_EUtranCellFDD_files.txt (51830 líneas)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SCaracoza\\AppData\\Local\\Temp\\ipykernel_6388\\2274692285.py:62: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(cleanfile_path, delimiter='\\t', header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertido a Excel -> Converted_Clean_Integrated_EUtranCellFDD_files.txt.xlsx  (shape=(51814, 23))\n",
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\ENodeBFunction_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\ENodeBFunction_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\ENodeBFunction_9.txt']\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\ENodeBFunction_14.txt\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\ENodeBFunction_9.txt\n",
      "Integrado => Integrated_ENodeBFunction_files.txt\n",
      "Limpieza OK -> Clean_Integrated_ENodeBFunction_files.txt (6953 líneas)\n",
      "Convertido a Excel -> Converted_Clean_Integrated_ENodeBFunction_files.txt.xlsx  (shape=(6937, 3))\n",
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\nodeid_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\nodeid_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\nodeid_9.txt']\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\nodeid_14.txt\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\nodeid_9.txt\n",
      "Integrado => Integrated_nodeid_files.txt\n",
      "Limpieza OK -> Clean_Integrated_nodeid_files.txt (2658 líneas)\n",
      "Convertido a Excel -> Converted_Clean_Integrated_nodeid_files.txt.xlsx  (shape=(2648, 4))\n",
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\MME_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\MME_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\MME_9.txt']\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\MME_14.txt\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\MME_9.txt\n",
      "Integrado => Integrated_MME_files.txt\n",
      "Limpieza OK -> Clean_Integrated_MME_files.txt (39456 líneas)\n",
      "Convertido a Excel -> Converted_Clean_Integrated_MME_files.txt.xlsx  (shape=(39438, 9))\n",
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\NbIotCell_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\NbIotCell_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\NbIotCell_9.txt']\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\NbIotCell_14.txt\n",
      "Agregado: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\NbIotCell_9.txt\n",
      "Integrado => Integrated_NbIotCell_files.txt\n",
      "Limpieza OK -> Clean_Integrated_NbIotCell_files.txt (7529 líneas)\n",
      "Convertido a Excel -> Converted_Clean_Integrated_NbIotCell_files.txt.xlsx  (shape=(7517, 14))\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T16:09:57.188502Z",
     "start_time": "2025-09-26T16:09:24.489833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Archivo base desde la conversión de EUtranCellFDD\n",
    "wb = load_workbook(BASE_DIR / eu_xlsx)  # ej. Converted_Clean_Integrated_EUtranCellFDD_files.txt.xlsx\n",
    "ws = wb.active\n",
    "\n",
    "ultima_fila = ws.max_row\n",
    "\n",
    "# Mover B -> +30 columnas (B1:B{ultima_fila} => AF1:AF{ultima_fila})\n",
    "rango = f\"B1:B{ultima_fila}\"\n",
    "ws.move_range(rango, rows=0, cols=30)\n",
    "\n",
    "# Mover C:AF -> -1 columna (C..AF => B..AE)\n",
    "rango = f\"C1:AE{ultima_fila}\"\n",
    "ws.move_range(rango, rows=0, cols=-1)\n",
    "\n",
    "wb.save(BASE_DIR / \"Modified_workfile.xlsx\")\n",
    "print(\"Reacomodo OK -> Modified_workfile.xlsx\")\n",
    "\n"
   ],
   "id": "d88444e0446c642e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reacomodo OK -> Modified_workfile.xlsx\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T16:10:46.267396Z",
     "start_time": "2025-09-26T16:10:08.036955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Leemos el archivo reacomodado SIN headers\n",
    "df_base = pd.read_excel(BASE_DIR / \"Modified_workfile.xlsx\", header=None)\n",
    "\n",
    "# Verificación de columnas\n",
    "n_cols = df_base.shape[1]\n",
    "print(\"Columnas detectadas en Modified_workfile.xlsx:\", n_cols)\n",
    "\n",
    "# Si tu tabla reacomodada debe tener exactamente len(HEADERS) columnas:\n",
    "expected = len(HEADERS)\n",
    "if n_cols < expected:\n",
    "    # agrega columnas vacías para completar\n",
    "    for i in range(expected - n_cols):\n",
    "        df_base[f\"__tmp_empty_{i}\"] = pd.NA\n",
    "    n_cols = expected\n",
    "\n",
    "# Asigna nombres: si hay más columnas que headers, nómbralas para NO perderlas\n",
    "if n_cols > expected:\n",
    "    extra_names = [\"\"for i in range(n_cols - expected)]\n",
    "    df_base.columns = HEADERS + extra_names\n",
    "else:\n",
    "    df_base.columns = HEADERS\n",
    "\n",
    "# (Opcional) guardado de control sin formato\n",
    "df_base.to_excel(BASE_DIR / \"Modified_with_headers.xlsx\", index=False)\n",
    "print(\"Headers asignados en pandas -> Modified_with_headers_pandas.xlsx\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "d6659a16606a67c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas detectadas en Modified_workfile.xlsx: 32\n",
      "Headers asignados en pandas -> Modified_with_headers_pandas.xlsx\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T19:04:56.723060Z",
     "start_time": "2025-09-26T19:04:31.555524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_base = df_base.loc[:, [c for c in df_base.columns if c in HEADERS]]\n",
    "for c in HEADERS:\n",
    "    if c not in df_base.columns:\n",
    "        df_base[c] = pd.NA\n",
    "df_base = df_base[HEADERS]\n",
    "\n",
    "# --- 2) Cargar NbIot sin headers (A..N => 14 cols) ---\n",
    "df_nbiot = pd.read_excel(BASE_DIR / nbiot_xlsx, header=None, dtype=str)\n",
    "if df_nbiot.shape[1] < 14:\n",
    "    raise ValueError(\"NbIotCell debe tener al menos 14 columnas (A..N).\")\n",
    "df_nbiot = df_nbiot.iloc[:, :14]\n",
    "\n",
    "# --- 3) Construir bloque NbIot alineado por POSICIÓN (tu mapeo) ---\n",
    "# A->A, B->AF, C->B, D->C, E->E, F->F, G->G, H->K, I->L, J->P, K->R, L->D, M->S, N->T\n",
    "df_nb = pd.DataFrame(pd.NA, index=df_nbiot.index, columns=HEADERS)\n",
    "df_nb[\"eNodeB Name\"]               = df_nbiot.iloc[:, 0]   # A -> A\n",
    "df_nb[\"ENodeBFunctionId\"]          = df_nbiot.iloc[:, 1]   # B -> AF\n",
    "df_nb[\"CellName\"]                  = df_nbiot.iloc[:, 2]   # C -> B\n",
    "df_nb[\"activePlmnList_mcc\"]        = df_nbiot.iloc[:, 3]   # D -> C\n",
    "df_nb[\"administrativeState\"]       = df_nbiot.iloc[:, 4]   # E -> E\n",
    "df_nb[\"cellBarred\"]                = df_nbiot.iloc[:, 5]   # F -> F\n",
    "df_nb[\"cellId\"]                    = df_nbiot.iloc[:, 6]   # G -> G\n",
    "df_nb[\"earfcndl\"]                  = df_nbiot.iloc[:, 7]   # H -> K\n",
    "df_nb[\"earfcnul\"]                  = df_nbiot.iloc[:, 8]   # I -> L\n",
    "df_nb[\"operationalState\"]          = df_nbiot.iloc[:, 9]   # J -> P\n",
    "df_nb[\"physicalLayerCellIdGroup\"]    = df_nbiot.iloc[:,10]   # K -> R\n",
    "df_nb[\"additionalPlmnList_mcc\"]    = df_nbiot.iloc[:,11]   # L -> D\n",
    "df_nb[\"sectorCarrierRef\"]          = df_nbiot.iloc[:,12]   # M -> S\n",
    "df_nb[\"tac\"]                       = df_nbiot.iloc[:,13]   # N -> T\n",
    "\n",
    "# --- 4) Pegar debajo y guardar ---\n",
    "df_out = pd.concat([df_base, df_nb], ignore_index=True)\n",
    "df_out.to_excel(BASE_DIR / \"Modified_with_headers.xlsx\", index=False)\n",
    "print(\"OK: NbIotCell agregado debajo, con B->AF y el resto según mapeo por posición.\")"
   ],
   "id": "941410eb832d68a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: NbIotCell agregado debajo, con B->AF y el resto según mapeo por posición.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T22:55:02.159958Z",
     "start_time": "2025-09-25T22:51:30.011654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MA. 20250924.\n",
    "### Información del EPT ###\n",
    "\n",
    "# Prefijo del archivo\n",
    "prefijo_ept = 'EPT_ATT_UMTS_LTE_'\n",
    "\n",
    "# Busca archivo que empiece con el prefijo\n",
    "archivo = glob.glob(os.path.join(BASE_DIR, f'{prefijo_ept}*.xlsx'))\n",
    "\n",
    "# Verifica si se encontró archivo\n",
    "if archivo:\n",
    "    archivo_encontrado = archivo[0]\n",
    "    # Crea DataFrame por cada hoja de Excel\n",
    "    df_EPT_3G_LTE_OUTDOOR = pd.read_excel(archivo_encontrado, sheet_name='EPT_3G_LTE_OUTDOOR', engine='openpyxl')\n",
    "    df_PLAN_OUTDOOR = pd.read_excel(archivo_encontrado, sheet_name='PLAN_OUTDOOR', engine='openpyxl')\n",
    "    df_EPT_3G_LTE_INDOOR = pd.read_excel(archivo_encontrado, sheet_name='EPT_3G_LTE_INDOOR', engine='openpyxl')\n",
    "    df_PLAN_INDOOR = pd.read_excel(archivo_encontrado, sheet_name='PLAN_INDOOR', engine='openpyxl')\n",
    "    df_Eventos_Especiales = pd.read_excel(archivo_encontrado, sheet_name='Eventos_Especiales', engine='openpyxl')\n",
    "\n",
    "# Crea un solo DataFrame unificado\n",
    "lista_dfs = [df_EPT_3G_LTE_OUTDOOR, df_PLAN_OUTDOOR, df_EPT_3G_LTE_INDOOR, df_PLAN_INDOOR, df_Eventos_Especiales]\n",
    "df_EPT_inicial = pd.concat(lista_dfs, ignore_index=True)\n",
    "\n",
    "# Elimina registros duplicados\n",
    "df_EPT_inicial.drop_duplicates()\n",
    "\n",
    "# Renombramiento columna(s)\n",
    "nuevos_nombres = {'Latitud' : 'LAT', 'Longitud' : 'LON'}\n",
    "df_EPT_inicial.rename(columns=nuevos_nombres, inplace=True)\n",
    "\n",
    "# EPT\n",
    "df_EPT_final = df_EPT_inicial[['LAT', 'LON', 'AT&T_Site_Name']]\n",
    "\n",
    "# Archivo unificado en Excel, para fines de validación.\n",
    "#ruta_salida = os.path.join(ruta_destino, f'EPT_{fecha_ejecucion}.xlsx)\n",
    "#df_EPT_final.to_excel(ruta_salida, index=False) #, encoding='utf-8')"
   ],
   "id": "87311c5778ca3718",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T19:24:57.699322Z",
     "start_time": "2025-09-26T19:23:35.107638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- eNBId desde ENodeBFunction ---\n",
    "df_nodeb = pd.read_excel(BASE_DIR / nb_xlsx, header=None, usecols=[0, 1, 2])\n",
    "# Lee el archivo Excel convertido de ENodeBFunction (ruta 'nb_xlsx'),\n",
    "# sin encabezado (header=None), y solo las 3 primeras columnas (0,1,2).\n",
    "\n",
    "df_nodeb.columns = [\"NodeId\", \"ENodeBFunctionId\", \"eNBIdnew\"]\n",
    "# Asigna nombres a las 3 columnas: NodeId (clave), ENodeBFunctionId (solo informativo),\n",
    "# y eNBIdnew (el valor que queremos traer por JOIN).\n",
    "\n",
    "df_out[\"eNodeB Name\"] = df_out[\"eNodeB Name\"].astype(str)\n",
    "# Asegura que la columna clave en df_out sea string (evita mismatches de tipo).\n",
    "\n",
    "df_nodeb[\"NodeId\"] = df_nodeb[\"NodeId\"].astype(str)\n",
    "# Asegura que la clave en el catálogo (NodeId) también sea string.\n",
    "\n",
    "df_nodeb = df_nodeb.drop_duplicates(subset=[\"NodeId\"], keep=\"first\")\n",
    "# Si hay filas duplicadas por NodeId en el catálogo, conserva la primera\n",
    "# para evitar que el merge genere duplicados.\n",
    "\n",
    "df_tmp = df_out.merge(df_nodeb[[\"NodeId\", \"eNBIdnew\"]],\n",
    "                       left_on=\"eNodeB Name\", right_on=\"NodeId\", how=\"left\")\n",
    "# LEFT JOIN: por cada fila de df_out, busca en df_nodeb la fila con el mismo NodeId.\n",
    "# - Clave izquierda: eNodeB Name (df_out)\n",
    "# - Clave derecha: NodeId (df_nodeb)\n",
    "# - how=\"left\": conserva todas las filas de df_out aunque no haya match.\n",
    "\n",
    "df_out[\"eNBId\"] = df_tmp[\"eNBIdnew\"]\n",
    "# Copia (asigna) a df_out la columna eNBId con el valor traído (eNBIdnew).\n",
    "# Nota: si no hubo match, quedará NaN.\n",
    "\n",
    "# --- eNodeB Name Unique (solo cuando cambia) ---\n",
    "#df_out es mi df main\n",
    "_name = df_out[\"eNodeB Name\"].astype(str).fillna(\"\").str.strip()\n",
    "# Toma la columna eNodeB Name, la convierte a str, reemplaza NaN por\"\",\n",
    "# y recorta espacios en extremos para comparar bien.\n",
    "\n",
    "is_new = _name.ne(_name.shift())\n",
    "# Crea una serie booleana True/False que vale True cuando\n",
    "# el nombre actual es diferente al de la fila anterior (inicio de bloque).\n",
    "\n",
    "df_out[\"eNodeB Name Unique\"] = np.where(is_new & _name.ne(\"\"), df_out[\"eNodeB Name\"], \"\")\n",
    "# Si cambia el nombre (is_new=True) y no está vacío: escribe el nombre.\n",
    "# En caso contrario: deja cadena vacía\"\" (equivalente a tu SI(A2=A1,\"\",A2)).\n",
    "\n",
    "# --- LAT/LON/AT&T_Site_Name desde All_Ericsson_4G_{YYYYMM} (mes anterior) ---\n",
    "# YYYYMM del mes anterior\n",
    "# 1) YYYYMM del mes anterior\n",
    "# 1) YYYYMM del mes anterior\n",
    "today = date.today()\n",
    "prev_year  = today.year if today.month > 1 else today.year - 1\n",
    "prev_month = today.month - 1 or 12\n",
    "yyyymm = f\"{prev_year}{prev_month:02d}\"\n",
    "\n",
    "# 2) Fuente principal: All_Ericsson\n",
    "ae_path = BASE_DIR / f\"All_Ericsson_4G_{yyyymm}.xlsx\"\n",
    "ae_df = pd.read_excel(ae_path, usecols=[\"eNodeB Name\", \"LAT\", \"LON\", \"AT&T_Site_Name\"])\n",
    "ae_df[\"eNodeB Name\"] = ae_df[\"eNodeB Name\"].astype(str).str.strip()\n",
    "ae_df = ae_df.drop_duplicates(subset=[\"eNodeB Name\"], keep=\"first\")\n",
    "\n",
    "df_out[\"eNodeB Name\"] = df_out[\"eNodeB Name\"].astype(str).str.strip()\n",
    "\n",
    "merged = df_out.merge(ae_df, on=\"eNodeB Name\", how=\"left\", suffixes=(\"\", \"_ae\"))\n",
    "def _is_blank(s):\n",
    "    # NaN o string vacío (incluye espacios)\n",
    "    return s.isna() | s.astype(str).str.strip().eq(\"\")\n",
    "\n",
    "for col in [\"LAT\", \"LON\", \"AT&T_Site_Name\"]:\n",
    "    m = _is_blank(merged[col])\n",
    "    merged.loc[m, col] = merged.loc[m, col + \"_ae\"]\n",
    "    merged.drop(columns=[col + \"_ae\"], inplace=True)\n",
    "\n",
    "# (Opcional) armoniza dtypes sin downcasting silencioso\n",
    "merged = merged.infer_objects(copy=False)\n",
    "\n",
    "faltan = (\n",
    "    _is_blank(merged[\"LAT\"]) |\n",
    "    _is_blank(merged[\"LON\"]) |\n",
    "    _is_blank(merged[\"AT&T_Site_Name\"])\n",
    ")\n",
    "\n",
    "if not faltan.any():\n",
    "    # All_Ericsson llenó todo → no usar EPT\n",
    "    df_out = merged\n",
    "    print(\"All_Ericsson cubrió 100% (LAT/LON/AT&T_Site_Name). Se omite fallback EPT.\")\n",
    "else:\n",
    "    print(f\"Quedan {int(faltan.sum())} filas con faltantes. Se aplica fallback EPT…\")\n",
    "\n",
    "    # 3) Fallback EPT (solo completa donde aún falte)\n",
    "    ept_matches = glob.glob(str(BASE_DIR / \"EPT_ATT_UMTS_LTE_*.xlsx\"))\n",
    "    if ept_matches:\n",
    "        ept_file = ept_matches[0]\n",
    "        ept_sheets = [\"EPT_3G_LTE_OUTDOOR\", \"PLAN_OUTDOOR\", \"EPT_3G_LTE_INDOOR\", \"PLAN_INDOOR\", \"Eventos_Especiales\"]\n",
    "\n",
    "        frames = []\n",
    "        for sh in ept_sheets:\n",
    "            try:\n",
    "                frames.append(pd.read_excel(\n",
    "                    ept_file, sheet_name=sh,\n",
    "                    usecols=[\"ATT_CELL_ID_Name\", \"Latitud\", \"Longitud\", \"AT&T_Site_Name\"],\n",
    "                    engine=\"openpyxl\"\n",
    "                ))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if frames:\n",
    "            ept_df = pd.concat(frames, ignore_index=True).rename(columns={\n",
    "                \"ATT_CELL_ID_Name\": \"Cell Name\",\n",
    "                \"Latitud\": \"LAT\",\n",
    "                \"Longitud\": \"LON\",\n",
    "            })\n",
    "\n",
    "            # Usamos AT&T_Site_Name del EPT como llave contra eNodeB Name (ajusta si prefieres 'Cell Name')\n",
    "            ept_lookup = (ept_df[[\"AT&T_Site_Name\", \"LAT\", \"LON\"]]\n",
    "                          .rename(columns={\"AT&T_Site_Name\": \"eNodeB Name\",\n",
    "                                           \"LAT\": \"LAT_ept\", \"LON\": \"LON_ept\"}))\n",
    "            ept_lookup[\"eNodeB Name\"] = ept_lookup[\"eNodeB Name\"].astype(str).str.strip()\n",
    "            ept_lookup = ept_lookup.drop_duplicates(subset=[\"eNodeB Name\"], keep=\"first\")\n",
    "\n",
    "            merged[\"eNodeB Name\"] = merged[\"eNodeB Name\"].astype(str).str.strip()\n",
    "            merged = merged.merge(ept_lookup, on=\"eNodeB Name\", how=\"left\")\n",
    "\n",
    "            # Completar SOLO lo que sigue faltando\n",
    "            need_lat = _is_blank(merged[\"LAT\"])\n",
    "            need_lon = _is_blank(merged[\"LON\"])\n",
    "            need_site = _is_blank(merged[\"AT&T_Site_Name\"])\n",
    "\n",
    "            if \"LAT_ept\" in merged:\n",
    "                merged.loc[need_lat, \"LAT\"] = merged.loc[need_lat, \"LAT_ept\"]\n",
    "            if \"LON_ept\" in merged:\n",
    "                merged.loc[need_lon, \"LON\"] = merged.loc[need_lon, \"LON_ept\"]\n",
    "            # Si aún falta el SiteName, úsalo desde la llave del EPT (que aquí es eNodeB Name)\n",
    "            merged.loc[need_site, \"AT&T_Site_Name\"] = merged.loc[need_site, \"eNodeB Name\"]\n",
    "\n",
    "            merged.drop(columns=[c for c in [\"LAT_ept\", \"LON_ept\"] if c in merged.columns], inplace=True)\n",
    "\n",
    "            print(\"Fallback EPT aplicado:\", ept_file)\n",
    "        else:\n",
    "            print(\"No se pudieron leer hojas del EPT; se omite fallback.\")\n",
    "    else:\n",
    "        print(\"No se encontró archivo EPT_ATT_UMTS_LTE_*.xlsx; se omite fallback.\")\n",
    "\n",
    "# 4) Guardar para que tu celda de formateo final (headers/estilos) tome este resultado\n",
    "final_path = BASE_DIR / \"Datos_Modified.xlsx\"\n",
    "merged.to_excel(final_path, index=False)\n",
    "print(\"Guardado con All_Ericsson + fallback EPT ->\", final_path)\n",
    "\n",
    "# --- Calcular PCI = IF(R blank, Q, Q*3 + R) y guardar ---\n",
    "q_col = \"physicalLayerCellIdGroup\"\n",
    "r_col = \"physicalLayerSubCellId\"\n",
    "\n",
    "merged[q_col] = pd.to_numeric(merged[q_col], errors=\"coerce\")\n",
    "merged[r_col] = pd.to_numeric(merged[r_col].astype(str).str.strip().replace({\"\": None}), errors=\"coerce\")\n",
    "\n",
    "merged[\"PCI\"] = pd.Series(\n",
    "    np.where(merged[r_col].isna(),\n",
    "             merged[q_col],\n",
    "             merged[q_col]*3 + merged[r_col]),\n",
    "    index=merged.index,\n",
    "    dtype=\"Int64\"\n",
    ")\n",
    "\n",
    "# --- Marcar \"MOCN Activo por Celda\" (match EXACTO) y guardar ---\n",
    "# Cadena exacta contra la que quieres comparar\n",
    "pattern = \"[{mncLength=3, mcc=334, mnc=90}, {mncLength=2, mcc=334, mnc=3}, {mncLength=2, mcc=1, mnc=1}, {mncLength=2, mcc=1, mnc=1}, {mncLength=2, mcc=1, mnc=1}]\"\n",
    "\n",
    "# Crear/actualizar la columna \"MOCN Activo por Celda\"\n",
    "merged[\"MOCN Activo por Celda\"] = np.where(\n",
    "    merged[\"additionalPlmnList_mcc\"].astype(str).str.strip() == pattern,\n",
    "    \"Si\",\n",
    "    \"No\"\n",
    ")\n",
    "# --- \"Al menos una celda de MOCN encendida\" basado en match eNodeB Name ∈ {AT&T_Site_Name con MOCN=SI} ---\n",
    "def _norm(x):\n",
    "    s = \"\" if pd.isna(x) else str(x)\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    return s.strip().lower()\n",
    "\n",
    "# Normaliza columnas relevantes\n",
    "mocn_norm = df_out[\"MOCN Activo por Celda\"].map(_norm)\n",
    "enb_norm  = df_out[\"eNodeB Name\"].map(_norm)\n",
    "site_norm = df_out[\"eNodeB Name\"].map(_norm)\n",
    "\n",
    "truthy = {\"si\", \"sí\", \"yes\", \"true\", \"1\"}\n",
    "mocn_flag = mocn_norm.isin(truthy)\n",
    "\n",
    "# Conjunto de sitios con MOCN=Sí (sin strings vacíos)\n",
    "sites_with_mocn = set(site_norm[mocn_flag])\n",
    "sites_with_mocn.discard(\"\")  # elimina vacío si existe\n",
    "\n",
    "if sites_with_mocn:\n",
    "    # Ruta A: por sitio\n",
    "    merged[\"Al menos una celda de MOCN encendida\"] = np.where(\n",
    "        enb_norm.isin(sites_with_mocn), \"Si\", \"No\"\n",
    "    )\n",
    "else:\n",
    "    # Ruta B: por eNodeB Name (si cualquier celda de ese eNB tiene MOCN=Sí)\n",
    "    any_mocn_by_enb = mocn_flag.groupby(enb_norm).transform(\"any\")\n",
    "    merged[\"Al menos una celda de MOCN encendida\"] = np.where(any_mocn_by_enb, \"Si\", \"No\")\n",
    "\n",
    "print(\"MOCN=Sí filas detectadas:\", int(mocn_flag.sum()))\n",
    "print(\"Sitios con MOCN=Sí:\", len(sites_with_mocn))\n",
    "# --- eNBId desde MME ---\n",
    "df_MME = pd.read_excel(BASE_DIR / mme_xlsx, header=None, usecols=[0,1,2], dtype=str)\n",
    "df_MME.columns = [\"NodeId\", \"eNodeBFunction\", \"TermPointToMmeId\"]\n",
    "df_MME[\"NodeId\"] = df_MME[\"NodeId\"].astype(str).str.strip()\n",
    "\n",
    "# Filtrar solo NodeId de longitud 7\n",
    "df_MME_7 = df_MME[df_MME[\"TermPointToMmeId\"].str.len() == 7].copy()\n",
    "\n",
    "# Conteo tipo COUNTIF: cuántas veces aparece cada NodeId\n",
    "mme_counts = df_MME_7[\"NodeId\"].value_counts()\n",
    "\n",
    "# Normalizar clave y mapear conteo a tu base por eNodeB Name\n",
    "df_out[\"eNodeB Name\"] = df_out[\"eNodeB Name\"].astype(str).str.strip()\n",
    "merged[\"MME TEF\"] = (\n",
    "    df_out[\"eNodeB Name\"].map(mme_counts).fillna(0).astype(\"Int64\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "df_out = merged\n",
    "# Actualiza df_out con el DataFrame enriquecido.\n",
    "\n",
    "# --- Guardar preliminar (lo tomará la [7] para formateo final) ---\n",
    "final_path = BASE_DIR / \"Datos_Modified.xlsx\"\n",
    "# Define la ruta del Excel preliminar (sin estilos).\n",
    "\n",
    "df_out.to_excel(final_path, index=False)\n",
    "# Escribe el Excel con todas las columnas (aquí todavía sin formato openpyxl).\n",
    "\n",
    "print(\"Guardado enriquecido ->\", final_path, \"shape=\", df_out.shape)\n",
    "# Log: confirma guardado y muestra dimensiones finales.\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "dd34249ef17f6f73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All_Ericsson cubrió 100% (LAT/LON/AT&T_Site_Name). Se omite fallback EPT.\n",
      "Guardado con All_Ericsson + fallback EPT -> C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\Datos_Modified.xlsx\n",
      "MOCN=Sí filas detectadas: 51765\n",
      "Sitios con MOCN=Sí: 6926\n",
      "Guardado enriquecido -> C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\Datos_Modified.xlsx shape= (59331, 32)\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T16:46:05.621795Z",
     "start_time": "2025-09-26T16:44:34.530795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "final_excel = BASE_DIR / \"Datos_Modified.xlsx\"\n",
    "tmp_excel = BASE_DIR / \"~tmp_Datos_Modified.xlsx\"\n",
    "\n",
    "# Releer, forzar columnas y orden\n",
    "df_out = pd.read_excel(final_excel)\n",
    "\n",
    "# Garantiza que TODAS las columnas existan\n",
    "for col in HEADERS:\n",
    "    if col not in df_out.columns:\n",
    "        df_out[col] = pd.NA\n",
    "\n",
    "# Reordena exactamente como HEADERS\n",
    "df_out = df_out[HEADERS]\n",
    "\n",
    "# Escribe temporal\n",
    "df_out.to_excel(tmp_excel, index=False)\n",
    "\n",
    "# Reaplicar estilo vertical de headers\n",
    "wb = load_workbook(tmp_excel)\n",
    "ws = wb.active\n",
    "\n",
    "# Congelar encabezado\n",
    "ws.freeze_panes = \"A2\"\n",
    "\n",
    "# Aplicar estilo a fila 1\n",
    "for col_idx, header in enumerate(HEADERS, start=1):\n",
    "    cell = ws.cell(row=1, column=col_idx)\n",
    "    cell.value = header\n",
    "    cell.font = Font(name=\"Aptos Narrow\", size=11)  # bold=True si quieres negrita\n",
    "    cell.alignment = Alignment(textRotation=90, horizontal=\"center\", vertical=\"bottom\", wrap_text=True)\n",
    "\n",
    "wb.save(final_excel)\n",
    "\n",
    "# Limpia temporal\n",
    "try:\n",
    "    tmp_excel.unlink()\n",
    "except Exception as e:\n",
    "    print(\"No se pudo borrar temporal:\", e)\n",
    "\n",
    "print(\"Ajuste final OK -> Headers verticales y columnas forzadas/ordenadas.\")\n",
    "\n"
   ],
   "id": "cdc94f3d0b991d80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajuste final OK -> Headers verticales y columnas forzadas/ordenadas.\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
