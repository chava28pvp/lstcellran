{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-29T22:38:25.167703Z",
     "start_time": "2025-09-29T22:38:25.163914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, glob, io\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Alignment, Font\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "# Ruta base (ajústala si cambia)\n",
    "BASE_DIR = Path(r\"C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\4G\")\n",
    "\n",
    "# Lista de 31 encabezados, en el orden en que los quieres (eNBId queda en W si mantienes este orden)\n",
    "HEADERS = [\n",
    "     \"eNodeB Name\",\"CellName\",\"activePlmnList_mcc\",\"additionalPlmnList_mcc\",\n",
    "    \"administrativeState\",\"cellBarred\",\"cellId\",\"cellSubscriptionCapacity\",\n",
    "    \"channelSelectionSetSize\",\"dlChannelBandwidth\",\"earfcndl\",\"earfcnul\",\n",
    "    \"freqBand\",\"noOfPucchCqiUsers\",\"noOfPucchSrUsers\",\"operationalState\",\n",
    "    \"physicalLayerCellIdGroup\",\"physicalLayerSubCellId\",\"sectorCarrierRef\",\n",
    "    \"tac\",\"timeOfLastModification\",\"ulChannelBandwidth\",\n",
    "    \"eNBId\",\"eNodeB Name Unique\",\"LAT\",\"LON\",\"PCI\",\"AT&T_Site_Name\",\n",
    "    \"MOCN Activo por Celda\",\"Al menos una celda de MOCN encendida\",\"MME TEF\"\n",
    "]\n",
    "\n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T22:38:27.031891Z",
     "start_time": "2025-09-29T22:38:27.023272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def appendfiles(filenamepattern: str, save: bool = True) -> str | str:\n",
    "    searchpattern = str(BASE_DIR / f\"{filenamepattern}_*.txt\")\n",
    "    filestoread = glob.glob(searchpattern)\n",
    "    print(\"Buscando:\", searchpattern)\n",
    "    print(\"Archivos:\", filestoread)\n",
    "\n",
    "    chunks = []\n",
    "    for name in filestoread:\n",
    "        with open(name, \"r\", encoding=\"utf-8\") as f:\n",
    "            chunks.append(f.read())\n",
    "    integrated = \"\".join(chunks)\n",
    "\n",
    "    if save:\n",
    "        outputfile_name = f\"Integrated_{filenamepattern}_files.txt\"\n",
    "        output_path = BASE_DIR / outputfile_name\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as out:\n",
    "            out.write(integrated)\n",
    "        print(\"Integrado =>\", outputfile_name)\n",
    "        return outputfile_name\n",
    "    else:\n",
    "        print(\"Integrado en memoria (no se guardó archivo).\")\n",
    "        return integrated  # ← devuelve el TEXTO\n",
    "\n",
    "\n",
    "def cleanfile(source: str, ignorelines=None, save: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Si save=True, 'source' es NOMBRE de archivo. Si save=False, 'source' es TEXTO.\n",
    "    \"\"\"\n",
    "    if ignorelines is None:\n",
    "        ignorelines = [\"SubNetwork,\", \"instance(s)\", \"NodeId\"]\n",
    "\n",
    "    if save:\n",
    "        inputfile = BASE_DIR / source\n",
    "        with open(inputfile, 'r', encoding=\"utf-8\") as f_in:\n",
    "            lines = f_in.readlines()\n",
    "        kept = [ln for ln in lines if not any(p in ln for p in ignorelines)]\n",
    "        cleanfile_name = f\"Clean_{source}\"\n",
    "        with open(BASE_DIR / cleanfile_name, 'w', encoding=\"utf-8\") as f_out:\n",
    "            f_out.writelines(kept)\n",
    "        print(f\"Limpieza OK -> {cleanfile_name} ({len(kept)} líneas)\")\n",
    "        return cleanfile_name\n",
    "    else:\n",
    "        kept = []\n",
    "        for line in source.splitlines(keepends=True):\n",
    "            if any(p in line for p in ignorelines):\n",
    "                continue\n",
    "            kept.append(line)\n",
    "        cleaned = \"\".join(kept)\n",
    "        print(f\"Limpieza en memoria OK -> líneas: {len(kept)}\")\n",
    "        return cleaned  # ← devuelve el TEXTO\n",
    "\n",
    "\n",
    "def convert_to_excel(clean_source: str, save: bool = True) -> str | pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Si save=True, 'clean_source' es NOMBRE de archivo .txt; guarda un .xlsx.\n",
    "    Si save=False, 'clean_source' es TEXTO; devuelve un DataFrame.\n",
    "    \"\"\"\n",
    "    if save:\n",
    "        cleanfile_path = BASE_DIR / clean_source\n",
    "        out_xlsx = f\"Converted_{clean_source}.xlsx\"\n",
    "        out_path = BASE_DIR / out_xlsx\n",
    "        df = pd.read_csv(cleanfile_path, delimiter=\"\\t\", header=None)\n",
    "        df.to_excel(out_path, index=False, header=None)\n",
    "        print(f\"Convertido a Excel -> {out_xlsx}  (shape={df.shape})\")\n",
    "        return out_xlsx\n",
    "    else:\n",
    "        df = pd.read_csv(io.StringIO(clean_source), delimiter=\"\\t\", header=None)\n",
    "        print(f\"Convertido a DataFrame en memoria (shape={df.shape})\")\n",
    "        return df  # ← devuelve el DF\n",
    "\n"
   ],
   "id": "6b2453460000e987",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T22:38:31.102397Z",
     "start_time": "2025-09-29T22:38:30.354572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# --- EUtranCellFDD en memoria ---\n",
    "eu_txt   = appendfiles(\"EUtranCellFDD\", save=False)   # texto crudo\n",
    "eu_clean = cleanfile(eu_txt, save=False)              # texto limpio\n",
    "df_eu    = convert_to_excel(eu_clean, save=False)     # DataFrame (no Excel)\n",
    "\n",
    "\n",
    "# --- ENodeBFunction ---\n",
    "nb_txt   = appendfiles('ENodeBFunction', save=False)     # string con contenido crudo\n",
    "nb_clean = cleanfile(nb_txt, save=False)                # string limpio\n",
    "df_nb    = convert_to_excel(nb_clean, save=False)       # DataFrame en memoria\n",
    "\n",
    "# --- nodeid ---\n",
    "nd_txt   = appendfiles('nodeid', save=False)\n",
    "nd_clean = cleanfile(nd_txt, save=False)\n",
    "df_nd    = convert_to_excel(nd_clean, save=False)\n",
    "\n",
    "# --- MME ---\n",
    "mme_txt   = appendfiles('MME', save=False)\n",
    "mme_clean = cleanfile(mme_txt, save=False)\n",
    "df_mme    = convert_to_excel(mme_clean, save=False)\n",
    "\n",
    "# --- NbIotCell ---\n",
    "nbiot_txt   = appendfiles('NbIotCell', save=False)\n",
    "nbiot_clean = cleanfile(nbiot_txt, save=False)\n",
    "df_nbiot    = convert_to_excel(nbiot_clean, save=False)\n",
    "\n",
    "\n"
   ],
   "id": "822c1f2141799a7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\4G\\EUtranCellFDD_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\4G\\\\EUtranCellFDD_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\4G\\\\EUtranCellFDD_9.txt']\n",
      "Integrado en memoria (no se guardó archivo).\n",
      "Limpieza en memoria OK -> líneas: 51830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SCaracoza\\AppData\\Local\\Temp\\ipykernel_3540\\1924825416.py:67: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(io.StringIO(clean_source), delimiter=\"\\t\", header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertido a DataFrame en memoria (shape=(51814, 23))\n",
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\4G\\ENodeBFunction_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\4G\\\\ENodeBFunction_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\4G\\\\ENodeBFunction_9.txt']\n",
      "Integrado en memoria (no se guardó archivo).\n",
      "Limpieza en memoria OK -> líneas: 6953\n",
      "Convertido a DataFrame en memoria (shape=(6937, 3))\n",
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\4G\\nodeid_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\4G\\\\nodeid_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\4G\\\\nodeid_9.txt']\n",
      "Integrado en memoria (no se guardó archivo).\n",
      "Limpieza en memoria OK -> líneas: 2658\n",
      "Convertido a DataFrame en memoria (shape=(2648, 4))\n",
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\4G\\MME_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\4G\\\\MME_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\4G\\\\MME_9.txt']\n",
      "Integrado en memoria (no se guardó archivo).\n",
      "Limpieza en memoria OK -> líneas: 39456\n",
      "Convertido a DataFrame en memoria (shape=(39438, 9))\n",
      "Buscando: C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\4G\\NbIotCell_*.txt\n",
      "Archivos: ['C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\4G\\\\NbIotCell_14.txt', 'C:\\\\Users\\\\SCaracoza\\\\Documents\\\\AT&T\\\\LST Cell Ran\\\\Ericsson\\\\4G\\\\NbIotCell_9.txt']\n",
      "Integrado en memoria (no se guardó archivo).\n",
      "Limpieza en memoria OK -> líneas: 7529\n",
      "Convertido a DataFrame en memoria (shape=(7517, 14))\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T22:38:35.036681Z",
     "start_time": "2025-09-29T22:38:34.957930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# %% Reacomodo de columnas en memoria\n",
    "# df_eu es el DataFrame base que obtuviste de EUtranCellFDD (ya limpio)\n",
    "\n",
    "cols = list(df_eu.columns)\n",
    "\n",
    "# Guardamos nombre de la columna en posición B (índice 1)\n",
    "col_B = cols[1]\n",
    "\n",
    "# Resto de columnas a partir de la C\n",
    "resto = cols[2:]\n",
    "\n",
    "# Nuevo orden: primera columna (A), resto (C.), y al final la B\n",
    "cols_reordered = [cols[0]] + resto + [col_B]\n",
    "\n",
    "# Aplicar el reordenamiento\n",
    "df_base = df_eu[cols_reordered]\n",
    "\n",
    "print(\"Reacomodo OK -> DataFrame en memoria con columnas reordenadas:\", df_base.shape)\n",
    "\n",
    "\n"
   ],
   "id": "d88444e0446c642e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reacomodo OK -> DataFrame en memoria con columnas reordenadas: (51814, 23)\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T22:42:37.874220Z",
     "start_time": "2025-09-29T22:42:37.820904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Ya tienes df_base (reacomodado y sin headers reales todavía)\n",
    "\n",
    "# Verificación de columnas\n",
    "n_cols = df_base.shape[1]\n",
    "print(\"Columnas detectadas en df_base (reacomodado):\", n_cols)\n",
    "\n",
    "# %% Ajuste de headers en memoria (Opción B - reindex)\n",
    "\n",
    "# Aseguramos que df_base sea copia real (evita warnings de \"view\")\n",
    "df_base = df_base.copy()\n",
    "\n",
    "# Forzar que tenga exactamente las columnas de HEADERS:\n",
    "# - Si falta alguna, la agrega rellena con NA.\n",
    "# - Si sobra alguna, la elimina.\n",
    "# - Ordena exactamente como HEADERS.\n",
    "df_base = df_base.reindex(columns=HEADERS, fill_value=pd.NA)\n",
    "\n",
    "print(\"Headers asignados y ordenados con reindex:\", df_base.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "d6659a16606a67c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas detectadas en df_base (reacomodado): 31\n",
      "Headers asignados y ordenados con reindex: (51814, 31)\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T22:44:04.332922Z",
     "start_time": "2025-09-29T22:44:03.798329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% [A] Alinear df_base exactamente a HEADERS (sin Excel)\n",
    "df_base = df_base.copy()\n",
    "df_base = df_base.reindex(columns=HEADERS, fill_value=pd.NA)\n",
    "print(\"df_base listo:\", df_base.shape)\n",
    "\n",
    "# %% [B] NbIotCell en memoria (df_nbiot ya creado previamente)\n",
    "# Validación de columnas mínimas (A..N => 14)\n",
    "if df_nbiot.shape[1] < 14:\n",
    "    raise ValueError(\"NbIotCell debe tener al menos 14 columnas (A..N).\")\n",
    "\n",
    "# Tomar solo A.N\n",
    "df_nbiot14 = df_nbiot.iloc[:, :14].copy()\n",
    "\n",
    "# %% [C] Construir bloque NbIot alineado por POSICIÓN (mapeo requerido)\n",
    "# Mapeo (según tu comentario original):\n",
    "# A->A, C->B, D->C, E->E, F->F, G->G, H->K, I->L, J->P, K->R, L->D, M->S, N->T\n",
    "df_nb = pd.DataFrame(pd.NA, index=df_nbiot14.index, columns=HEADERS)\n",
    "\n",
    "df_nb[\"eNodeB Name\"]                 = df_nbiot14.iloc[:, 0]    # A -> A\n",
    "# (OJO: ya NO mapeamos ENodeBFunctionId para que no aparezca en el header)\n",
    "df_nb[\"CellName\"]                    = df_nbiot14.iloc[:, 2]    # C -> B\n",
    "df_nb[\"activePlmnList_mcc\"]          = df_nbiot14.iloc[:, 3]    # D -> C\n",
    "df_nb[\"administrativeState\"]         = df_nbiot14.iloc[:, 4]    # E -> E\n",
    "df_nb[\"cellBarred\"]                  = df_nbiot14.iloc[:, 5]    # F -> F\n",
    "df_nb[\"cellId\"]                      = df_nbiot14.iloc[:, 6]    # G -> G\n",
    "df_nb[\"earfcndl\"]                    = df_nbiot14.iloc[:, 7]    # H -> K\n",
    "df_nb[\"earfcnul\"]                    = df_nbiot14.iloc[:, 8]    # I -> L\n",
    "df_nb[\"operationalState\"]            = df_nbiot14.iloc[:, 9]    # J -> P\n",
    "df_nb[\"physicalLayerCellIdGroup\"]    = df_nbiot14.iloc[:,10]    # K -> R\n",
    "df_nb[\"additionalPlmnList_mcc\"]      = df_nbiot14.iloc[:,11]    # L -> D\n",
    "df_nb[\"sectorCarrierRef\"]            = df_nbiot14.iloc[:,12]    # M -> S\n",
    "df_nb[\"tac\"]                         = df_nbiot14.iloc[:,13]    # N -> T\n",
    "\n",
    "# %% [D] Unir debajo y listo (sin guardar a disco)\n",
    "df_out = pd.concat([df_base, df_nb], ignore_index=True)\n",
    "\n",
    "print(\"OK: NbIotCell agregado debajo (en memoria). Shape final:\", df_out.shape)\n"
   ],
   "id": "941410eb832d68a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_base listo: (51814, 31)\n",
      "OK: NbIotCell agregado debajo (en memoria). Shape final: (59331, 31)\n"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T22:53:58.636774Z",
     "start_time": "2025-09-29T22:50:12.237582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "# ============ eNBId desde ENodeBFunction (DF en memoria) ============\n",
    "# df_enbfun: sin headers (como viene del TXT). Tomamos primeras 3 columnas.\n",
    "df_nodeb = df_nb.iloc[:, :3].copy()\n",
    "df_nodeb.columns = [\"NodeId\", \"ENodeBFunctionId\", \"eNBIdnew\"]\n",
    "\n",
    "# normaliza tipos/espacios\n",
    "df_out[\"eNodeB Name\"] = df_out[\"eNodeB Name\"].astype(str).str.strip()\n",
    "df_nodeb[\"NodeId\"] = df_nodeb[\"NodeId\"].astype(str).str.strip()\n",
    "\n",
    "# dedupe por NodeId\n",
    "df_nodeb = df_nodeb.drop_duplicates(subset=[\"NodeId\"], keep=\"first\")\n",
    "\n",
    "# LEFT JOIN por eNodeB Name (df_out) = NodeId (df_nodeb)\n",
    "df_tmp = df_out.merge(\n",
    "    df_nodeb[[\"NodeId\", \"eNBIdnew\"]],\n",
    "    left_on=\"eNodeB Name\",\n",
    "    right_on=\"NodeId\",\n",
    "    how=\"left\"\n",
    ")\n",
    "df_out[\"eNBId\"] = df_tmp[\"eNBIdnew\"]\n",
    "\n",
    "# ============ eNodeB Name Unique (solo cuando cambia) ============\n",
    "_name = df_out[\"eNodeB Name\"].astype(str).fillna(\"\").str.strip()\n",
    "is_new = _name.ne(_name.shift())\n",
    "df_out[\"eNodeB Name Unique\"] = np.where(is_new & _name.ne(\"\"), df_out[\"eNodeB Name\"], \"\")\n",
    "\n",
    "# ============ LAT/LON/AT&T_Site_Name desde All_Ericsson_4G_{YYYYMM-1} ============\n",
    "today = date.today()\n",
    "prev_year  = today.year if today.month > 1 else today.year - 1\n",
    "prev_month = today.month - 1 or 12\n",
    "yyyymm_prev = f\"{prev_year}{prev_month:02d}\"\n",
    "\n",
    "ae_path = BASE_DIR / f\"All_Ericsson_4G_{yyyymm_prev}.xlsx\"\n",
    "ae_df = pd.read_excel(ae_path, usecols=[\"eNodeB Name\", \"LAT\", \"LON\", \"AT&T_Site_Name\"])\n",
    "ae_df[\"eNodeB Name\"] = ae_df[\"eNodeB Name\"].astype(str).str.strip()\n",
    "ae_df = ae_df.drop_duplicates(subset=[\"eNodeB Name\"], keep=\"first\")\n",
    "\n",
    "merged = df_out.merge(ae_df, on=\"eNodeB Name\", how=\"left\", suffixes=(\"\", \"_ae\"))\n",
    "\n",
    "def _is_blank(s: pd.Series) -> pd.Series:\n",
    "    return s.isna() | s.astype(str).str.strip().eq(\"\")\n",
    "\n",
    "for col in [\"LAT\", \"LON\", \"AT&T_Site_Name\"]:\n",
    "    m = _is_blank(merged[col])\n",
    "    if f\"{col}_ae\" in merged:\n",
    "        merged.loc[m, col] = merged.loc[m, f\"{col}_ae\"]\n",
    "        merged.drop(columns=[f\"{col}_ae\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "merged = merged.infer_objects(copy=False)\n",
    "\n",
    "# ======= Fallback EPT por \"Cell Name\" (solo donde aún falte) =======\n",
    "faltan = _is_blank(merged[\"LAT\"]) | _is_blank(merged[\"LON\"]) | _is_blank(merged[\"AT&T_Site_Name\"])\n",
    "if not faltan.any():\n",
    "    df_out = merged\n",
    "    print(\"All_Ericsson cubrió 100% (LAT/LON/AT&T_Site_Name). Se omite fallback EPT.\")\n",
    "else:\n",
    "    print(f\"Quedan {int(faltan.sum())} filas con faltantes. Se aplica fallback EPT…\")\n",
    "    ept_matches = glob.glob(str(BASE_DIR / \"EPT_ATT_UMTS_LTE_*.xlsx\"))\n",
    "    if ept_matches:\n",
    "        ept_file = ept_matches[0]\n",
    "        ept_sheets = [\"EPT_3G_LTE_OUTDOOR\", \"PLAN_OUTDOOR\", \"EPT_3G_LTE_INDOOR\", \"PLAN_INDOOR\", \"Eventos_Especiales\"]\n",
    "\n",
    "        frames = []\n",
    "        for sh in ept_sheets:\n",
    "            try:\n",
    "                tmp = pd.read_excel(ept_file, sheet_name=sh, engine=\"openpyxl\")\n",
    "                frames.append(tmp)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        def coalesce_to_single_column(df, variants, target):\n",
    "            present = [c for c in variants if c in df.columns]\n",
    "            if not present:\n",
    "                return df\n",
    "            cols = [target] + [c for c in present if c != target] if target in present else present\n",
    "            merged_series = df[cols].bfill(axis=1).iloc[:, 0]\n",
    "            df[target] = merged_series\n",
    "            df.drop(columns=[c for c in present if c != target], inplace=True, errors=\"ignore\")\n",
    "            return df\n",
    "\n",
    "        if frames:\n",
    "            ept_df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "            # Unificar nombres de columnas\n",
    "            ept_df = coalesce_to_single_column(ept_df, [\"Cell Name\", \"CellName\", \"ATT_CELL_ID_Name\"], \"Cell Name\")\n",
    "            ept_df = coalesce_to_single_column(ept_df, [\"Latitud\"], \"LAT\")\n",
    "            ept_df = coalesce_to_single_column(ept_df, [\"Longitud\"], \"LON\")\n",
    "            ept_df = coalesce_to_single_column(ept_df, [\"AT&T_Site_Name\"], \"AT&T_Site_Name\")\n",
    "\n",
    "            # LAT/LON a float (evita FutureWarning)\n",
    "            for c in [\"LAT\", \"LON\"]:\n",
    "                if c in ept_df.columns:\n",
    "                    ept_df[c] = (\n",
    "                        ept_df[c].astype(str).str.strip(\"[]\").str.replace(\",\", \"\", regex=False)\n",
    "                    )\n",
    "                    ept_df[c] = pd.to_numeric(ept_df[c], errors=\"coerce\")\n",
    "\n",
    "            # Clean strings\n",
    "            for c in [\"Cell Name\", \"AT&T_Site_Name\"]:\n",
    "                if c in ept_df.columns:\n",
    "                    ept_df[c] = ept_df[c].astype(str).str.strip()\n",
    "\n",
    "            ept_df = ept_df.drop_duplicates(subset=[\"Cell Name\"], keep=\"first\")\n",
    "\n",
    "            # Asegurar llave en base\n",
    "            created_temp_key = False\n",
    "            if \"Cell Name\" not in merged.columns:\n",
    "                if \"CellName\" in merged.columns:\n",
    "                    merged[\"Cell Name\"] = merged[\"CellName\"]\n",
    "                    created_temp_key = True\n",
    "                else:\n",
    "                    raise KeyError(\"No se encontró 'Cell Name' ni 'CellName' en la base para hacer join con EPT.\")\n",
    "\n",
    "            merged[\"Cell Name\"] = merged[\"Cell Name\"].astype(str).str.strip()\n",
    "\n",
    "            # Merge y completar SOLO faltantes\n",
    "            ept_lookup = ept_df[[\"Cell Name\", \"LAT\", \"LON\", \"AT&T_Site_Name\"]].rename(\n",
    "                columns={\"LAT\": \"LAT_ept\", \"LON\": \"LON_ept\", \"AT&T_Site_Name\": \"AT&T_Site_Name_ept\"}\n",
    "            )\n",
    "            merged = merged.merge(ept_lookup, on=\"Cell Name\", how=\"left\")\n",
    "\n",
    "            need_lat  = _is_blank(merged[\"LAT\"])\n",
    "            need_lon  = _is_blank(merged[\"LON\"])\n",
    "            need_site = _is_blank(merged[\"AT&T_Site_Name\"])\n",
    "\n",
    "            if \"LAT_ept\" in merged: merged.loc[need_lat,  \"LAT\"] = merged.loc[need_lat,  \"LAT_ept\"]\n",
    "            if \"LON_ept\" in merged: merged.loc[need_lon,  \"LON\"] = merged.loc[need_lon,  \"LON_ept\"]\n",
    "            if \"AT&T_Site_Name_ept\" in merged: merged.loc[need_site, \"AT&T_Site_Name\"] = merged.loc[need_site, \"AT&T_Site_Name_ept\"]\n",
    "\n",
    "            merged.drop(columns=[c for c in [\"LAT_ept\",\"LON_ept\",\"AT&T_Site_Name_ept\"] if c in merged.columns], inplace=True)\n",
    "            if created_temp_key:\n",
    "                merged.drop(columns=[\"Cell Name\"], inplace=True)\n",
    "\n",
    "            print(\"Fallback EPT aplicado por 'Cell Name':\", ept_file)\n",
    "        else:\n",
    "            print(\"No se encontraron archivos EPT_ATT_UMTS_LTE_*.xlsx; se omite fallback.\")\n",
    "\n",
    "    df_out = merged  # resultado tras fallback (o no)\n",
    "\n",
    "# ============ Calcular PCI = IF(R blank, Q, Q*3 + R) ============\n",
    "q_col = \"physicalLayerCellIdGroup\"\n",
    "r_col = \"physicalLayerSubCellId\"\n",
    "\n",
    "df_out[q_col] = pd.to_numeric(df_out[q_col], errors=\"coerce\")\n",
    "df_out[r_col] = pd.to_numeric(df_out[r_col].astype(str).str.strip().replace({\"\": None}), errors=\"coerce\")\n",
    "\n",
    "df_out[\"PCI\"] = pd.Series(\n",
    "    np.where(df_out[r_col].isna(), df_out[q_col], df_out[q_col]*3 + df_out[r_col]),\n",
    "    index=df_out.index,\n",
    "    dtype=\"Int64\"\n",
    ")\n",
    "\n",
    "# ============ MOCN ============\n",
    "pattern = \"[{mncLength=3, mcc=334, mnc=90}, {mncLength=2, mcc=334, mnc=3}, {mncLength=2, mcc=1, mnc=1}, {mncLength=2, mcc=1, mnc=1}, {mncLength=2, mcc=1, mnc=1}]\"\n",
    "df_out[\"MOCN Activo por Celda\"] = np.where(\n",
    "    df_out[\"additionalPlmnList_mcc\"].astype(str).str.strip() == pattern, \"Si\", \"No\"\n",
    ")\n",
    "\n",
    "truthy = {\"si\", \"sí\", \"yes\", \"true\", \"1\"}\n",
    "enb = df_out[\"eNodeB Name\"].astype(str).str.strip()\n",
    "mocn = df_out[\"MOCN Activo por Celda\"].astype(str).str.strip().str.lower()\n",
    "enbs_con_mocn = set(enb[mocn.isin(truthy)])\n",
    "df_out[\"Al menos una celda de MOCN encendida\"] = np.where(enb.isin(enbs_con_mocn), \"Si\", \"No\")\n",
    "\n",
    "# ============ MME TEF (DF en memoria) ============\n",
    "# df_mme: sin headers. Tomamos 3 primeras columnas y nombramos.\n",
    "df_MME = df_mme.iloc[:, :3].copy()\n",
    "df_MME.columns = [\"NodeId\", \"eNodeBFunction\", \"TermPointToMmeId\"]\n",
    "df_MME[\"NodeId\"] = df_MME[\"NodeId\"].astype(str).str.strip()\n",
    "\n",
    "# Filtrar NodeId de longitud 7 (según tu requerimiento)\n",
    "df_MME_7 = df_MME[df_MME[\"NodeId\"].str.len() == 7].copy()\n",
    "\n",
    "# Conteo por NodeId\n",
    "mme_counts = df_MME_7[\"NodeId\"].value_counts()\n",
    "\n",
    "# Mapear conteo a tu base por eNodeB Name\n",
    "df_out[\"eNodeB Name\"] = df_out[\"eNodeB Name\"].astype(str).str.strip()\n",
    "df_out[\"MME TEF\"] = df_out[\"eNodeB Name\"].map(mme_counts).fillna(0).astype(\"Int64\")\n",
    "\n",
    "# ============ Guardado final con nombre dinámico (si quieres escribir) ============\n",
    "today = date.today()\n",
    "yyyymm = f\"{today.year}{today.month:02d}\"   # p.ej. 202509\n",
    "final_path = BASE_DIR / f\"All_Ericsson_4G_{yyyymm}.xlsx\"\n",
    "\n",
    "# Si quieres guardar aquí:\n",
    "# df_out.to_excel(final_path, index=False)\n",
    "print(\"Reporte final listo →\", final_path, \"shape=\", df_out.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "dd34249ef17f6f73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quedan 1 filas con faltantes. Se aplica fallback EPT…\n",
      "Fallback EPT aplicado por 'Cell Name': C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\4G\\EPT_ATT_UMTS_LTE_2025-08-14.xlsx\n",
      "Reporte final listo → C:\\Users\\SCaracoza\\Documents\\AT&T\\LST Cell Ran\\Ericsson\\4G\\All_Ericsson_4G_202509.xlsx shape= (59331, 31)\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T22:55:24.499728Z",
     "start_time": "2025-09-29T22:54:06.305735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "today = date.today()\n",
    "yyyymm = f\"{today.year}{today.month:02d}\"   # ej. 202509\n",
    "\n",
    "final_excel = BASE_DIR / f\"All_Ericsson_4G_{yyyymm}.xlsx\"\n",
    "tmp_excel   = BASE_DIR / f\"~tmp_All_Ericsson_4G_{yyyymm}.xlsx\"\n",
    "\n",
    "# Releer, forzar columnas y orden\n",
    "df_out = pd.read_excel(final_excel)\n",
    "\n",
    "# Garantiza que TODAS las columnas existan\n",
    "for col in HEADERS:\n",
    "    if col not in df_out.columns:\n",
    "        df_out[col] = pd.NA\n",
    "\n",
    "# Reordena exactamente como HEADERS\n",
    "df_out = df_out[HEADERS]\n",
    "\n",
    "# Escribe temporal\n",
    "df_out.to_excel(tmp_excel, index=False)\n",
    "\n",
    "# Reaplicar estilo vertical de headers\n",
    "wb = load_workbook(tmp_excel)\n",
    "ws = wb.active\n",
    "\n",
    "# Congelar encabezado\n",
    "ws.freeze_panes = \"A2\"\n",
    "\n",
    "# Aplicar estilo a fila 1\n",
    "for col_idx, header in enumerate(HEADERS, start=1):\n",
    "    cell = ws.cell(row=1, column=col_idx)\n",
    "    cell.value = header\n",
    "    cell.font = Font(name=\"Aptos Narrow\", size=11)  # bold=True si quieres negrita\n",
    "    cell.alignment = Alignment(textRotation=90, horizontal=\"center\", vertical=\"bottom\", wrap_text=True)\n",
    "\n",
    "wb.save(final_excel)\n",
    "\n",
    "# Limpia temporal\n",
    "try:\n",
    "    tmp_excel.unlink()\n",
    "except Exception as e:\n",
    "    print(\"No se pudo borrar temporal:\", e)\n",
    "\n",
    "print(\"Ajuste final OK -> Headers verticales y columnas forzadas/ordenadas.\")\n",
    "\n"
   ],
   "id": "cdc94f3d0b991d80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajuste final OK -> Headers verticales y columnas forzadas/ordenadas.\n"
     ]
    }
   ],
   "execution_count": 94
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
